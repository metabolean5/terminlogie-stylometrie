Le première approche utilisée pour traduire des textes était basée sur des règles linguistiques  qui visent à formaliser toutes les connaissances nécessaires à la traduction. Celle-ci nécessite beaucoup de travail de la part des linguistes pour définir le vocabulaire et la grammaire. Cette méthode a donné naissance, par exemple, au célèbre système de traduction Systran. D'autres méthodes existent, comme les méthodes empiriques et parmi elles, l'approche statistique, dont les bases théoriques ont été posées par (Brown et al., 1993) puis (Koehn et al., 2003), qui fait en sorte que toute la connaissance translingue soit acquise de manière automatique à partir de corpus.  Plusieurs travaux se sont intéressés à la comparaison des performances des systèmes issus des  différentes approches. Il en ressort que, pour des performances équivalentes, les approches expertes à base de règle et les approches empiriques font des erreurs différentes lors de la traduction. Dans (Dugast et al., 2008), par exemple, on remarque que les oublis de mots et la génération de mots inconnus sont des types d'erreurs spécifiques aux systèmes statistiques alors que les erreurs dans l'ordre des mots, tout comme les erreurs de vocabulaire, sont spécifiques aux systèmes à base de règles. D'autre part, (Rayner & Bouillon, 1995) précisent que les règles semblent encoder les informations grammaticales alors que les statistiques encodent les informations liées au domaine. Par ailleurs, la combinaison d'un système basé sur les règles et d'un module statistique, par exemple au moyen de la post-édition comme dans (Simard et al., 2007) et (Koehn et al., 2007), montre une amélioration significative de la qualité de traduction.  Cet article propose de combiner les résultats de plusieurs moteurs de traduction automatique  issus de systèmes de natures différentes. Son originalité est qu'il fait appel à différents moteurs de traduction automatique du Web afin d'obtenir plusieurs traductions qui sont ensuite classées par ordre de pertinence à l'aide d'une métrique basée sur la fluidité de la traduction. L'idée est de tirer parti de cette variabilité inter-systèmes en regroupant, pour une même phrase, les résultats obtenus des différents moteurs afin de les mettre en concurrence. Le système utilise plusieurs moteurs de traduction simultanément ; par définition ; nous pouvons lui attribuer l'appellation de méta-moteur de traduction.  Dans un premier temps, des interfaces de traduction sont référencées, sélectionnées puis testées.  Il en résulte une liste de N moteurs de traduction qui sont ensuite interrogés par programme afin d'obtenir N traductions d'une phrase source initiale.  Une fois les résultats des différents moteurs de traduction obtenus, il s'agit de scorer chaque  traduction, en vue de les classer. Pour une phrase source, il en résulte une liste ordonnée d'hypothèses de traduction. La section 3, décrit la métrique utilisée pour classer ces traductions. La démarche est validée par une évaluation automatique (score BLEU) et par une évaluation subjective dont les résultats sont décrits dans la section suivante. Enfin, le système a été implémenté à travers une interface graphique, accessible sur le Web, présentée dans la section 5.  Le cadre applicatif est restreint au domaine des informations journalistiques ou dépêches, plus  communément appelé &#34;news&#34;, en vue de valoriser ce travail par des d'applications directes comme la traduction automatique d'articles journalistiques de la langue anglaise vers la langue  française.   Une sélection manuelle des moteurs de traduction a été réalisée sur la base des caractéristiques  suivantes : disponible sur le net ; gratuit ; permettre la traduction de phrases ou de textes ; traiter la traduction de la langue anglaise vers la langue française ; ne pas pratiquer de &#34;blacklistage&#34; . Peuvent également être prises en compte la performance des résultats, la rapidité de réponse et une bonne utilisabilité. Selon ces critères, 22 interfaces ont ainsi été répertoriées. Lors de l'analyse et des tests des systèmes de traduction présents sur le web, plusieurs interfaces différentes présentaient systématiquement, pour une même phrase à traduire, des résultats identiques. Ces interfaces ont été regroupées et neuf moteurs de traduction distincts ont été identifiés. Comme plusieurs interfaces utilisent le même outil traductif, le choix se porte sur celles qui présentent un temps de réponse court et une bonne utilisabilité (pas d'identification auprès du serveur, méthode d'envoi du formulaire, etc). Par soucis d'anonymat, dans la suite de l'article, les neuf interfaces finalement retenues seront désignées par : MT , MT , MT , MT , MT , MT , MT , MT , MT . Etant donné le cadre applicatif, les corpus doivent nécessairement comporter des données journalistiques de type &#34;news&#34; et ils doivent atteindre une taille suffisante pour permettre des traitements statistiques fiables. Etant donné le peu de ressources disponibles, dans le domaine spécifique des données journalistiques, nous avons, pour le besoin, constitué nos propres corpus à partir de textes écrits collectés sur le World Wide Web. Nous avons constitué, d'une part, un corpus monolingue en français pour le modèle de langage sur lequel d'appuie la métrique ainsi qu'un corpus bilingue français/anglais pour tester le système.  L'apprentissage du modèle de langage, utilisé pour classer les traductions de chaque moteurs,  nécessite un ou des corpus représentatifs des conditions d'utilisation et de l'application envisagée : des corpus monolingues français de données contemporaines et journalistiques et de taille suffisante pour une estimation fiable des probabilités.  Trois modèles de langage ont été entraînés à partir des corpus d'apprentissage ( décrits dans le  tableau 1). Ils sont ensuite combinés par interpolation linéaire. Le corpus de développement nécessaire à l'estimation des pondérations de chaque modèle de langage, est composé de données journalistiques du site France24 du 1er au 14 mai 2008.  Par ailleurs, l'évaluation des systèmes nécessite un corpus de test, bilingue et aligné, de phrases  issues du domaine journalistique qui vont servir de référence pour mesurer la qualité des traducteurs. Pour cela, nous avons donc constitué un corpus bilingue à partir des versions anglaises et françaises du site Web de F rance24. Les phrases alignées ont ensuite été sélectionnées et extraites manuellement, directement d'après la mise en correspondance automatique des do Source  Description Nombre de mots Période France24 www.France24.com 4 M février/avril 2008 Web données du web 72 M juin 2003/avril 2008 Le Monde CDRom de ELRA 23 M janvier/décembre 2003  cuments. Il est à noter que les textes relataient le même événement sans être pour autant la  traduction l'un de l'autre. Les phrases traduites prélevées ont donc dû être vérifiées et, au besoin, corrigées une par une. Le résultat est un corpus de 300 phrases, bilingue et aligné de données journalistiques récentes.  Avant tout traitement, les corpus d'apprentissage ont été normalisés avec la boîte à outils CLIPS-  Text-Toolkit-2.5 (Bigi & Le, 2008). Les modèles de langage ont été entraînés à partir des corpus à l'aide de l'outil SRILM (Stolcke, 2002). Les modèles utilisent des trigrammes de mots, ils ont étés lissés avec la méthode de repli de Kneser-Ney modifiée et un pruning à 10 leur été appliqué.  Nous avons donc créé trois modèles de langage que nous nommerons, en rappel à leur corpus  d'apprentissage respectif (vus dans la table 1), LeMonde, Web et France24. Le modèle de langage final est le résultat de l'interpolation linéaire du modèle France24 avec un coefficient de 0,41, du modèle Web avec un coefficient de 0,42 et du modèle LeMonde avec un coefficient de 0,17. Son vocabulaire est de 60 147 mots et le taux de mots inconnus est de 1,36 % dans le corpus de test.  Nous avons choisi un apprentissage à vocabulaire fermé, un vocabulaire ouvert étant, à notre  avis, inadapté dans le cas de données journalistiques fortement dépendante de l'actualité car le nombre de mots inconnus est très important et la distribution des probabilités en est fortement affectée.  Un modèle de langage analyse une phrase et lui applique plusieurs métriques. Celles qui nous  interressent sont : le nombre de mots de la phrase, le nombre de mots de la phrase ne figurant pas dans le modèle de langage ou mots inconnus (notés OOV s ) et le logarithme de la probabilité de la phrase W = w w · · · w , avec w les K mots de la phrase. Cette dernière notée logprob s'estime comme suit :  logprob(W  ) = log P (w |w , w )  avec  P (w |w , w ) la probabilité du trigramme w , w , w .  En pratique, l'outil SRILM possède une commande  ngram qui, à partir d'un modèle de langage, applique à toute phrase les métriques précédemment citées. Cette commande appliquée à la phrase &#34;je fais un essai&#34; donnera le résultat suivant :  <s> je fais un essai </s>  1 sentence, 4 words, 0 00Vs, logprob= -10.1085  Dans notre application, la métrique recherchée sera utilisée pour comparer les hypothèses de  traduction issues des différents moteurs en ligne.  Pour l'attribution d'un score aux différentes traductions, il est inutile d'introduire un paramètre  destiné à normaliser la longueur des phrases car les phrases, étant la traduction d'une seule et même phrase source, ont nécessairement approximativement le même nombre de mots. Le critère logprob, bien que fortement dépendant du nombre de mots dans la phrase, semble donc en parti approprié à notre application. Le problème est, qu'avec le modèle de langage utilisé, ce critère sur-évalue les phrases comportant un ou plusieurs mots inconnus.  Dans le cas des données d'actualité, le vocabulaire du corpus d'apprentissage est très large et  le modèle de langage associe une probabilité trop importante aux mots hors vocabulaire. Ainsi, certaines suites de mots comme &#34;Je fais un MOTINC&#34; peuvent se voir attribuer une probabilité d'apparition beaucoup plus forte que la phrase &#34;Je fais un essai&#34;. Pour traiter ce problème, une technique consiste à assigner une probabilité que l'on définira aux mots n'apparaissant pas dans le corpus d'apprentissage. Notre métrique de classement doit donc tenir compte du nombre de mots inconnus dans la phrase, en leur attribuant une pondération adéquate. Il est alors nécessaire d'introduire une pénalité  appliquée à chacun des mots hors vocabulaire rencontrés dans les phrases. Pour cela nous avons déterminé empiriquement une valeur adéquate en vérifiant qu'elle soit inférieure au plus petit des logarithmes de probabilité attribué à un mot connu. Celle-ci a été fixée à  = 8. Finalement, le score LP attribué à une phrase traduite W s'écrit :  LP  (W ) = logP (w | w w ) + OOV s ×  (1)  où  OOV s est le nombre de mots hors vocabulaire de la phrase.  Par la suite, nous classerons les phrases par maximisation du score  LP de l'équation 1. Le méta-moteur a l'intéret de présenter, pour une seule et même phrase à traduire, plusieurs traductions provenant de différents systèmes de traduction, d'une part, et de les présenter classées selon un critère de qualité, d'autre part. C'est ce classement qui va être jugé lors des évaluations. Le système de classement est donc évalué, avec les méthodes usuelles, en considérant uniquement la traduction qu'il classe première ( 1stBest). Il présente, en effet, un intérêt particulier s'il apporte plus d'information que l'utilisation d'un seul des moteurs de traduction, pris séparément, en l'occurrence le meilleur. On prévoit donc que les performances de notre système soient meilleures que celles du meilleur moteur de traduction utilisé. Pour vérifier cette hypothèse, nous procéderons à une évaluation automatique que l'on confirmera par une évaluation subjective.  La qualité des traductions est estimée automatiquement par la métrique existante la plus utilisée  dans le domaine, le score BLEU (Papineni et al., 2002), complété avec le score NIST (Doddington, 2002) et le score METEOR (Lavie & Agarwal., 2005). La métrique BLEU repose sur la comparaison de la sortie du traducteur avec les traductions dites de référence. Cette métrique mesure le recouvrement lexical de la phrase traduite avec une ou plusieurs phrases données comme références de la traduction. Le score BLEU varie de 0 à 1 et, étant un score de précision, il est d'autant meilleur qu'il est grand. Néanmoins difficile à interpréter dans l'absolu, nous comparerons le score BLEU de notre système au score Bleu des moteurs de traduction en ligne utilisés. Les scores NIST et METEOR, quant à eux, reprennent le principe du score BLEU et l'adaptent légèrement.  Le tableau 2 montre les résultats de l'évaluation des neufs moteurs de traduction utilisés. Ils sont  classés par ordre de performance, sur notre corpus de test aligné de 300 phrases. Le traducteur MT est en tête avec un score BLEU de 0,311. On constate également que les scores NIST et METEOR sont corrélés avec l'évaluation BLEU.  METEOR  NIST BLEU MT 0,165 6,68 0,311 MT 0,145 6,20 0,258 MT 0,141 6,07 0,252 MT 0,142 6,06 0,251 MT 0,135 5,89 0,234 MT 0,122 5,83 0,216 MT 0,130 5,78 0,230 MT 0,122 5,78 0,216 MT 0,120 5,51 0,206  Le tableau 3 présente les résultats de l'évaluation du classement proposé par la métrique de  l'équation 1. L'ensemble des traductions classées premières par notre système, que l'on nommera par la suite 1stBest, obtient un score BLEU de 0,317. De la même façon, on appelle respectivement 2ndBest et 3rdBest l'ensemble des traductions classées par le système en deuxième et troisième position.  METEOR  NIST BLEU 1stBest 0,170 6,82 0,317 2ndBest 0,154 6,48 0,285 3rdBest 0,144 6,21 0,261  Le tableau 4 détaille la proportion des différents moteurs de traduction présents dans les trois  premiers résultats. Bien que MT soit présent à 80,7 % dans les trois premières réponses données par le système, les autres traducteurs apparaissent également en tête des résultats du clas sement. Les neufs moteurs de traduction sélectionnés apportent donc tous leur contribution aux  performances des trois meilleurs sélectionnés.  %  MT MT MT MT MT MT MT MT MT total 1stBest 54,88 21,44 5,4 5,7 2,8 4,3 3,4 2,0 0 100 2ndBest 15,3 46,4 13,0 6,0 5,8 6,0 4,3 4,3 0,9 100 3rdBest 46,8 8,5 8,8 8,0 9,0 6,3 4,8 4,5 3,0 100 Total 39,0 25,4 9,0 5,5 5,9 5,5 4,1 3,6 1,3 100  En sélectionnant automatiquement la meilleure phrase, au sens de la métrique, proposée par  les neuf moteurs de traduction, notre système présente un score BLEU de 0,317 alors que le meilleur des moteurs de traduction utilisé ( MT ) obtient un score de 0,311. Les scores NIST et METEOR sont eux aussi nettement améliorés. Néanmoins, il est difficile de savoir si ce gain est réellement significatif. De ce fait, il paraît utile d'avoir recours à une évaluation humaine subjective pour vérifier le résultat obtenu de façon automatique.  Le but de cette évaluation subjective est de faire appel à des annotateurs pour confirmer les  résultats obtenus avec l'évaluation automatique. Vérifier le classement complet des traductions par le système serait très complexe à mettre en oeuvre, on préfèrera évaluer le premier choix établi par notre système comparativement au meilleur des moteurs de traduction utilisé, en l'occurrence MT .  On évalue donc la meilleure traduction sélectionnée par notre système comparativement à la  traduction que donne MT . La consigne donnée aux participants est de choisir, parmi les deux phrases, celle qui leur semble la meilleure, ou de n'effectuer aucun choix s'ils jugent cellesci équivalentes. Il y a donc trois réponses possibles : la phrase donnée par notre système est meilleure, la phrase donnée par MT est meilleure, aucune n'est meilleure que l'autre. Il est à noter que nous ne précisons évidemment pas la provenance des phrases. Seize volontaires ont participé à l'expérimentation et chaque phrase a été évaluée par six annotateurs différents. Nous avons vérifié la cohérence des réponses à l'aide de l'indice de Bayes. Plus l'erreur de Bayes est petite, plus les annotateurs sont cohérents dans leurs réponses. Nous avons donc éliminé les phrases pour lesquelles les annotateurs étaient trop partagés, pour obtenir 78 phrases de test restantes (indice de Bayes < 0,9).  Sur ces phrases, on peut donc dire que les participants préfèrent dans 55 % des cas les réponses  renvoyées par notre système contre 33 % pour MT . L'évaluation subjective confirme l'évaluation automatique : il est préférable d'utiliser une traduction choisie parmi plusieurs, provenant de moteurs différents, que de n'utiliser systématiquement que celui qui obtient le meilleur score moyen. On peut en déduire que ce concept de méta-traducteur est une solution pour améliorer la qualité des traductions et que le classement proposé par notre système est pertinent.  Pour mettre ce service de méta-traducteur à disposition d'utilisateurs potentiels, nous avons créé  une interface graphique qui permet d'interagir avec le système à partir du Web. L'interface permet à l'utilisateur d'entrer un texte à traduire et de choisir les moteurs de traduction à utiliser parmi les 9 disponibles ( figure 1). Une fois le texte traduit entré et les moteurs de traduction à utiliser sélectionnés, l'utilisateur a le choix de l'affichage des résultats. Ceux-ci peuvent apparaître classés par ordre de pertinence ou non. L'option d'affichage classé enverra la requête aux moteurs de traduction sélectionnés et, après réception des réponses, les évaluera à l'aide du modèle de langage et les affichera classées (figure 2). L'option d'affichage non classé déroule le processus jusqu'à réception des requêtes et affiche les traductions selon leur ordre d'arrivée sur le serveur. L'interface est accessible, à partir de l'URL suivante : http : //www  clips.imag.f r/geod/User/marion.potet/GUI/metatranslator.php.  Au terme de ce travail, il ressort que, une solution possible pour améliorer la qualité de la  traduction automatique, est de tirer parti de la variabilité des résultats de plusieurs systèmes de traduction. C'est l'intéret et l'originalité de l'outil traductif proposé dans cet article. Notre  méta-traducteur utilise, en effet, plusieurs moteurs de traduction existants et met à profit, pour  chaque situation, ceux qui semblent répondre le mieux. La validation des solutions proposées a été réalisée dans la cadre d'une traduction de phrases anglaises vers la langue française de données de type journalistique. Pour chaque phrase donnée à traduire, les traductions obtenues sont classées selon un critère qualitatif évalué par une métrique reposant sur un modèle de langage. Dans l'échantillon de phrases testé, il s'est révélé être plus pertinent de proposer la meilleure traduction, sélectionnée automatiquement par notre système parmi les neufs hypothèses de traduction, que de proposer systématiquement un seul des moteurs de traduction utilisé. Ceci a été confirmé aussi bien lors de l'évaluation automatique que lors de tests subjectifs auprès d'utilisateurs potentiels. Au vu de ces résultats, l'outil a été mis à disposition sur le web à travers une interface graphique. Il est ainsi possible de soumettre un texte à traduire, de sélectionner des traducteurs automatiques à interroger et de visualiser la liste des traductions proposées classées par ordre de pertinence.  
