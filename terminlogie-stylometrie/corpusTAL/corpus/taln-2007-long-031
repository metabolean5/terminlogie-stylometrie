X est un lexique syntaxique extrait semi-automatiquement des tables du  LADL. Comme les autres lexiques syntaxiques du français disponibles et utilisables pour le TAL (L , D V ), il est incomplet et n'a pas fait l'objet d'une évaluation permettant de déterminer son rappel et sa précision par rapport à un lexique de référence. Nous présentons une approche qui permet de combler au moins partiellement ces lacunes. L'approche s'appuie sur les méthodes mises au point en acquisition automatique de lexique. Un lexique syntaxique distinct de S L X est acquis à partir d'un corpus de 82 millions de mots puis utilisé pour valider et compléter S L X. Le rappel et la précision de cette version améliorée de S L X sont ensuite calculés par rapport à un lexique de référence extrait de D V . S L X is a syntactic lexicon extracted semi-automatically from the LADL tables. Like the other syntactic lexicons for French which are both available and usable for NLP (L , D V ), it is incomplete and its recall and precision wrt a gold standard are unknown. We present an approach which goes some way towards adressing these shortcomings. The approach draws on methods used for the automatic acquisition of syntactic lexicons. First, a new syntactic lexicon is acquired from an 82 million words corpus. This lexicon is then used to validate and extend S L X. Finally, the recall and precision of the extended version of S L X is computed based on a gold standard extracted from D V . lexique syntaxique, évaluation. syntactic lexicon, evaluation.  Un lexique syntaxique décrit les propriétés syntaxiques des mots d'une langue. En particulier,  un lexique syntaxique associe à chaque foncteur syntaxique un cadre de sous-catégorisation spécifiant le nombre et le type (catégorie syntaxique, marqueur introductif, mode, etc.) de ses arguments. Comme l'ont montré (Carroll & Fang, 2004), un lexique syntaxique exhaustif et détaillé permet d'améliorer les performances des analyseurs syntaxiques. Un tel lexique est également une composante essentielle de tout réalisateur de surface puisqu'il permet de réaliser un contenu  sémantique donné par une phrase bien formée et en particulier, une phrase où chaque foncteur  syntaxique a le nombre et le type d'arguments requis par son régime. Plus généralement, un lexique syntaxique est une composante de base pour tout système faisant intervenir soit l'analyse, soit la réalisation.  Pour le francais, il existe à l'heure actuelle trois lexiques syntaxiques disponibles librement  et utilisables par des systèmes de traitement automatique des langues : Proton récemment renommé DicoValence, (van den Eynde & Mertens, 2003), Lefff (Clément et al., 2004) et S L X (Gardent et al., 2006). Néanmoins aucun de ces lexiques n'est entièrement satisfaisant pour deux raisons.  Premièrement, aucun de ces lexiques ne couvre l'ensemble des verbes du français. Ainsi pour  8 790 verbes identifiés pour le français dans Morphalou (Romary et al., 2004), DicoValence inclut 3 700 verbes, Lefff 6 798 et S L X 5244.  Deuxièmement, la qualité de leur contenu et plus précisément, leur rappel et leur précision  restent inconnus : Pour l'ensemble des entrées contenues dans chacun de ses dictionnaires, on ne connait ni quelle proportion des entrées correctes est présente (rappel) ni quelle est la proportion d'entrées incorrectes (précision).  Dans cet article, nous considérons S  L X et présentons une approche qui vise à pallier ces lacunes. L'approche s'appuie sur les méthodes mises au point en acquisition automatique de lexique. Un lexique syntaxique (C L X) distinct de S L X est acquis à partir d'un corpus de 82 millions de mots. Ce lexique est ensuite utilisé pour valider et compléter S L X. Le rappel et la précision de S L X, de la version améliorée de S L X et de C L X sont ensuite calculés par rapport à un lexique de référence extrait de D V .  L'article est structuré comme suit. La section 2 décrit le processus de création de S  L X et présente son format et son contenu. La section 3 présente les travaux visant à valider et à étendre S L X puis commente les résultats obtenus. La section 4 conclut en indiquant les directions de recherche futures.  Synlex est un lexique créé à partir des tables du LADL (Gross, 1975; Guillet & Leclère, 1992;  Boons et al., 1976). Le processus de création a été décrit dans (Gardent et al., 2005b; Gardent et al., 2006; Gardent et al., 2005a) et peut être résumé comme suit :  1. une représentation du contenu des colonnes des tables et de leurs interdépendance est  créée manuellement sous la forme d'un graphe et/ou dont les noeuds contiennent à la fois des conditions et des pointeurs vers le contenu des colonnes 2. ce graphe et/ou est ensuite utilisé en conjonction avec les tables pour produire de facon automatique un lexique syntaxique représentant leur contenu 3. ce lexique est ensuite simplifié pour ne contenir que le type d'information habituellement présente dans un lexique syntaxique (i.e., nombre et types de syntagmes sous-catégorisés par les verbes)  Le format des entrées de S  L X est spécifié dans la figure 1 et peut être décrit comme suit. Une entrée se compose d'un verbe, d'une liste d'arguments syntaxiques ayant un rôle sémantique, d'une liste optionnelle d'associés c-à-d, d'arguments régis par le verbe mais ne remplissant pas de rôle sémantique (e.g., l'explétive il dans il pleut) et d'une liste de macros donnant des informations supplémentaires sur les propriétés syntaxiques du verbes (e.g., contrôle, passivisation). Les associés et les macros sont des listes finies d'atomes. Un argument en revanche est défini par un triplet de la forme F :M-C où F est une fonction grammaticale, M un marqueur optionnel (une préposition ou un clitique indiquant la cliticisation d'un argument en cas d'ambiguité comme par exemple les arguments en à qui peuvent se cliticiser soit en y, soit en lui) et C est une catégorie syntaxique.  Entree  ::= Verb : Arg , Associe , Macro (1) Arg ::= Fonction : Marqueur  Categorie (2) Fonction ::= suj | obj | obja | objde | obl | attr (3) Marqueur ::= Prep | Clitic | Compl (4) Categorie ::= sn | pinf | pcompl | qcompl (5) Associe ::= ilimp | cln | cla | cld | clg | pron (6) Macro ::= CtrlArgXArgY | passivable | nonPassivable (7)  Seules 60% des tables du LADL étant disponibles, nous avons complété manuellement le  lexique extrait des tables disponibles avec environ 2 000 verbes et leurs cadres de base. Le lexique S L X résultant contient 5244 verbes et 19127 entrées (paires verbe - cadre) faisant intervenir 726 cadres de sous-catégorisation en considérant les associés et 538 cadres de sous-catégorisation sans associés.  Comme nous l'avons mentionné, S  L X est produit à partir des tables du LADL par un processus de conversion faisant intervenir une représentation intermédiaire. Or l'information contenue dans les tables peut être inexacte et la conversion dans le format S L X peut introduire des erreurs. Enfin, le lexique produit ne couvre ni l'ensemble des verbes du français, ni nécessairement, l'ensemble des entrées d'un verbe. Il est donc nécessaire à la fois de valider et de compléter le lexique obtenu.  Au cours des 15 dernières années, des travaux (Brent, 1991; Briscoe & Carroll, 1997; Manning,  1993) ont montré qu'il est possible d'extraire un lexique syntaxique d'un corpus en utilisant d'abord un analyseur puis un filtre statistique. L'idée est la suivante. Dans un premier temps, un analyseur déterministe est utilisé pour produire à partir d'un corpus des hypothèses sur les cadres de sous-catégorisation des verbes présents dans ce corpus. Plus précisément, l'analyse produite pour chaque proposition par l'analyseur est utilisée pour associer au verbe de la proposition une description des syntagmes maximaux (groupe nominal, groupe prépositionnel, proposition infinitive, etc.) apparaissant avec ce verbe. Dans un deuxième temps, les hypothèses sont soumises à un calcul statistique et seules sont conservées les hypothèses pour lesquelles la probabilité d'erreur est suffisament basse. Le lexique ainsi obtenu est ensuite évalué (rappel et précision) par rapport à un lexique de référence validé manuellement.  Nous utilisons ici les idées issues de ces travaux pour évaluer la qualité de S  L X. D'une part, nous montrons comment un lexique extrait d'un corpus (C L X) peut être utilisé pour valider S L X et l'enrichir. Le lexique résultant est appelé S L X. D'autre part, nous comparons les trois lexiques ainsi créés (S L X, S L X et C L X) avec un corpus de référence (R L X) extrait de D V .  Afin d'évaluer la précision et la couverture de S  L X, nous commençons par le comparer avec un lexique acquis automatiquement à partir d'un corpus. Ce lexique (C L X) est acquis selon la méthodologie décrite ci-dessus : un corpus et un analyseur sont d'abord utilisés pour émettre des hypothèses sur les entrées lexicales (association verbe - cadre) possibles. Ensuite, ces hypothèses sont soumises à un calcul statistique permettant de classifier les hypothèses en hypothèses plausibles et hypothèses non plausibles. Dans ce qui suit, nous détaillons chacun de ces procédés.  Création des hypothèses. Le corpus exploité est un corpus de 82 millions de mots avec 65%  d'articles de presse, 30 % de compte rendus de débats parlementaires et 5% de textes littéraires.  L'analyseur (T  P ) est un analyseur robuste ascendant qui exploite des connaissances très fines sur la combinaison des mots grammaticaux classifiés en 300 classes de mots simples ou composés (Francopoulo, 2005). Dans la version actuelle (version 1), mise à part, une catégorisation binaire des adjectifs et l'indication comme quoi le verbe accepte ou non, une complétive, l'analyseur n'utilise pas d'information portant sur la sous-catégorisation des verbes et des noms prédicatifs. La technologie mise en oeuvre combine un automate et une matrice statistique induite à partir d'un corpus de 77 000 mots annotés en syntaxe de surface.  Enfin notons que pour cette première expérience, nous nous sommes limités aux cadres qui sont  relativement faciles à détecter pendant l'analyse syntaxique i.e., les cadres ne faisant intervenir ni la fonction oblique, ni la fonction attribut. En outre, les associés (e.g., reflexif intrinsèque, clitique figé) et les macros qui concernent des propriétés syntaxiques non détectables par un analyseur (e.g.,phénomènes de contrôle, acceptation ou non pour les verbes transitifs de la forme passive, etc.) ne sont pas pris en compte.  L'analyse du corpus par T  P permet d'extraire 38 550 hypothèses où chaque hypothèse est l'association d'un verbe, d'un cadre et d'une fréquence d'apparition de cette association dans le corpus.  Filtrage des hypothèses. Afin d'évaluer la plausibilité des hypothèses émises, nous utilisons  un test souvent mis en oeuvre (Brent, 1991; Briscoe & Carroll, 1997; Manning, 1993) par les approches portant sur l'acquisition automatique de lexiques à savoir le test binomial sur les hypothèses (BHT). Ce test calcule la probabilité que m occurrences du cadre c apparaissent avec un verbe v n'acceptant pas ce cadre, étant donné n occurrence de ce verbe. Plus la probabilité est basse, plus l'hypothèse est douteuse et par conséquent, plus il est probable que c est un cadre valide de v. En pratique, nous fixons à 0.05% le seuil utilisé pour déterminer si ou non une association verbecadre apparait suffisament peu fréquemment pour être une erreur. En d'autres termes, toutes les  hypothèses pour lesquelles la probabilité d'erreur donnée par le test BHT est en dessous de  0.05% sont acceptées comme valides - les autres sont rejetées. Pour calculer la probabilité d'erreur des hypothèses émises, nous utilisons le UCS toolkit (http://www.collocations. de/ ). Après filtrage, le lexique syntaxique obtenu (C L X) comporte 8 742 entrées.  Comparaison et fusion des deux lexiques (S  L X et C L X ). La figure 2 donne une analyse détaillée des résultats obtenus à partir de l'analyse de corpus. Plus généralement, on peut diviser et classifier les données suivant les critères suivants :  C  : les entrées présentes dans S L X et dans C L X et pour lesquelles la probabilité d'erreur est inférieure à 0.05% . I : les entrées présentes dans S L X et dans C L X et pour lesquelles la probabilité d'erreur est supérieure à 0.05% . A : les entrées absentes dans S L X qui sont présentes dans C L X et pour lesquelles la probabilité d'erreur est inférieure à 0.05% .  J  : les entrées absentes dans S L X qui sont présentes dans C L X et pour lesquelles la probabilité d'erreur est supérieure à 0.05% . I : les entrées présentes dans S L X absentes dans C L X et pour lesquels le verbe impliqué apparait plus de 5 000 fois dans le corpus. P E P : les entrées présentes dans S L X absentes dans C L X et pour lesquels le verbe impliqué apparait moins de 5 000 fois dans le corpus.  La classe C  permet de valider la partie de S L X trouvée en corpus et validée par les statistiques. Inversement, la classe I permet de détecter les entrées de S L X qui sont sans doute incorrectes. Les données montrent en particulier, que sur la base de cette analyse, plus de la moitié des entrées de S L X sont jugées incorrectes.  Par ailleurs, la classe A  permet d'étendre S L X avec les entrées jugées fiables par l'analyse de corpus mais non contenues par S L X. Ceci permet d'augmenter le nombre d'entrées de S L X de 34.56%.  Enfin, les classes I  et P E P regroupent les entrées de S L X qui n'apparaissent pas dans les données extraites du corpus. Les I sont des cas où le verbe considéré apparaît plus de 5 000 fois dans le corpus mais jamais avec le cadre prescrit par S L X. Ils sont éliminés de S L X. Si le verbe apparait moins de 5 000 fois dans le corpus, l'entrée est conservée mais étiquettée comme peu fiable (P E P ).  En résumé, la fusion  S L X de S L X avec C L X peut être définie par l'union de C avec A : S L X = C  A  P E P  Cependant, cette fusion ne garantit pas un lexique parfait. En effet, la validation statistique  reste imparfaite. Par exemple, les meilleurs lexiques extraits pour l'anglais avec des méthodes similaires à celle utilisée ici ont une F-mesure maximum tournant autour de 80 % . La deuxième étape a donc consisté à évaluer les différents lexiques (S L X, C L X et S L X) en mesurant leur rappel et précision par rapport à un lexique de référence R L X. L'objectif est de déterminer si l'extension de S L X par les données issues de C L X accroit non seulement le nombre d'entrées mais également la qualité du lexique résultant. YN E  Une façon de déterminer la qualité d'un lexique consiste à calculer son rappel et sa précision  par rapport à un lexique de référence. Soit Acquis le contenu du lexique à évaluer et Ref celui du lexique de référence, précision et rappel sont définis de la façon suivante :  Précision   P =  Acquis  Ref Acquis  La précision indique la proportion d'entrées correctes dans le lexique acquis (combien  d'entrées sont correctes ?)  Rappel   R =  Acquis  Ref Ref  Le rappel indique la proportion entre entrées correctes présentes dans le lexique acquis  et entrées présentes dans le lexique de référence (combien d'entrées correctes ont été trouvées ?).  Calcul du rappel et de la précision. Pour l'évaluation, nous avons sélectionné 100 verbes  présents dans tous les lexiques (i.e., S L X, S L X, D V et C L X) et distribués de façon régulière sur l'échelle du nombre d'apparition dans le corpus.  Pour chacun de ces 100 verbes, nous avons créé un lexique de référence R  L X à partir de D V . Les entrées de ces verbes ont été épurées des entrées non prises en compte dans C L X (c-à-d, les entrées faisant intervenir des arguments obliques ou attributifs) puis traduites dans le format S L X (cf. Figure 1) afin de permettre une comparaison automatique avec S L X, S L X et C L X. Les performances des statistiques ont été évaluées sur ces 100 verbes à travers quatre expériences visant à mesurer l'impact de la fréquence d'un cadre sur ces performances.  Etant donné C le nombre total d'entrées présentes dans C  L X, la fréquence f d'un cadre c est dite si c apparait dans plus de 1% des entrées de C L X (f  0.01 × C) ; si 0.001 × C  f  0.01 × C ; et si f  0.0001 × C .  Pour chaque lexique (S  L X, S L X et R L X), quatre (sous-)lexiques sont créés : un premier contenant toutes les entrées du lexique ( ) et trois autres contenant uniquement les entrées faisant intervenir des cadres de haute ( ), moyenne ( ) et basse ( ) fréquence. La référence minimum (baseline) est fixée comme étant le lexique acquis à partir du corpus sans filtrage statistique (toutes les entrées trouvées par T P sont prises en compte).  Le rappel et la précision pour chacun des 5 cas considérés sont donnés dans la Figure 3.   Discussion. Ces premiers résultats montrent que pour l'échantillon de cadres considérés (les  cadres ne faisant pas intervenir d'obliques ou d'attributs), la couverture et la précision de S L X sont relativement bas. La couverture faible n'est pas surprenante et s'explique du fait de l'incomplétude inhérente aux tables du LADL puisque seules 60% des tables sont disponibles.  La mauvaise précision est en revanche plus surprenante mais peut, peut être, être expliquée par  la relative permissivité des tables du LADL : si une construction est possible pour un verbe donné, elle sera marquée comme telle même si elle est très rare.  Un autre facteur contribuant à diminuer la précision concerne la décision de ne pas prendre  en compte les associés c-à-d, les arguments régis par le verbe mais ne remplissant pas de rôle sémantique. Or parmi ces associés, on trouve le clitique réfléchi intrinsèque (e.g., se dans s'évanouir). En conséquence, toutes les entrées faisant intervenir un clitique intrinsèque (l'associé ) sont traitées de façon incorrecte comme des entrées sans ce clitique.  Malgré tout, un examen plus approfondi des cas fautifs reste à faire pour déterminer les causes  précises de ce manque de précision et éventuellement, y remédier.  Le rappel et la précision de  S L X , le lexique enrichi à partir du corpus, sont relativement  TOUT HF  MF LF  S  L X P 0.30 0.63 0.16 0.02 R 0.44 0.45 0.47 0.3 F 0.37 0.54 0.31 0.16 S L X P 0.58 0.69 0.23 0.29 R 0.63 0.66 0.56 0.5 F 0.59 0.67 0.4 0.4 S L X + I P 0.49 0.61 0.21 0.27 R 0.76 0.78 0.67 0.5 F 0.62 0.70 0.44 0.38  B  P 0.22 0.29 0.07 0.15 R 0.89 0.95 0.70 0.5 F 0.56 0.62 0.39 0.32  bas mais proches de certains résultats obtenus dans la litérature pour des langues autres que  l'anglais. (Fast & Przepiórkowski, 2005) par exemple, cite un rappel de 47% et une précision de 49% pour une expérience similaire sur le polonais. Pour ce lexique, le rappel et la précision sont meilleurs que pour S L X. En d'autres termes, le lexique extrait du corpus permet de valider et d'étendre la partie de S L X faisant intervenir les cadres considérés pour l'acquisition automatique.  Enfin, les données concernant  S L X+ I montrent qu'ignorer la plausibilité statistique des hypothèses (i.e., conserver les entrées de S L X qui sont infirmées par les statistiques) permet d'améliorer le rappel (0.76 contre 0.63 dans S L X) au détriment bien sûr de la précision (0.49 contre 0.58 dans S L X). Comme nous l'avons mentionné dans l'introduction, trois lexiques syntaxiques sont actuellement disponibles et utilisables dans le domaine du traitement automatique des langues. Cependant, ils sont tous incomplets et leur contenu n'a pas fait l'objet d'une évaluation permettant de déterminer rappel et précision. Le travail présenté dans cet article est un premier pas vers la définition d'une procédure d'évaluation et de fusion de ces lexiques.  Il montre en particulier que D  V peut servir de base à la création d'un lexique de référence permettant ainsi de calculer le rappel et la précision de lexiques créés de façon automatique ou semi-automatique.  Il montre également, qu'un lexique acquis à partir d'un corpus peut permettre d'améliorer la   couverture et la précision d'un lexique existant ; et plus généralement, que la comparaison et  la fusion de plusieurs lexiques pourrait permettre à relativement court terme de produire un lexique syntaxique du français complet et de bonne qualité.  Néanmoins, plusieurs aspects méritent d'être approfondis.   Tout d'abord, notons que l'évaluation de S  L X présentée ici est très partielle puisqu'elle ne porte que sur 33 des 726 cadres présents dans S L X. Une évaluation plus extensive prenant en compte les obliques et les attributs est donc nécessaire. Un second point concerne la procédure d'acquisition automatique. En effet, l'approche présentée ici est une approche préliminaire qui peut être améliorée sur au moins deux points à savoir, la qualité des hypothèses émises d'une part et la qualité du filtre statistique d'autre part.  Les hypothèses émises peuvent être affinées par l'emploi d'un analyseur plus performant - par  exemple, en utilisant une information de sous-catégorisation pour informer l'analyseur ou encore en utilisant un analyseur profond plutôt que local. Une autre possibilité que nous entendons explorer prochainement, est d'utiliser plusieurs analyseurs en parallèle et de comparer/fusionner leurs résultats par un système de vote.  Les travaux fait sur l'anglais suggèrent en outre que le filtre statistique peut être amélioré de  deux façons. Ainsi (Briscoe & Carroll, 1997) montre que le seuil permettant de déterminer l'acceptabilité d'une hypothèse doit être fixé différemment suivant le type de cadre considéré plutôt que de façon uniforme pour l'ensemble des hypothèses comme nous l'avons fait ici. Et (Korhonen, 2002) montre que l'utilisation de techniques de lissages informées par les classes sémantiques de verbes permet d'améliorer les résultats. L'exploitation de ces résultats devrait permettre d'améliorer la qualité du lexique extrait.  Une troisième point, plus ouvert celui-là, concerne l'élargissement des méthodes explorées à  l'ensemble du lexique et en particulier au traitement des macros. Comme nous l'avons vu, S L X, L et D V contiennent outre des informations portant sur la valence (arguments régis par le verbe remplissant ou non un rôle sémantique), des informations portant sur les phénomènes de contrôle, la passivation, la possibilité pour un verbe d'être utilisé dans une tournure impersonnelle, etc. Si elles sont utiles pour le traitement automatique des langues et en particulier, pour l'analyse et la réalisation de surface, ces informations ne peuvent pas être extraites à partir des corpus par les techniques utilisées en acquisition automatique de lexique. Elles sont en revanche partiellement présentes dans les lexiques existants (L , D V et S L X). Une question intéressante est donc de savoir comment cette information peut être utilisée pour informer la complétion d'un lexique partiellement sous-spécifié dans cette dimension. Ou en d'autres termes, comment un lexique acquis à partir de corpus peut être fusionné avec un ou des lexiques acquis par des méthodes «symboliques» (L , S L X) de façon à enrichir la partie acquise statistiquement avec l'information additionnelle contenue dans les lexiques symboliques.  Dans tous les cas, la précision relativement basse des lexiques produits suggère qu'une phase de  validation manuelle est nécessaire. Dans cette optique, une approche qui consiste à privilégier (dans une juste mesure) le rappel plutôt que la précision est sans doute préférable (il est plus facile d'éliminer que d'ajouter). Ce qui suggère en particulier, que S L X+ I est préférable à S L X et plus spécifiquement, que l'extraction de S L X à partir des tables est utile.  
