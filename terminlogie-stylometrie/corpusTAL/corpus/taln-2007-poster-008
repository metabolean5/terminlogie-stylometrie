question answering, definitional question answering.    Les systèmes de Questions/Réponses (sQR) se proposent d'aller au delà de la recherche de   documents  pertinents  afin  de  répondre,  précisément  et  avec  concision,  à  une  question  directement  formulée  en  langue  naturelle.  L'étude  de  ces  systèmes  est  encouragée  par  des  campagnes d'évaluation qui spécifient des axes de recherche comme la nature des questions à  considérer. Cet article s'intéresse tout particulièrement aux questions « définitoires » (QD), en  domaine  ouvert,  telles  que  ,  qui  interroge  sur  les  aspects  biographiques d'un individu, ou les questions   et  , qui attendent une forme étendue ou encore une (voire La) caractéristique  remarquable  de  l'objet  à  définir.  Ces  questions  ont  été  introduites  lors  du  volet  Questions/Réponses (QR) de la campagne TREC-9. Il est à noter que dans ce qui suit, nous  nous limitons au contexte des campagnes EQueR (Ayache et al., 2006) et plus spécifiquement  CLEF (Vallin et al., 2006), où une seule et unique réponse est à produire. En effet, depuis  2003  (Voorhees,  2003)  et  dans  les  campagnes  TREC  (Voorhees,  2005),  les  réponses  attendues aux QD sont constituées de l'intégralité des faits pertinents connus sur le sujet à  définir ; cela, au travers d'un découpage en « pépites » d'informations vitales (à maximiser),  non vitales (indifférentes) et inintéressantes (à minimiser et pénalisantes). A Contrario, dans   ce  travail,  notre  objectif  est  d'extraire  LA meilleure  des  réponses  pour  une  QD ;  et  nous   souhaitons y parvenir depuis la détection de mises en apposition et l'emploi d'un minimum de  ressources.  De  plus,  ce  travail,  préliminaire  du  point  de  vue  de  la  tâche,  nous  permet  d'explorer  l'utilisation  de  la  proximité  immédiate  des  objets  à  définir  avec  leur  définition.  Une  autre  motivation  provenait  des  faibles  performances  obtenues  par  notre  système  lors  d'EQueR sur ces QD : seulement 7% de réponses courtes correctes pour les QD concernant  des personnes et 42% pour les autres. Cela est d'autant peu qu'une partie était obtenue grâce à  des bases de connaissances et par conséquent la projection de ressources exogènes (couteuses  à maintenir). La méthode employée était basée sur un appariement entre un type de réponse  attendu  et  la  détection  au  sein  des  documents  d'Entités  du  type  adéquat.  La  principale  difficulté  rencontrée  était  liée  à  la  l'identification  des  limites  précises  de  l'énoncé  correspondant à la définition. Pour illustrer cette détection parfois mal aisée, considérons une  fonction  (ou  profession),  qui  débute  pourtant  par    (une  telle  construction  est  assez  fréquente en QR) :   Il apparaît, comme le signale les crochets fermants, que la frontière droite de l'expression est   délicate à apprécier, et donc sujette à erreurs lors d'un étiquetage en Entités Nommées (ou  même une extraction à l'aide de patrons). Par conséquent, le risque existe de faire glisser la  réponse extraite vers une réponse incorrecte ou inexacte. Pourtant, cet exemple reste un cas  simple :  pour  les  QD  qui  commencent  par  ,  la  définition  à  trouver  n'est  pas  systématiquement  un  rôle  social  mais  peut  être  n'importe  quelle  raison  pour  laquelle  une  personnalité  est  connue  (de  telles  QD  sont  parfois  étiquetées « WhyFamous »).  Et  pour  les  , les possibilités sont encore plus vastes. Aussi, notre approche a été de partir des  expressions mises en apposition avec les objets à définir pour sélectionner celle qui semble la  meilleure (selon différents critères et indices, qui sont présentés en section 3) et de la proposer  comme  réponse.  L'utilisation  d'une  apposition  permet  de  s'affranchir  d'une  détection  plus  hasardeuse, et puisqu'il s'agit d'un extrait du document, cela nous permet de supposer une  réponse mieux construite. L'approche que nous présentons est évaluée dans le cadre de notre  participation à la campagne QA@CLEF-2006 (Gillard et al., 2006) et obtient autour de 80%  de  bonnes  réponses.  Ensuite  (section  4),  les  cas  d'échecs  que  nous  avons  rencontrés  sur  CLEF-2006 sont analysés et discutés en détails afin de mettre en évidence des améliorations à  mettre en oeuvre ou des points importants à considérer pour traiter de telles questions.   Un travail similaire à celui-ci a été fait par (Malaisé et al., 2005) mais il porte sur la détection   d'énoncé définitoires dans le domaine médical de la tâche spécialisée de la campagne EQueR.  Étrangement, les constructions faisant intervenir des appositions ne sont pas listées dans les  nombreux patrons d'extraction lexico-syntaxiques utilisés pour répondre aux questions.   De nombreuses approches ont été envisagées, et pratiquement chaque système emploie une   stratégie différente pour traiter les questions définitoires. Par exemple, (Greenwood, Saggion,  2004) utilisent une étape préalable d'acquisition de termes secondaires depuis des ressources  exogènes  (WordNet,  l'encyclopédie  Britannica  et  le  Web,  ce  dernier  contribue  d'ailleurs  a  78%)  pour  aider  à  sélectionner  des  définitions  d'abord  extraites  avec  l'aide  de  patrons.  (Fleischman et al., 2003) centrent également leur travaux sur des patrons pour la collecte des  réponses candidates, mais se limitent à deux : la succession   et la   mise en apposition ; ensuite un apprentissage automatique permet un filtrage. L'apposition est   également l'un des patrons de (Hildebrandt et al., 2004) parmi 11 autres constructions pour  extraire a  priori  des  connaissances  du  corpus ;  ils  s'aident  également  d'une  projection  des  définitions de dictionnaires en ligne. (Prager et al., 2001) utilisent les liens d'hyperonymie de  WordNet pour localiser un passage contenant une réponse. D'autres, comme (Cui et al., 2005)  proposent d'utiliser des patrons lexico-syntaxique probabilistes aux travers de deux modèles  (bigrammes et PHMM). (Han et al., 2006) proposent un modèle purement probabiliste basé  sur une séparation entre les modèles pour la question et la définition.   Comme dans tous les systèmes de Questions/Réponses, les questions définitoires sont d'abord   identifiées  comme  telles  puis  classées  suivant  quatre  catégories  (au  moyen  de  motifs  d'expressions régulières) :  , pour les questions telles que   ;    pour  les  questions  telles  que   ;  pour  celles  comme  ;  et  enfin,  ce  qui  constitue le choix par défaut, la catégorie   pour les questions définitoires qui n'entrent dans  aucune  des  précédentes  catégories  comme  par  exemple  , ou .  Ensuite,  l'objet  à  définir  est  obtenu  en  filtrant,  après  un  étiquetage  morphosyntaxique  (TreeTagger),  les  différents  pronoms  interrogatifs  et  mots  vides  de  sens  comme  , , , , etc. Puis toutes les phrases du corpus contenant l'objet à définir sont conservées et étiquetées  morphosyntaxiquement. Si aucune phrase n'est trouvée la réponse   est retournée.   Ce n'est qu'après ces différents prétraitements que les différentes expressions candidates sont   extraites. Chacune d'entre elles est accompagnée de différents critères qu'il est possible de  percevoir comme des juges dont les votes pondérés permettent de faire préférer in fine l'une  plutôt que l'autre. La fusion de ces différents jugements est variable suivant l'appartenance de  la  question  à  l'une  des  quatre  catégories  initiales.  Les  réglages  utilisés  ont  été  obtenus  de  manière empirique sur les QR des campagnes CLEF-2004, 2005 et EQueR.   Les expressions apposées à celle à définir et séparées d'elle par des virgules sont extraites et   constituent  l'ensemble  préférentiel  dans  lequel  la  réponse  devrait  être  extraite.  Un  critère  correspondant à leur fréquence d'apparition dans l'ensemble des phrases leur est associé. Un  autre  ensemble  plus  particulièrement  adapté  aux  acronymes  et  abréviations  est  défini  et  correspond  à  une  construction    ou  ,  par  l'intermédiaire  de  deux  extractions  de  ces   :  l'une  est  obtenue  depuis  un  alignement  dynamique  entre  la  forme    et  l' ,  l'autre  depuis  la  partie  gauche  (respectivement  droite)  la  plus  redondante ;  là  encore,  un  critère  de  fréquence  est  associé avec chacune d'elles. D'autres juges sont définis : la présence en tête de l'expression  du nom le plus fréquent à la position immédiatement à gauche ; de même, et après application  d'un motif morphosyntaxique minimal pour détecter des groupes nominaux, la présence en  tête de ce groupe nominal du nom principal déterminé le plus fréquent ; un taux de couverture  avec  le  centroïde  des  noms  les  plus  fréquents  au  sein  d'une  fenêtre  de  10  mots  autour  de  l'objet  à  définir ;  un  fonction  de  la  longueur  de  l'expression ;  un  fonction  du  nombre  de  noms ;  et  deux  autres  juges  binaires  pour  la  présence  des  noms    ou    en  tête  d'expression.  Enfin,  depuis  ces  expressions,  et  une  stratégie  de  fusion  des  provenances,  catégories et juges, les meilleures sont proposées comme réponse avec un comportement par  défaut qui consiste à répondre par : l'expression apposée qui est à la fois la plus longue et en  adéquation avec le motif de détection des groupes nominaux, si elle existe, ou le nom le plus   fréquent précédant l'objet à définir. Cette réponse par défaut est systématiquement proposée   en position 5.   Contexte et évaluation de la méthode : Le tableau 1 propose un référentiel pour l'évaluation   de  notre  méthode  au  travers  d'un  bilan  sur  les  résultats  obtenus  par  l'ensemble  des  participants aux questions définitoires en français des 3 campagnes QA@CLEF passées, mais  selon les ventilations que nous avons retenues. En 1  colonne (#Q) est présenté le nombre de  QD pour chacune des campagnes et catégories : 20 en 2004, 50 en 2005 et 42 en 2006. Le  2   groupe  de  colonnes  correspond  aux  nombres  et  pourcentages  de  ces  questions  pour  lesquelles  une  réponse  correcte  a  été  trouvée  (RC)  par  au  moins  l'un  des  participants.  Il  ressort de cette colonne que l'ensemble des stratégies mises en oeuvre par tous les systèmes  permet, à partir de 2005, de s'approcher de la totalité des réponses à obtenir (94%). Enfin le  dernier groupe (RC par soumission) permet de situer les performances des systèmes puisque  figure le nombre de réponses correctes obtenues par : la/les moins bonnes des soumissions  (Min.), la/les meilleures (Max.), ainsi que leur moyenne arithmétique (Moy.). Il est à noter que  l'année 2004 est moins représentative puisqu'un seul système a participé. Également, si le (ou  les) meilleur des systèmes dépasse 80% de bonnes réponses, la moyenne de ceux-ci se situe  en  deçà  (34%  et  50%).  Les  questions  portant  sur  les  mots/concepts  les  plus  généraux  (D+minuscules), et dont la réponse devrait être proche d'une définition du type dictionnaire,  rencontrent une réussite moindre, il est possible d'envisager deux raisons à cela : le fait que  les corpus journalistiques employées se prêtent peu à ce type d'extraction (contrairement à  des d'informations plus biographiques), ou tout simplement leur relative nouveauté en QR.   Le tableau 2 présente les résultats obtenus par notre méthode sur les mêmes jeux de QD que   le tableau 1. Les questions des campagnes CLEF 2004, 2005 et EQueR (non présentées) ont  été  utilisées  pour  raffiner  la  méthode  mise  au  point  après  notre  participation  à  EQueR.  L'évaluation  de  ces  résultats  a  été  faite  manuellement.  Il  en  est  de  même  pour  la  dernière  colonne (Au moins une réponse correcte dans les 5 premières réponses). En revanche pour les  autres  données  de  CLEF  2006,  les  résultats  présentés  sont  ceux  obtenus  lors  de  notre  participation à la campagne au volet Français-Français (les résultats en Anglais-Français sont  inférieurs en raison d'erreurs de traduction ; 67% de réponses correctes sont trouvées au rang  1 au lieu de 79%). Les (+1) et (+2) qui figurent dans le tableau correspondent à des réponses  qui auraient dû être extraites mais qui ont été perdues à cause de problèmes d'ingénierie, ou  pour  l'une  de  la  ligne  D+Personne  en  raison  d'une  incertitude  que  nous  avons :  est-il   acceptable de définir    comme une   ?  . Il apparaît de ce tableau que les taux de bonnes réponses sont constants et aux alentours de  80%  au  premier  rang  et  dépassent  83%  pour  une  réponse  correcte  placée  parmi  les  5  premières.  Cependant,  et  comme  précédemment  souligné  par  le  tableau  1,  les  questions  définitoires  portant  sur  des  concepts  génériques  (D+minuscules)  ou  qui  ne  concerne  ni  les  personnes ni les acronymes, rencontrent également moins de succès avec notre méthode.   Cette partie propose une discussion sur les cas d'échecs que nous avons rencontrés. Aussi,   elle s'articule autour de quelques questions qui illustrent des difficultés représentatives pour  notre  système  et  parfois  la  tâche  elle-même.  Il  faut  également  rappeler  que  notre  méthode  n'utilise pas d'analyse syntaxique et repose essentiellement sur une recherche d'expressions  apposées depuis un découpage en phrases, et, par conséquent, avec un contexte limité.  Au delà de la simplicité évidente de cette question pour un  « lecteur moyen de journaux », la difficulté est réelle puisqu'aucun des systèmes n'a trouvé  une réponse correcte. En effet, la réponse retournée par notre système est une nationalité au  travers  du  nom   soit le nom qui qualifie le plus fréquemment   dans le  corpus (  est présent 21 fois sur les 104 occurrences de  ). Cette réponse n'a pas été jugée correcte, la raison étant qu'une nationalité n'est pas suffisante,  à  elle  seule,  pour  qualifier  convenablement  une  personne.  Cette  règle  connue,  il  apparaît  possible, notamment dans ce cas simple, de filtrer les « mauvais » candidats à l'aide de règles  de rejet. Aussi, il faut poursuivre l'investigation. L'une des premières définitions qui viendrait  à l'esprit pour définir   serait très probablement sa qualité de  . Mais,  au  sein  d'un  découpage  en  phrase  du  corpus,    entre  que  très  rarement  en  cooccurrence avec un motif comprenant   (qu'il soit issu de l'expression  ou même  ). Et, sur les neuf fois où cela se produit sur les 104 phrases contenant  , une seule permet effectivement de le définir directement au travers de ce trait :   Cependant, malgré la présence du    introductif à une explicitation, il faut souligner ici à  quel point le processus de réponse apparaît délicat : il est nécessaire de ne pas tenir compte de   la présence du nom    pour revenir en arrière jusqu'à celui de  . En outre, une  inversion de la position des deux joueurs   et   dans la phrase aurait  encore  complexifié  son  analyse  et  aurait  nécessité  la  mise  en  balance  des  deux  groupes  nominaux par la conjonction  . Et dans ce dernier cas, la tâche aurait été compliquée par la  présence  des  mots    et    puisqu'ils  apparaissent  comme  des  éléments  perturbateurs difficiles à prévoir (notamment dans le cas d'une extraction à partir de patrons  morphosyntaxiques)  mais  pourtant  à  ignorer.  Enfin,  il  faut  se  remémorer  qu'il  s'agit  de  l'unique  cooccurrence  entre    et    dans  le  corpus  et  par  conséquent  une  prise en compte fréquentielle n'est pas envisageable dans ce cas (cependant une parenthèse  mérite  d'être  ouverte :  des  procédés  de  résolutions  d'anaphores  pourraient  augmenter  le  nombre de candidats mais notre système en est actuellement dépourvu).   Cette  (probable)  bonne  réponse  étant  écartée,  il  est  possible  de  s'intéresser  à  une  autre    réponse candidate ou plutôt un autre lot de réponses envisageables. En effet,   est à  plusieurs reprises qualifié de   comme explicité dans les exemples ci-dessous :   Ainsi, une première difficulté survient dans le cas d'une éventuelle factorisation fréquentielle   sur  l'expression n'est pas suffisante si le qualifié n'est pas  . Aussi, il peut être nécessaire de la complémenter. Dans ce cas, cela suppose d'être en mesure  de prendre en compte les différentes écritures de  . Pourtant cela serait une erreur de  penser  qu'il  s'agit  d'un  même  classement  comparable  et  qu'il  est  possible  de  suivre  sa  variation dans le temps au travers des différents documents (auquel cas, une décision aurait pu  être de ne considérer que le plus récent). En effet, une   correspond à un classement  préalable à un tournoi, sorte d'estimation faite en fonction du niveau d'un participant, pour  faire en sorte que les meilleurs d'entre eux ne se rencontrent qu'à la fin de la compétition.  Aussi  la  notion  de    n'a  de  sens  que  vis-à-vis  d'une  compétition  sportive.  Cependant,  comme  il  est  possible  de  le  voir  sur  ces  exemples,  ces  références  ne  sont  pas  toujours présentes dans la fenêtre de la phrase : seul le deuxième exemple propose à la fois la  compétition et le domaine au travers des  . Aussi, et finalement,  l'ensemble  de  ces  ambiguïtés  liées  à  autant  d'élisions  peut  diminuer  considérablement  l'intérêt d'une réponse extraite de ces phrases (sauf à pouvoir synthétiser une réponse telle  que  mais dans ce cas il serait probablement  préférable d'aller jusqu'à  ).  Il est d'ailleurs à noter qu'une partie de cette discussion eut été différente si plutôt que   ,  l'expression    avait  été  présente,  puisque  alors  il  aurait  fallu  prendre  justement  en  compte  une  variabilité  dans  le  temps  de  ce  classement  parmi  les  meilleurs  joueurs mondiaux. Et de s'interroger sur l'opportunité de qualifier   avec autant de  classements différents malgré un dénominateur commun d'être « l'un des 10 premiers joueurs  mondiaux  de  tennis  en  1994  »  (d'ailleurs,  n'est-ce  pas  la  réponse,  actuellement  hors  de  portée, qu'il aurait fallu pouvoir inférer de ces différentes réalisations ?).   Après avoir considéré ces candidats de réponses propulsés en tête des possibles en raison de   leur  répétition,  il  apparaît  parmi  les  appositions  restantes  quelques  autres  susceptibles  de  donner  lieu  à  des  réponses  correctes.  Cependant,  dans  trois  de  celles-ci,  le  rapprochement   avec le monde du    n'est pas présent, ce qui par conséquent amènera à présupposer cette  connaissance (qui peut ne pas aller de soi). Il est aussi intéressant de noter que trois d'entres  elles commencent par des adjectifs numéraux ou multiplicateurs (peut-être faut-il y voir une  particularité  de  l'univers  sportif  ?).  Enfin,  et  comme  pour  étayer  la  discussion  passée,  une  erreur  de  typographie  peut  compliquer  les  rapprochements  des .  Tout  comme il apparaît délicat de décider si   est   ou bien   (respectivement dans un document de 1994 et 1995, aussi a-t-il été successivement,  l'un puis l'autre ; du moins au moment de l'écriture de chacun de ces documents).   Enfin, il est possible de remarquer que, depuis ce dernier exemple, et puisque la réponse est à   fournir hors du contexte du document sans doute serait-il préférable d'être capable de perdre  le  pour ne conserver que  . C'est  un  problème  d'ingénierie  lié  aux  nombreuses ponctuations qui a empêché notre système de répondre à cette question depuis :   Mais cette question nous permet d'illustrer une autre difficulté qu'il ne faut pas oublier de   considérer lors des étapes de recherche ou d'appariement, qu'il s'agisse de la question, des  documents voire de l'un avec l'autre. En effet, il faut autoriser une certaine variabilité dans  les noms propres afin de s'assurer qu'  et   soit une même personne, qu'un  éventuel  nom  intermédiaire,  deuxième  prénom,  etc.  puisse  également  être  facultatif  ou  présent. Cela pour qu'  puisse s'écrire  mais  n'être  parfois  qu' .  S'il  en  doit  en  être  de  même  pour  , ou même , force est de constater que dans ce cas la solution  n'apparaît pas triviale (quelle forme canonique pour les noms propres ?). Enfin, dans certains  cas, plus chanceux, si l'écriture fautive apparaît à la fois dans la question et les documents, le  système  peut  établir  qu'   est  tout  de  même  . Néanmoins, il est aussi possible d'utiliser des algorithmes de tolérance aux fautes.  Cette  question  est  particulièrement  intéressante :  comment peut-on définir quelque chose qui apparaît comme déjà transparent ? Notre système  n'y  est  pas  parvenu  et  s'y  est  même  trompé :  le    est  effectivement  un  (et  à  de  nombreuses  reprises),  mais  ce  n'est  pas  très  satisfaisant  et  si  c'est  le  ,  c'est  déjà  trop.  En  effet,  il  faut  prendre  garde  à  certains  adjectifs  qui  nuisent  plus  qu'ils n'apportent. Nous avions envisagé ce problème mais oublié de l'implémenter : s'il est  intéressant  de  savoir  que    est  un  ,  ou  que  est un  ; d'autres adjectifs tels  doivent être soigneusement évités (et à plus forte raison pour un corpus ancien). Ainsi notre  système aurait pu filtrer  le  jugé inexact (mais  aussi parce qu'il était question de l'une des versions de   dans le document). En outre,  et  pour  revenir  au  cas  ,  les  systèmes  participants  n'ont  pu  faire  mieux  que  de  répondre   qui  a  été  la  seule  bonne  réponse  acceptée,  à  deux  reprises,  lors  de  l'évaluation  (mais  il  reste  possible  de  s'interroger  sur  l'intérêt  de  cette  définition,  surtout  lorsque  de   est refusé, peut être parce qu'il s'agit d'une « copie » ou une « imitation »). Il est à noter que notre système propose en 4  position,   mais la fréquence    (7) n'a pu prendre le pas sur celle de   (31). Un autre point est  qu'il  peut  apparaître  opportun  (d'ailleurs  plus  par  conformité  avec  les  spécifications  des  réponses à produire dans le cadre des campagnes d'évaluations CLEF que du point de vue de  la pertinence de la « pépite » d'information elle-même) d'éloigner la proposition subordonnée  relative pour ne conserver que  . Et encore une fois, dans le cas de   :   Pour conclure avec l'institution financière, dans l'exemple ci-dessous, il ne faut pas extraire   trop rapidement  puisqu'il s'agit en fait de la  et non du  , l'une des   du  . Mais la compréhension nécessaire  n'apparait pas dans la fenêtre de la phrase. Cette question cristallise les difficultés. Dans le corpus,  il n'est pas possible d'obtenir la définition à laquelle on s'attend. Pourtant, c'est justement  grâce à une apposition qu'il est envisageable de répondre depuis l'une des 9 phrases utilisant  le  mot  (les  8  autres  sont  susceptibles  de  brouiller  les  pistes).  Mais  l'analyse  pour  aboutir  apparaît particulièrement complexe : il faut considérer la seconde occurrence du mot plutôt  que  la  première  (laquelle  seconde  est  à  ignorer  sinon),  passer  outre  la  forme  plurielle,  les  guillemets fermants, une partie de la première apposition, pour s'arrêter après la troisième, et  non  plus  loin.  Ensuite,  un  samovar  peut  être  . Aucun des systèmes participants n'y est parvenu. Et finalement, cette question pose un autre  problème sous-jacent à toute la tâche Questions/Réponses : est-il toujours possible d'extraire  automatiquement une réponse concise depuis un passage qui la contient manifestement ?  Cette question a donné lieu à une réponse vestimentaire  sans intérêt liée à une énumération :  . La réponse attendue était  , soit celle  correspondant à une absence de réponse dans le corpus. Cette bonne absence de réponse a été  proposée par 5 systèmes, dont le nôtre mais uniquement dans sa version anglais vers français  et cela, seulement à cause d'une erreur de traduction. En effet, notre approche tend à toujours   proposer  une  réponse  (par  défaut  l'expression  apposée  la  plus  longue  ou  le  nom  le  plus   fréquent précédant l'objet de la question). Le seul cas où un   est retourné survient lorsque  l'expression à définir est absente du corpus (comme ce fut le cas à raison pour  , pas encore assez populaire en 1994 et 95, dates des documents de la campagne).   Un tel processus de réponse à des questions définitoires ouvre une perspective intéressante : il   permet d'améliorer la notion de satisfaction en s'essayant à une génération ou plutôt à une  synthèse pour la réponse proposée. En effet, les réponses extraites depuis un contexte unique  ne sont que très rarement exhaustives (surtout s'il est limité à quelques phrases). Pourtant, et à  leur lecture, il apparaît évident qu'il est possible d'améliorer LA réponse grâce à l'ensemble  des réponses candidates (mais il est alors nécessaire de s'assurer de la qualité de ces réponses  candidates, par exemple, depuis des critères fréquentiels ou des coefficients d'association).   Ainsi, lorsque nous cherchons à définir   , le nom le plus fréquemment associé  est   et  parmi  les  expressions  les  plus  fréquentes  sont  , , et  et ont étés  présentées  par  les  systèmes  et  jugées  correctes  lors  de  l'évaluation).  D'autre  part    apparaît  dans  le  corpus  mais  épisodiquement.  Toutefois,  force est de constater que ce dernier semble plus satisfaisant (même si sa fréquence moindre  le rend peu sujet à extraction). En outre, il peut être synthétisé depuis les premiers. Il suffit de  complémenter  le  nom  le  plus  fréquemment  apposé  par  tous  les  adjectifs  épithètes  qui  l'accompagnent dans ses réalisations. Un simple étiquetage morphosyntaxique est suffisant.  Enfin,  une  projection  (sur  le  web  ou  dans  le  corpus)  des  expressions  ainsi  créées  peut  permettre de vérifier leur validité du point de vue de leur construction et, notamment, dans ce  cas, fixer l'ordre des adjectifs (sauf à définir des règles : une nationalité apparaît souvent en  dernier). Il en est de même pour le classique  tour à tour  et   mais  pourtant  les  deux  à  la  fois  ou  des  et , avant tout   et  . Par ailleurs, il peut être opportun de  disposer  d'une  ressource,  même  limitée,  afin,  par  exemple,  de  manipuler  comme  un  même  concept des noms tels que   et  (certaines fonctions reviennent particulièrement dans  les QD de ces campagnes), et/ou éviter une éventuelle redite des adjectifs.   Idéalement une telle méthode pourrait être appliquée avec des subordonnées relatives mais   ces  « pépites »  d'informations  seraient  probablement  jugées  comme  inexactes  dans  le  contexte actuel des campagnes, tout en étant susceptibles d'induire plus d'erreurs durant leur  génération. Cependant, cela permettrait d'apporter une réponse concise adéquate à la question  (fictive)  ,  depuis  le  seul  passage  du  corpus    contenant  l'expression ,  puisqu'elle  serait  alors  . Il est à noter qu'ici, le procédé de réponse implique une mise en relation  entre  et  puis d'utiliser ce qui joue le rôle d'un générique comme antécédent de la  proposition  relative.  En  d'autres  termes,  cela  revient  à  complémenter  une  sorte  de  classe/hyperonyme par la proposition relative initialement apposée à l'expression à définir.   Dans cet article nous avons étudié l'utilisation de l'apposition pour répondre à des questions   définitoires telles qu'elles sont proposées dans les campagnes d'évaluation des systèmes de  Questions/Réponses. En effet, dans notre système, les réponses candidates pour ces questions  sont extraites depuis leur mise en apposition (ou entre parenthèses) avec les objets à définir.  Ensuite,  afin  de  filtrer  et  retenir  la  meilleure  des  expressions  apposées  comme  réponse,  un  choix  est  effectué  depuis  une  stratégie  impliquant  différents  indices  (principalement  fréquentiels) dérivés du voisinage des objets à définir. L'hypothèse forte de notre approche  est  qu'elle  ne  nécessite  pas  de  connaissances  externes  ni  même  une  analyse  syntaxique.  Évaluée  dans  le  cadre  d'une  participation  à  la  campagne  QA@CLEF-2006,  elle  trouve  environ 80% de bonnes réponses. Cependant une analyse détaillée de quelques cas d'échecs  rencontrés nous a amené au constat qu'il n'est pas toujours possible d'aboutir à une réponse.  Enfin, et parce que les réponses aux questions définitoires s'y prêtent particulièrement, nous  avons  esquissé  une  perspective  quant  à  la  synthèse  de  réponses  (depuis  les  meilleures  réponses extraites) pour aller plus avant vers des réponses plus satisfaisantes.   
