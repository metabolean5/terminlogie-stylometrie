Dans cet article, nous présentons les résultats de la campagne d'évaluation  des analyseurs syntaxiques du français. a été la toute première campagne d'évaluation comparative des analyseurs syntaxiques du français en mode boîte noire utilisant des mesures objectives quantitatives. fait partie du programme du Ministère délégué à la Recherche et à l'Éducation, avec le soutien du ministère de délégué à l'industrie et du ministère de la culture et de la communication. Nous exposons tout d'abord la position de la campagne par rapport aux autres projets d'évaluation en analyse syntaxique, puis nous présentos son déroulement, et donnons les résultats des 15 analyseurs participants en fonction des différents types de corpus et des différentes annotations (constituants et relations). Nous proposons ensuite un ensemble de leçons à tirer de cette campagne, en particulier à propos du protocole d'évaluation, de la définition de la segmentation en unités linguistiques, du formalisme et des activités d'annotation, des critères de qualité des données, des annotations et des résultats, et finalement de la notion de référence en analyse syntaxique. Nous concluons en présentant comment les résultats d' se prolongent dans le projet (ANR-06-MDCA-013) qui vient de débuter et dont l'objectif est d'étiqueter un grand corpus par plusieurs analyseurs en les combinant selon des paramètres issus de l'évaluation. In this paper, we present the results of the evaluation campaign on parsers of French. has been the very first black-box comparative evaluation campaign for parsers of French, with objective quantitative performance measures. was part of the program of the Delegate Ministry of Research, jointly supported by the Delegate Ministry of Industry and the ministry of Culture and Communication. After setting in the context of parsing evaluation and giving an account of the campaign, we present the results obtained by 15 parsers according to syntactic relation and subcorpus genre. Then we propose some lessons to draw from this campaign, in particular about the evaluation protocole, the segmenting into linguistic units, the formalism and the annotation activities, the quality criteria to apply for data, annotations and results and finally about the notion of reference for parsing. We conclude by showing how results extend through the project (ANR-06-MDCA-013), which has just started and whose aim is the automatic annotation of a large corpus by several parsers, the combination of which being parametrized by results stemming from evaluation. analyseur syntaxique, évaluation, français. parser, evaluation, french.  Les premières tentatives d'évaluation des analyseurs ont été le fait d'experts qui fondaient leur  appréciation d'un analyseur sur les observations qu'ils avaient faites de ses sorties sur différentes phrases de test, parfois aidés d'une grille d'analyse (Blache & Morin, 2003). Pour le français, à notre connaissance la première tentative d'évaluation comparative a été faite par A. Abeillé (Abeillé, 1991). Dans le souci de réduire la part de subjectivité dans le processus d'évaluation et pour réutiliser les connaissances acquises lors d'une évaluation, les chercheurs se sont ensuite tournés vers des jeux de test prédéfinis, dont TSNLP (Oepen et al., 1996), qui contient des exemples d'analyses correctes et erronées classés par type de constructions linguistiques, est un archétype. Cependant les jeux de test ne peuvent pas rendre compte de la distribution des phénomènes dans un corpus. De plus leur utilité à des fin d'évaluation dans des campagnes ouvertes est limitée dès lors qu'ils sont rendus publics. En effet, il sont de petite taille et paramétrer un analyseur en fonction d'un jeu de test donné devient alors une tâche aisée. Avec le développement conjoint des standards pour les méta-données et des capacités des ordinateurs, nous avons vu appraître les corpus arborés (treebanks), dont le plus célèbre est certainement le Penn Treebank (Marcus et al., 1993). Depuis sa création de nombreux développements pour différents formalismes et pour différentes langues ont vu le jour, dont certains pour le français (Brant et al., 2002) (Abeillé et al., 2000). Cependant, si les corpus arborés peuvent apporter un élément de réponse en ce qui concerne la représentativité des différents genres de texte et la distribution des phénomènes linguistiques, ils n'apportent pas de réponse au problème du formalisme pivot, pour lequel il n'existe à ce jour aucun standard . Comparer des analyseurs implique donc de pouvoir projeter leurs annotations dans une représentation unique, ce qui en général ne peut se faire sans perte d'information. Pour résoudre ce problème, certains (Gaizauskas et al., 1998) proposent de définir une fonction entre systèmes d'annotation, d'autres de tenir compte de la quantité d'information (Musillo & Sima'an, 2002) (méthode qui a le désavantage de nécessiter la construction d'un corpus parallèle par formalisme d'annotation), d'autres encore proposent d'utiliser des mécanismes d'apprentissage grammatical ou des mesures basées sur la distance d'édition (Roark, 2002). En remontant un peu plus dans le passé, (Black et al., 1991) fut le premier à proposer une mesure d'évaluation fondée sur les limites des constituants pour comparer les analyseurs en mesurant le taux de croisement des frontières avec les annotations de référence (crossing brackets) et le rappel. En ajoutant la précision aux deux mesures précédentes, on obtient le protocole (Grammar Evaluation Interest Group) (Srinivas et al., 1996), ou mesures (Carroll et al., 2002). Cependant ces mesures ont été appliquées uniquement sur des constituants non étiquetés, car il était impossible alors de définir un jeu d'étiquettes commun (Black et al., 1991).  À part quelques tentatives ponctuelles, de comparaisons d'analyseurs syntaxiques, comme celle  du projet SPARKLE qui a comparé des analyseurs syntaxiques pour déterminer le plus approprié pour une tâche d'extraction terminologique, ou encore les expériences développées récemment sur des transcriptions orales (Roark et al., 2006), le paradigme d'évaluation n'a jusqu'à présent pas été appliqué à l'analyse syntaxique sur une grande échelle, à l'exception du projet EASY (Vilnat et al., 2004) (Paroubek et al., 2005) qui concerne les analyseurs du français.  La campagne  était une des 8 campagnes d'évaluation des technologies de la langue du projet du programme (décembre 2002 - avril 2006). Dans cette campagne, 15 analyseurs provenant de 13 participants différents : ERSS, FT R&D, INRIA, LATL, LIC2M, LIRMM, LORIA, LPL, STIM, SYNAPSE, SYSTAL, TAGMATICA, VALORIA et XRCE ont été évalués sur les données fournies par les 5 fournisseurs de corpus que sont l'ATILF, le LLF, le DELIC, le STIM et ELDA. La tâche des fournisseurs de corpus a consisté en la collecte du corpus de différents genres de textes et en leur annotation. Le rapport entre la portion de texte annoté et la taille totale du corpus est choisie de manière à décourager une annotation manuelle de l'intégralité du corpus. Le corpus contient des articles de journaux (Le Monde), des textes littéraires (issus de la base Frantext de l'ATILF), des textes médicaux (pathologies et traitements), des questions (issues de la campagne de ), des transcriptions de débats parlementaires (Sénat français et Parlement Européen), des pages WEB du site ELDA, des courriers électroniques et des transcriptions de parole . On pourra trouver dans le tableau 4 plus loin dans l'article, les tailles respectives de ces différents corpus. Le protocole d'évaluation EASY suppose que tous les participants adoptent la même segmentation en mots et en énoncés (voir (Roark, 2002) pour les problèmes que cela pose). Le formalisme inspiré de (Carroll et al., 2002) et défini en collaboration avec les participants doit permettre d'exprimer l'essentiel d'une annotation syntaxique quelle que soit son type (de surface ou profonde, complète ou partielle), ceci sans privilégier une approche particulière. Le formalisme d'annotation EASY permet d'annoter des constituants continus et non-récursifs ainsi que des relations représentant les fonctions syntaxiques. Les relations (binaires pour la plupart ou ternaires) peuvent associer indifféremment des formes individuelles ou des constituants. Notons, qu'EASY ne connait pas la notion de tête lexicale (Gendner et al., 2003) (Vilnat et al., 2004).  Dans  , il y a 6 types de constituants : (1) nominal, (2) adjectival, (3) prépositionnel, (4) adverbial, (5) verbal et (6) prépositionnel-verbal, le dernier étant utilisé pour les verbes à l'infinitif introduits par une préposition, et 14 types de relations de dépendance : (1) sujet-verbe, (2) auxilliaire-verbe, (3) c-o-d, (4) complément-verbe, (5) modifieur de non, (6) modifieur de verbe, (7) modifieur d'adjectif, (8) modifieur d'adverbe, (9) modifieur de préposition, (10) complémenteur, (11) attribut du sujet/objet, (12) coordination, (13) apposition, (14) juxtaposition. Le choix de ces constituants et de ces relations a été fait à la suite de discussions avec l'ensemble des participants à la campagne. Il a ensuite fait l'objet d'une description plus d'étaillée à la fois pour les participants et pour les annotateurs dans un guide , Ils sont également décrits dans (Vilnat et al., 2004). La figure 1 donne un exemple d'annotation d'une phrase issue du corpus littéraire.  Pour comparer les résultats des différents analyseurs, les mesures d'évaluation sont la précision  et le rappel (ainsi que la f-mesure qui les combine) sur lesquelles nous avons expérimenté 15 relâchements de contrainte différents (Paroubek et al., 2006), obtenus en combinant les 5 manières présentées dans la table 1 de comparer les empans de textes correspondant soit aux constituants soit aux cibles de relations, avec les 3 façons de considérer les définitions des constituants (ceux de l'hypothèse, ceux de la référence, ou ceux de l'hypothèse lorsqu'ils existent sinon ceux de la  référence). L'évaluation a été menée indépendamment sur les constituants et les relations. Les  résultats ont été calculés individuellement pour chaque constituant, chaque relation et chaque type de corpus ainsi que de manière globale.  Fonction  Formule H = R |H\R|  1 H  R R  H =  > 0.25 avec :  H  Empan de texte hypothèse et R Empan de texte référence,  Pour les constituants c'est le système P10 qui obtient les meilleurs résultats pour les 3 mesures  (précision, rappel, f-mesure), tous constituants et tous genres de corpus confondus avec la comparaison barycentre pour les empans de texte des constituants (voir table 1 pour la définition de ces notions). La figure 2 illustre les résultats obtenus par ce participant, avec les différents corpus et les constituants annotés sur le plan horizontal (respectivement axe des x et des y) et la performance calculée en vertical (axe des z). Le graphe de gauche correspond à une vue avant, celui de droite à une vue arrière, comme l'illustrent les petits schémas au-dessus des graphes.. Nous avons utilisé la mesure barycentrique, car c'est celle qui, tout en permettant un certain relâchement des contraintes imposées sur les frontières de constituant (qui sont parfois le résultat d'un choix arbitraire), sans toutefois être aussi laxiste que l'intersection (où il suffit qu'un seul mot soit partagé).  La figure 3 illustre les résultats des différents analyseurs en combinant tous les corpus et toutes  les annotations, à la fois en précision, rappel et f-mesure. Sur la figure de gauche, on peut observer 12 colonnes, car trois participants n'ont pas fourni de résultats pour les annotations en constituants mais uniquement l'annotation des relations de dépendance. De même sur la figure de droite, on voit que l'un des participants n'a pas fourni d'annotation en relations de dépendance.  lemonde littéraire médical oral_delic parlement questions  web P1 p=0 p=0 p=0 p=0 p=0 p=0 p=0 f=0 f=0 f=0 f=0 f=0 f=0 f=0 P2 p=0.717 p=0.329 p=0.332 p=0.612 p=0.702 p=0.395 p=0.719 f=0.690 f=0.320 f=0.312 f=0.591 f=0.644 f=0.373 f=0.679 P3 p=0.920 p=0.901 p=0.907 p=0.752 p=0.923 p=0.931 p=0 f=0.926 f=0.912 f=0.913 f=0.760 f=0.930 f=0.935 f=0 P4 p=0.813 p=0.802 p=0.459 p=0.787 p=0.808 p=0.877 p=0.841 f=0.660 f=0.770 f=0.436 f=0.717 f=0.653 f=0.856 f=0.696 P5 p=0.883 p=0.847 p=0.882 p=0.714 p=0.876 p=0.901 p=0.877 f=0.878 f=0.824 f=0.873 f=0.713 f=0.868 f=0.894 f=0.880 P6 p=0.837 p=0 p=0 p=0 p=0.849 p=0 p=0.903 f=0.782 f=0 f=0 f=0 f=0.803 f=0 f=0.893 P7 p=0.832 p=0.838 p=0.825 p=0.784 p=0.833 p=0.826 p=0.739 f=0.832 f=0.845 f=0.805 f=0.743 f=0.831 f=0.822 f=0.734 P8 p=0 p=0 p=0 p=0 p=0 p=0 p=0 f=0 f=0 f=0 f=0 f=0 f=0 f=0 P9 p=0.141 p=0.145 p=0.191 p=0.336 p=0.175 p=0.305 p=0.856 f=0.137 f=0.152 f=0.183 f=0.334 f=0.159 f=0.301 f=0.866 P10 p=0.904 p=0.910 p=0.909 p=0.849 p=0.921 p=0.913 p=0.924 f=0.904 f=0.909 f=0.902 f=0.794 f=0.917 f=0.902 f=0.922 P11 p=0 p=0 p=0 p=0 p=0 p=0 p=0 f=0 f=0 f=0 f=0 f=0 f=0 f=0 P12 p=0.737 p=0.714 p=0.806 p=0.605 p=0.712 p=0.832 p=0.801 f=0.685 f=0.681 f=0.733 f=0.562 f=0.649 f=0.767 f=0.749 P13 p=0.888 p=0.901 p=0.903 p=0.803 p=0.907 p=0.910 p=0.913 f=0.884 f=0.910 f=0.892 f=0.763 f=0.909 f=0.903 f=0.911 P14 p=0.855 p=0.887 p=0.879 p=0.775 p=0.867 p=0.873 p=0.879 f=0.855 f=0.895 f=0.869 f=0.731 f=0.867 f=0.866 f=0.875 P15 p=0.802 p=0.795 p=0.835 p=0.770 p=0.835 p=0.860 p=0.808 f=0.836 f=0.839 f=0.870 f=0.747 f=0.868 f=0.878 f=0.843  Pour les relations, c'est le système P8 qui obtient la meilleure précision, le système P3 qui  obtient le meilleur rappel et le système P10 qui obtient la meilleure f-mesure toutes relations et tous genres de corpus confondus en tenant compte des constituants de l'hypothèse lorsqu'ils existent sinon de ceux de la référence et avec la comparaison barycentre pour les empans de texte des constituants (voir table 1). On voit dans le figure 4 les graphes respectifs de ces trois participants, avec les mêmes conventions que dans la figure 2 .  Le tableau 3 présente les résultats de tous les analyseurs en précision et f-mesure, pour toutes  les relations, par type de corpus.  Tout d'abord, rappelons que ce n'est pas parce qu'un système a une valeur de performance  0 pour un sous-corpus ou une relation particulière qu'il a de mauvaises performances, il peut  genre  énoncés mots relations énoncés relations nb total nb total nb total erronés/testés erronées/testées 77 2104 113 3/7 = 43% 4/77= 03% M 380 10081 5072 12/39= 30% 22/519= 04% 276 7551 3884 14/28= 50% 57/366= 15% 892 24358 12725 36/93= 38% 92/1196= 07% 852 9243 3960 21/75= 28% 30/421= 07% 554 11799 5595 16/54= 29% 28/518= 05% _ 505 8117 4591 10/50= 20% 14/462= 03% 203 4116 2165 9/20= 45% 20/217= 09%  s'agir d'un choix délibéré de son concepteur de ne pas traiter un phénomène particulier ou de ne  retourner qu'une sorte d'annotation, par exemple seulement les relations. Ensuite, de mauvaises performances peuvent provenir de problèmes d'alignement entre les données du participant et celles de références et non d'un mauvais analyseur. Rappelons que dans , contraitement à ce qui avait été fait dans (Adda et al., 1999) ou dans (Roark et al., 2006), il n'y a pas de procédure de réalignement automatique des données du participant, celui-ci doit respecter la segmentation en mots et en phrases des données qu'il traite.  Concernant les résultats, nous constatons, comme cela était à prévoir, une plus grande variabilité  et de moins bonnes performances pour les relations que pour les constituants. Bien entendu, ces résultats ne sont qu'un point de vue ponctuel et sont à relativiser (comme dans toute évaluation quantitative) en fonction des facteurs décrits ci-après. Tout d'abord, la qualité des annotations de référence : nous avons réalisé une première estimation du taux d'erreur d'annotation sur les relations, par type de corpus en demandant à un expert d'examiner à la main un échantillon représentant environ un dixième de chaque corpus annoté. Les résultats sont donnés dans la table 4. Un énoncé est considéré comme erroné s'il contient au moins une erreur d'annotation en relation.  Pour les sous-corpus ayant un taux d'erreur en relation supérieur à 6%, nous avons effectué  des corrections systématiques des erreurs les plus fréquentes avant de lancer les calculs de performance . L'estimation du taux d'erreur d'annotation pour tous les sous-corpus permettra de déterminer des classes de performance parmi les différents systèmes sans prendre en compte des différences de performance inférieures aux taux d'erreur estimé.  Le second point dont il faut tenir compte concerne les erreurs de segmentation en mots/phrases  encore présentes dans la référence et qui nous ont conduit en particulier à abandonner le traitement du corpus oral provenant de la campagne . Ces erreurs (auxquelles parfois s'ajoutent les erreurs de format des données des participants) sont à notre avis le résultat de divers facteurs : l'absence de tests à blanc du protocole (par manque de temps) et le fait d'avoir imposé une segmentation en mots et phrases de la référence, qui se heurte au problème de déterminer une définition acceptable par tous.  Dans le projet  , qui regroupe certains des participants d' , nous annoterons un grand corpus en combinant automatiquement des analyseurs syntaxiques. Pour les deux campagnes d'évaluation prévues, nous envisageons de recourir à des procédure d'alignement automatique à partir du texte comme dans (Adda et al., 1999) ou (Roark et al., 2006). Les participants pourront ainsi conserver leurs propres algorithmes de segmentation en mots et phrases. La phrase dans les données de référence sera déterminée à partir des annotations elles-mêmes, une phrase étant constituée par l'empan de texte sur lequel un arbre syntaxique se projette, comme cela a déjà été fait dans pour le sous-corpus .  Bien entendu, le formalisme d'annotation  s'il semble suffisament abouti pour les relations les plus fréquentes comme la relation sujet-verbe, nécessite d'être approfondi pour les autres ; ce qui sera fait dans le cadre du projet , où cette fois nous considèrerons des constitutants admettant plusieurs niveaux de récursivité. a permis de poser les bases d'un protocole d'évaluation des analyseurs syntaxiques du français en mode boîte noire avec des mesures quantitatives objectives. Il a surtout été l'occasion de former un groupe autour du problème de l'évaluation comparative des technologies d'analyse syntaxique et d'acquérir une première expérience dans le cadre d'une campagne d'envergure qui déjà trouve des prolongements dans le projet . Concernant les mesures de performances proprement dites, l'image ponctuelle qu'elles donnent des performances des analyseurs syntaxiques à un instant particulier, nous montre qu'il reste encore un fort potentiel de développement dans la combinaison des approches pour l'annotation de relations syntaxiques, car ce sont 3 systèmes différents qui obtiennent chacun les meilleurs résultats pour la précision, le rappel et la f-mesure. Ce qui laisse à penser que ces systèmes ont des caractéristiques complémentaires, il reste encore à les identifier et à trouver le moyen de les combiner harmonieusement.  
