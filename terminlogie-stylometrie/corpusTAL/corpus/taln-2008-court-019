L'appariement d'entités nommées consiste à regrouper les différentes formes  sous lesquelles apparaît une entité. Pour cela, des mesures de similarité textuelle sont généralement utilisées. Nous proposons de combiner plusieurs mesures afin d'améliorer les performances de la tâche d'appariement. À l'aide d'expériences menées sur deux corpus, nous montrons la pertinence de l'apprentissage supervisé dans ce but, particulièrement avec l'algorithme C4.5. Matching named entities consists in grouping the different forms under which an entity may occur. Textual similarity measures are the usual tools for this task. We propose to combine several measures in order to improve the performance. We show the relevance of supervised learning in this objective through experiences with two corpora, especially in the case of the C4.5 algorithm. Entités nommées, Appariement, Mesures de similarité textuelle, Apprentissage supervisé. Named entities, Matching, Textual similarity measures, Supervised learning.  La capacité à structurer de grandes quantités d'informations provenant de sources différentes  est un enjeu technologique essentiel, et cette capacité réside notamment dans la possibilité de classer l'information par entité. Pour améliorer la recherche d'information, il est donc nécessaire de savoir reconnaître la même entité lorsqu'elle apparaît sous des formes différentes.  Nous traitons dans cet article du problème de l'identification de séquences de mots représentant  une même entité nommée (EN), dans un cadre où l'on dispose des entités elles-même et du contexte dans lequel elles apparaissent. La difficulté porte sur les nombreuses variations textuelles possibles d'une EN : ces variations peuvent être volontaires et/ou naturelles (écriture différente selon la langue, abréviations ou extensions, surnoms, etc.) ou involontaires (erreurs typographiques ou orthographique, erreurs d'OCR, etc.). On s'intéresse ici particulièrement aux variations dues aux translittérations (traductions entre systèmes d'écriture différents). L'hypothèse de départ réside donc dans l'idée qu'un même référent conduit généralement à des formes "similaires", ainsi qu'à des ressemblances de leurs contextes d'occurrence.  L'appariement d'entités nommées coréférentes est notamment étudié dans la problématique du   liage d'enregistrements (record linkage), qui consiste à repérer deux enregistrements distincts  représentant le même élément dans une base de données (pour la déduplication) ou dans deux bases différentes (fusion de bases) (Winkler, 1999; Bilenko et al., 2003). Certains travaux, tels que (Cohen et al., 2003; Christen, 2006), étudient plus spécifiquement l'appariement de noms de personnes. Dans (Freeman et al., 2006), les problèmes de translittération spécifiques à la question de l'appariement sont traités dans le cas particulier anglais/arabe, tandis que dans (Pouliquen et al., 2006) les auteurs présentent un système d'appariement multilingue.  Les mesures de similarité (ou de distance) textuelle sont les principaux outils utilisés pour ce  type de tâche. On peut grossièrement classer les différents types de mesures existants en trois classes : - les méthodes basées sur les séquences de caractères, qui définissent la similarité par la présence de caractères identiques à des positions similaires (e.g. Levenshtein, Jaro). - les méthodes de type "sac de mots", qui sont basées sur le nombre de mots en commun entre les deux chaînes, indépendamment de leur position. Notons que ces types de mesures sont également applicables aux n-grammes de caractères au lieu des mots. - les méthodes hybrides, qui combinent les caractéristiques des deux précédents types de mesures. Notre objectif est de combiner différentes mesures de similarité de façon à améliorer la qualité de l'appariemment d'entités. Dans cette optique, nous proposons de tirer profit aussi des similitudes éventuelles entre les contextes des entités, cette technique étant déjà utilisée pour le problème connexe de la désambiguisation d'homonymes, notamment dans (Pedersen & Kulkarni, 2007). La combinaison de plusieurs mesures est utilisée dans (Pouliquen et al., 2006), où celle-ci consiste en une simple moyenne sur trois mesures. L'apprentissage supervisé permet une prise en compte plus fine des caractéristiques des mesures et des données, comme le montrent (Bilenko & Mooney, 2003) à l'aide de l'algorithme SVM dans le cadre des bases de données. À l'inverse, nous proposons ici d'appliquer différentes méthodes d'apprentissage dans le cas de données issus de textes non structurés : dans ce contexte, on ne dispose pas de l'information apportée par les différents champs d'un enregistrement, mais seulement du contexte dans lequel sont trouvées les EN.  Notre approche privilégie la robustesse de l'appariement, au sens où nous proposons de réaliser  cette tâche sur tout type d'EN, indépendamment de ressources externes (e.g. dictionnaires de noms, heuristiques spécifiques à un domaine, etc.), et autant que possible sans distinction de langue. À ce titre nous travaillons sur des données potentiellement bruitées. En effet, dans le contexte d'applications réelles, cette tâche dépend de nombreuses phases en amont : la qualité du corpus d'origine, celle de l'extraction de sites web ainsi que celle de la phase de reconnaissance des EN. Notre but n'est donc pas d'obtenir les meilleurs résultats possibles sur des données spécifiques (objectif requérant un travail d'expertise précis et coûteux), mais plutôt des performances satisfaisantes facilement reproductibles sur différents types de données.  Nous présenterons dans un premier temps les données dont nous disposons et les principales  mesures de similarité utilisées dans le système que nous avons implémenté. Nous exposerons ensuite notre approche face aux spécificités du problème, et enfin nous présenterons les expériences réalisées et les résultats obtenus. Le premier corpus (qui sera noté MNI par la suite), en anglais, est constitué d'un recueil d'articles de presse, de dépêches et de rapports officiels de provenance variée consacrés à la menace nucléaire en Iran. Celui-ci provient du site www.nti.org , dont nous utilisons la partie traitant de l'Iran (pour la période 1991-2006) parce qu'elle contient de nombreux cas de translittérations de noms arabes. Ce corpus compte 236 000 mots, et la reconnaissance des EN a été réalisée à l'aide de l'outil GATE . Nous conservons seulement les noms de personnes, d'organisations et de lieux dans l'ensemble des EN ainsi constitué. Nous obtenons ainsi 35 000 EN (en nombre d'occurrences), contenant toutefois quelques erreurs de reconnaissance : principalement des cas de balisage erroné (entité tronquée ou mots superflus inclus dans l'entité) et de noms communs commençant par une majuscule. Nous travaillons sur les EN de fréquence supérieure ou égale à 2, ce qui restreint à 1588 le nombre d'entités distinctes (représentant 33 147 occurrences).  Le second corpus (noté MIF par la suite), long de 856 000 mots, est le résultat de l'aspiration du  contenu de 20 sites web de médias d'information francophones. Ces médias ont été sélectionnés selon les critères suivants : volume suffisant, facilité d'accès et surtout diversité géographique, de façon à maximiser les chances d'y trouver des translittérations (c'est pourquoi nous avons notamment intégré une part non négligeable de médias d'Afrique du Nord). L'extraction a été réalisée durant 4 jours en juillet 2007 par Pertimm . Le corpus ainsi obtenu a ensuite été traité par Arisem pour la phase d'extraction des entités : 34 000 occurrences d'EN ont été reconnues comme noms de personnes, organisations ou lieux, parmi lesquelles on trouve encore quelques erreurs (principalement des noms communs). De même que pour le corpus MNI, on restreint l'ensemble des entités traitées à celles apparaissant au moins deux fois : on dénombre alors 3278 entités, correspondant à 23725 occurrences.  Rappelons que notre tâche est centrée sur l'appariement et non la reconnaissance des EN, ce qui  signifie qu'on admet l'hypothèse que l'extraction des EN (en amont) est globalement "correcte". Nous n'avons pas cherché à corriger les erreurs de cette phase, puisque cela contredirait notre objectif de robustesse vis-à-vis des données disponibles.  Nous présentons ci-dessous quelques-unes des principales mesures fréquemment utilisées pour  l'appariement d'EN (Christen, 2006; Cohen et al., 2003; Bilenko et al., 2003).  La distance d'édition de Levenshtein (et variantes). Cette mesure de distance  d représente le nombre minimal d'insertions, suppressions ou substitutions nécessaires pour transformer une chaîne x en une chaîne y. Exemple : d(kitten, sitting) = 3 (k  s, e  i,   g). La similarité s(x, y) normalisée sur [0, 1], est définie par s = 1  d/max(|x|, |y|).  La métrique de Jaro. Cette mesure est basée sur le nombre et l'ordre des caractères communs  entre deux chaînes. Étant données deux chaînes x = a . . . a et y = b . . . b , soit H = min(n, m)/2 : un caractère a de x est en commun avec y s'il existe b dans y tel que a = b et i  H  j  i + H. Soit x = a . . . a (respectivement y = b . . . b ) la séquence de caractères de x (resp. y) en commun avec y (resp. x), dans l'ordre où les caractères apparaissent dans x (resp. y). Toute position i telle que a = b est appelée une transposition. Soit T le nombre de transpositions entre x et y divisé par 2, la mesure de similarité de Jaro est définie  par :  Jaro(x, y) = 1 3 × |x | |x| + |y | |y| + |y |  T |y | .  Les mesures de type "sac de mots" ou "de n-grammes de caractères". Pour ces mesures, chaque  entité est traitée comme un ensemble d'éléments (les mots ou les n-grammes). Soient X = {x } et Y = {y } les ensembles représentant les EN x, y à comparer. Les mesures les plus simples ne prennent en compte que le nombre d'éléments en commun , par exemple :  Jaccard(x, y) =  |X  Y | |X  Y | ; Overlap(x, y) = |X  Y | min(|X|, |Y |) ; Cos(x, y) = |X  Y |  |X| |Y |   Certaines mesures plus élaborées s'appuient sur une représentation vectorielle des ensembles  X et Y , qui peut tenir compte de paramètres extérieurs aux ensembles eux-même. Soient A = (a , . . . , a ) et B = (b , . . . , b ) ces vecteurs , la similarité définie par le cosinus de l'angle  formé par  A et B est fréquemment utilisée : cos(A, B) = A B A × B . La représentation des  éléments par leurs poids TF-IDF (Term Frequency-Inverse Document Frequency) est l'une des  plus classiques. Il s'agit dans notre cas de mesurer l'importance d'un élément w pour une EN x parmi un ensemble E d'entités :  tf  = n n , idf = log |E| |{x  E|w  x}| , tf idf = tf × idf .  Les combinaisons de mesures. Leur principe est la combinaison des propriétés des différents  types de mesures présentés ci-dessus. Il s'agit généralement d'appliquer une "sous-mesure" sim aux mots des deux EN à comparer, puis d'en déduire un éventuel alignement optimal des EN. Il s'agit donc d'appliquer une méthode de type "sac de mots", mais sans subir la rigidité d'un test d'identité entre mots : par exemple, les entités "Director ElBaradei" et "Director- General ElBareidi" présentent des similarités importantes que les mesures "sac de mots" classiques ne prennent pas en compte. La sous-mesure doit bien sûr être choisie judicieusement. - La mesure de Monge-Elkan calcule simplement la moyenne des meilleurs paires de mots  trouvés :  sim(x, y) = 1 n max (sim (x , y )) - La mesure Soft-TFIDF proposée dans (Cohen et al., 2003) est une forme assouplie du cosinus sur les vecteurs de poids TF-IDF : grossièrement, deux mots différents peuvent être considérés comme identiques selon que leur score de sous-mesure dépasse ou non un seuil.  Enfin, on peut mesurer la similarité des contextes des EN. On nomme contexte d'une occurrence  d'une EN l'ensemble des n mots qui la suivent et qui la précèdent, et le contexte (global) d'une entité distincte est formé par l'union des contextes de toutes ses occurrences. De façon  classique (Pedersen & Kulkarni, 2007), nous calculons l'ensemble des vecteurs représentant le  contexte de chaque entité, chaque vecteur contenant les poids TF-IDF des mots de ce contexte. La similarité entre les contextes de deux EN est alors le cosinus de leurs vecteurs respectifs.  Nous avons implémenté un prototype d'évaluation de mesures de similarités entre EN. À partir  d'une liste d'entités et de leur contexte, celui-ci calcule le score de similarité obtenu par chaque couple d'EN pour un ensemble prédéfini de mesures. 48 mesures sont disponibles, dont une vingtaine proviennent de deux librairies publiques : SimMetrics de S. Chapman et SecondString de W. Cohen, P. Ravikumar et S. Fienberg.  Il est bien entendu nécessaire de disposer de données étiquetées, d'une part pour pouvoir tester  et comparer les performances des différentes mesures, et d'autre part pour pratiquer l'apprentissage supervisé. Cependant, la tâche d'appariement présente certaines spécificités qui rendent la phase d'étiquetage de données difficile. En effet, nous cherchons à classer des couples d'EN comme positifs (coréférence) ou négatif (non coréférence). Or pour n entités distinctes l'ensemble des couples potentiel comprend n × (n  1)/2 éléments, il serait donc excessivement coûteux en temps d'envisager l'étiquetage manuel de cet ensemble (pour les valeurs de n étudiées). Dans de telles circonstances, une technique usuelle consiste à n'étiqueter qu'un sousensemble de couples tirés aléatoirement. Mais cette alternative n'est pas envisageable ici, à cause de la disproportion entre couples positifs et négatifs : dans nos données, on ne trouve respectivement que 0,06% (pour MNI) et 0,02% (pour MIF) de couples positifs.  C'est pourquoi notre approche vise à extraire de façon semi-automatique un ensemble contenant  tous les couples positifs. Seul cet ensemble sera examiné au cours de l'étiquetage manuel. Cette approche repose sur l'hypothèse selon laquelle les couples positifs seront jugés similaires par au moins une des métriques ; à l'inverse, les couples qui ne sont bien classés par aucune métrique sont considérés négatifs. Cette méthodologie n'est pas sans biais, mais une analyse approfondie d'un ensemble d'entités nous a permis de constater que ce biais était en réalité faible, du fait de la multiplicité et de la diversité des mesures utilisées.  Les critères d'étiquetage manuel ainsi que les méthodes automatiques de recherche de couples  candidats ont été affinés pour le traitement du corpus MIF, grâce à l'expérience acquise avec le corpus MNI. C'est pourquoi nous ne détaillons ci-dessous que la méthode employée sur MIF, sachant que le changement principal réside dans une définition beaucoup plus stricte de la coréférence.  Pour la recherche de couples candidats, notre système propose d'abord les couples obtenant  les k meilleurs scores selon chaque mesure. Deux autres techniques sont également mises en oeuvre : la première consiste à appliquer automatiquement les relations de transitivité (si les EN A et B sont coréférentes et que B et C le sont aussi, alors A et C sont coréférentes). La seconde vise à repérer d'éventuels couples difficiles à trouver de façon globable (par exemple,  les EN courtes sont défavorisées par la majorité des mesures) : dans ce but, l'ensemble des EN  est parcouru, en proposant pour chaque EN les n entités les plus proches selon m "bonnes" mesures. Les couples sont classés en trois catégories : les positifs (coréférence stricte, au moins dans le corpus) ; les négatifs (non-coréférence stricte) ; les couples incertains (ne pouvant être acceptés comme positifs mais présentant toutefois un lien étroit ). Enfin certaines EN sont éliminées (principalement les erreurs de reconnaissance ou les EN mal formées, mais aussi quelques cas ambigus).  Par rapport au corpus MNI, plus de temps a été consacré à la recherche de couples coréférents  parmi les entités. En particulier, bon nombre d'acronymes ont été appariés manuellement avec leur forme étendue, ainsi que quelques cas tels que "Quai d'Orsay" et "Ministère des affaires étrangères . Le parcours d'appariement local, au cours duquel chaque EN est prise comme référence, a permis de repérer une douzaine de couples positifs supplémentaires parmi environ 30 000. Pour toutes ces raisons, nous pensons que la probabilité pour un couple positif de n'avoir pas été étiqueté est très basse.  Corpus  EN EN éliminées positifs négatifs incertains total Corpus MNI 1588 0 805 1 877 3 836 1 260 078 Corpus MIF 3278 745 741 32 348 419 3 206 778  Nous développerons dans la partie 4.1 les résultats obtenus par les mesures de similarité testées.  On peut néanmoins déjà déduire de leurs définitions (cf. partie 2.2) qu'elles ont chacune des propriétés spécifiques qui les rendent potentiellement complémentaires. Nous pouvons observer ces différences sur quelques cas positifs non triviaux issus du corpus MIF dans le tableau 3.2.  Ces exemples illustrent le fait qu'aucune de ces mesures n'est capable de prendre en compte  tous les types d'indices permettant de statuer sur l'éventuelle coréférence d'un couple d'entités. C'est pourquoi nous proposons d'utiliser l'apprentissage supervisé : nous espérons ainsi déterminer une manière optimale de combiner les scores obtenus à l'aide de différentes mesures, dans le but d'améliorer les performances de la tâche d'appariement d'EN. Dans les données fournies à l'algorithme d'apprentissage, chaque couple d'entités est représenté par un ensemble de paramètres choisis pour leur contribution potentielle à la détection d'une coréférence. Parmi ces paramètres figurent bien entendu les scores de similarité obtenus avec différentes mesures, mais aussi certaines caractéristiques du couple d'EN telles que leurs longueurs (en nombre de caractères et de mots) et leurs fréquences minimales et maximales. Nous utilisons le logiciel Weka (Witten & Frank, 2005) pour réaliser l'apprentissage , et testons deux méthodes de classification : La régression logistique, qui apprend un séparateur linéaire, et L'algorithme C4.5 (Quinlan, 1993), qui apprend un arbre de décision.  Conformément à l'approche que nous avons suivie pour l'étiquetage des données, les résultats  détaillés ci-dessous sont évalués sous les hypothèses suivantes : tout couple non étiqueté est assimilé à un couple négatif ; tout couple marqué comme "incertain" est simplement ignoré.  Tout d'abord, nous constatons que les mesures se comportent de façon similaire sur les deux  corpus. Des différences de performances importantes sont observées, mais celles-ci sont principalement dues aux critères d'étiquetage différents (voir partie 3.1).  La typologie des ressemblances reconnues par type de mesure laisse apparaître quelques grandes  lignes : sur les mots simples qui présentent de légères différences textuelles, souvent des noms de lieux ou de personnes, les mesures de type Levenshtein/Jaro sont performantes. Mais cellesci deviennent inadaptées dès que plusieurs mots sont présents, ce qui est essentiellement le cas des noms d'organisation mais aussi souvent des noms de personnes (e.g. avec/sans prénom/titre) : les mesures "sac de mots" sont alors nettement meilleures (voir table 3.2).  En général, les mesures qui obtiennent les meilleures performances sont de type "sac de mots"  ou "sac de n-grammes", tandis que les mesures basées sur les séquences de caractères, moins souples, ne permettent d'identifier sans erreur que les couples positifs très proches. Sans surprise, la prise en compte de l'IDF améliore assez nettement les résultats des mesures de type sac de mots/n-grammes. Individuellement, la mesure de similarité des contextes n'a pas d'intérêt .  4.2.1  Mesures individuelles  Dans le tableau 1 sont indiquées les performances obtenues par quelques-une des meilleures  mesures individuelles. Celles-ci sont calculées selon les deux méthodes d'apprentissage, de façon à pouvoir servir de référence par rapport aux combinaisons de mesures décrites ci-après. Dans ce même tableau nous évaluons l'apport des paramètres de longueur/fréquence des EN. On peut constater que les résultats sont très proches avec les deux méthodes dans le cas des mesures seules, tandis que le C4.5 tire beaucoup mieux profit des paramètres de longueur/fréquence : la F-mesure va jusqu'à augmenter de 26 (MNI) ou 15 (MIF) points pour la mesure de Jaro.  4.2.2  Combinaisons de mesures  Nous avons testé plusieurs sélections de mesures comme paramètres de l'apprentissage. Les  résultats de ces expérimentations pour deux sélections de mesures et quelques variantes sont fournis dans le tableau 2. On constate globalement une nette amélioration des performances par rapport au cas des mesures individuelles : en comparant les meilleurs cas des deux situations, le rappel passe ainsi de 69% à 83% sur le corpus MNI et de 62% à 75% sur le corpus MIF. C'est encore une fois l'algorithme C4.5 qui combine les différents paramètres de façon optimale.  En revanche, la contribution de la mesure de similarité des contextes, particulièrement étudiée  ici, est quasiment nulle. Cependant, en considérant un ensemble restreint de mesures de façon à analyser plus en détail cette mesure (tableau 2), on constate un apport faible mais significatif de celle-ci : l'algorithme C4.5 en permet un usage positif, puisque la F-mesure gagne 2,7 points  dans le cas où ce paramètre est ajouté à deux autres bonnes mesures. Un gain similaire est  observé entre l'une des mesures prise individuellement (tableau 1) et la même avec le contexte.  rithme C4.5 est nettement en faveur de ce dernier. Dans ce cadre, nous avons également étudié  l'apport d'une mesure de similarité des contextes, qui semble faible mais non négligeable.  L'inconvénient le plus important de cette méthode est certainement la nécessité de données  étiquetées, très difficiles et/ou coûteuses à obtenir à cause des spécificités de cette problématique (nombre de couples potentiels très élevé et disproportion entre positifs et négatifs). C'est pourquoi il nous semble pertinent d'étudier les possibilités d'apprentissage non-supervisé ou semi-supervisé (par exemple en sélectionnant judicieusement les couples à étiqueter).  Ces travaux ont été financés par le projet Cap Digital - Infom@gic. Nous remercions L. Rigouste  (Pertimm), N. Dessaigne et A. Migeotte (Arisem) pour nous avoir fourni le corpus MIF annoté.  
