{silvia.fernandez, eric.sanjuan, juan-manuel.torres}@univ-avignon.fr  Dans cet article, nous présentons des applications du système Enertex au Traitement Automatique de la Langue Naturelle. Enertex est basé sur l'énergie textuelle, une approche par réseaux de neurones inspirée de la physique statistique des systèmes magnétiques. Nous avons appliqué cette approche aux problèmes du résumé automatique multi-documents et de la détection de frontières thématiques. Les résultats, en trois langues : anglais, espagnol et français, sont très encourageants. In this paper we present Enertex applications to study fundamental problems in Natural Language Processing. Enertex is based on textual energy, a neural networks approach, inspired by statistical physics of magnetic systems. We obtained good results on the application of this method to automatic multi-document summarization and thematic border detection in three languages : English, Spanish and French. Énergie textuelle, Réseaux de neurones, Modèle de Hopfield, Résumé automatique, Frontières thématiques. Textual Energy, Neural Networks, Hopfield Model, Automatic Summarization, Thematic Boundaries.  Des idées empruntées à la physique ont déjà été utilisées dans l'analyse de textes. Les exemples  plus notables sont l'approche entropique de (Shannon, 1948), les travaux de (Zipf, 1935; Zipf, 1949) et de (Mandelbrot, 1953) où les auteurs font des considérations thermodynamiques d'énergie et de température dans leurs études sur la Statistique Textuelle. Dernièrement (Takamura et al., 2005) se sont servi des notions de polarisation des systèmes de spins pour trouver les orientations sémantiques des mots (désirable ou indésirable) à partir de mots amorce. La sortie de ce système est une liste de mots indiquant leurs orientations estimés selon l'approximation du champ moyen. Dans notre travail, nous avons utilisé différemment la notion de spin. Elle nous a permis de représenter les présences ( ) où absences () des mots dans les documents. À partir de cet image, on aperçoit le document comme un matériaux composé d'un ensemble de unités en interaction dont l'énergie peut être calculée. Nous avons étudié les problèmes du Traitement Automatique de la Langue Naturelle (TALN) en utilisant la notion d'énergie textuelle. Récemment introduite (Fernández et al., 2007a; Fernández et al., 2007b), l'énergie textuelle a été appliquée au résumé automatique et à la détection de frontières sur des corpus en français  et en anglais. Elle est aussi un des algorithmes utilisés dans (da Cunha et al., 2007) où des  méthodes statistiques et linguistiques sont combinées pour résumer des articles médicaux en espagnol. Dans cet article nous étudions l'influence de deux facteurs, inspirés aussi de la physique, sur l'énergie textuelle. Il s'agit d'un champ externe et de la température. Cette démarche a permis d'améliorer les performances du modèle. Les résultats sur des corpus multi-documents et trilingues (français, anglais et espagnol) sont très encourageants. Nous présentons dans la Section 2 une brève introduction au modèle neuronal de Hopfield ainsi que son extension au TALN. Nous appliquons l'énergie textuelle à deux tâches bien distinctes : la génération de résumés multi-documents guidés par une thématique dans la Section 3 et l'amélioration de l'algorithme de détection de frontières thématiques dans la Section 4. Finalement nous présentons les conclusions et quelques perspectives. La contribution la plus importante de Hopfield à la théorie des réseaux neuronaux a été l'introduction du concept d'énergie issu de l'analogie avec les systèmes magnétiques : systèmes constitués d'un ensemble de N petits aimants appelés spins qui peuvent s'orienter selon plusieurs directions. Le cas le plus simple est représenté par le modèle d'Ising, avec deux directions possibles : vers le haut ( , +1 ou 1) ou vers le bas (, -1 ou 0). Ce modèle a été utilisé dans une grande variété de systèmes qui peuvent être décrits par des variables binaires (Ma, 1985). Un système de N unités binaires possède  = 1, ..., 2 configurations (patrons) possibles. Dans le modèle de Hopfield, les spins correspondent aux neurones qui interagissent selon la règle d'apprentissage de Hebb :  J  = s s (1)  s  et s sont les états des neurones i et j. La sommation porte sur les P patrons à stocker. Ce modèle est aussi connu sous le nom de mémoire associative. Il possède la capacité de stocker et de récupérer un certain nombre de configurations du système, car la règle de Hebb transforme ces configurations en attracteurs (minimaux locaux) de la fonction d'énergie (Hopfield, 1982) :  E  =  1 2 s J s (2)  Si on présente un patron proche à , chaque spin subira un champ local h  = J s induit par les N spins des autres patrons µ. Les spins s'aligneront selon h pour restituer le patron stocké . Hopfield a démontré que l'énergie du système diminue toujours pendant le processus de récupération. Nous ne développerons pas la méthode de récupération de patrons , car l'intérêt porte sur la distribution et les propriétés de l'énergie du système. Cette fonction monotone et décroissante a été utilisée uniquement pour montrer que l'apprentissage est borné.  D'un autre côté, le modèle vectoriel de textes (Salton & McGill, 1983) transforme un document  dans un espace adéquat où une matrice S contient l'information du texte sous forme de sacs de mots. On peut considérer S comme l'ensemble des configurations d'un système dont on peut  calculer l'énergie. Les documents sont pré-traités avec des algorithmes classiques de filtrage de  mots fonctionnels , de normalisation et de lemmatisation (Porter, 1980; Manning & Schütze, 1999) afin de réduire la dimensionnalité. Une représentation en sac de mots produit une matrice S de fréquences/absences :  S  = [s ] = T F si le terme i existe 0 autrement (3)  où µ  = 1, · · · , P phrases et i = 1, · · · , N termes. La présence du mot i représente un spin s  avec une magnitude donnée par sa fréquence T F (son absence par  respectivement), et une phrase est donc une chaîne de N spins. Pour calculer les interactions entre les N termes du vocabulaire, on applique la règle de Hebb, qui sous forme matricielle se traduit par :  J  = S × S (4)  Chaque élément J   J est équivalent au calcul de (1). Enfin, l'énergie textuelle d'interaction (2) peut alors s'exprimer comme :  E  =  1 2 S × J × S (5)  Un élément E   E représente l'énergie textuelle entre les phrases µ et . La représentation sous forme de graphe (Fernández et al., 2007b) nous a permis d'expliquer la nature des liens que la mesure d'énergie textuelle induit. On a déduit qu'elle relie à la fois des phrases ayant des mots communs, ainsi que des phrases qui partagent un même voisinage sans pour autant partager nécessairement un même vocabulaire. C'est pour cette raison que l'énergie textuelle peut être utilisée comme mesure de similarité dans les applications du TALN. Nous avons développé l'algorithme Enertex basé sur cette mesure de similarité. Les premières applications ont porté sur le résumé mono-document genérique et sur la détection de frontières thématiques. Dans la section suivante nous montrons une modification qui consiste en mettre un champ externe en rapport avec un corpus multi-document. Cette stratégie permet de générer des résumés guidés par les besoins de l'utilisateur. Une autre approche, montrée dans la section 4, utilise l'énergie textuelle représentée comme un spectre de la phrase. Ceci permet la détection de frontières thématiques d'un document au moyen d'un test de concordance de Kendall. Nous montrons ici comment l'introduction d'une température modifie les spectres des phrases afin que le test de Kendall puisse mieux les identifier.  Les premiers systèmes de résumé automatique multi-documents ont été développés dans les  années 90 (McKeown & Radev, 1995). Les conférences DUC portant sur la tâche de résumé automatique sont organisées depuis 2001 par le NIST . La tâche principale de DUC consiste à traiter des questions complexes et réelles. Le type de réponse attendue ne peut pas être une entité simple (un nom, une date ou une quantité telle que classiquement défini dans les conférences TREC Question-Answering ). Le problème peut se poser comme ceci : étant donnée une thématique et un ensemble L avec D documents pertinents, générer un court résumé de 250 mots,  cohérent et bien organisé qui répondra aux questions de la thématique. Les D  = 25 documents proviennent du corpus A : articles d' Associated Press , New York Times (1998-2000) et Xinhua News Agency (1996-2000). L'évaluation de la qualité des résumés mono-document reste une tâche difficile. En multi-documents le problème n'est pas plus simple. Des approches manuelles et semi-automatiques ont été utilisées à ce propos. Ainsi Pyramid (Passonneau et al., 2005), Basic Elements (Hovy et al., 2005) et R (Lin, 2004) ont été employées. Plusieurs mesures manuelles ont été évaluées : cohérence, grammaticalité, non-redondance, pertinence au sujet, qualité linguistique. R est utilisée par la communauté comme mesure d'évaluation semi-automatique. Elle mesure l'intersection d'ensembles de n-grammes entre les résumés candidats et ceux de référence. Les métriques les plus populaires sont R -2 (bigrammes) et SU4 (bigrammes séparés par un intervalle  4 mots). Nous avons utilisé l'énergie textuelle pour la tâche de résumé guidé par une thématique ou sujet. L'idée est d'observer la réponse du système face à un champ externe. Ce champ, représenté par le vecteur des termes d'un texte décrivant un sujet a été mis en relation avec le corpus multi-document. La figure 1 illustre le processus d'obtention du résumé guidé par un sujet. Les documents sont concaténés dans un seul document et un prétraitement standard (filtrage et stemming (Porter, 1980)) lui est appliqué. L'énergie textuelle entre le sujet et les phrases du document concaténé est calculée selon :  E  (sujet, phrase) =  1 2 s J s (6)  Finalement, le résumé est formé avec les phrases présentant la plus haute énergie textuelle avec  le sujet. Un post-traitement de diminution de la redondance lui a été appliqué.  Diminution de la redondance  Dans un résumé multi-document il y a une probabilité significative de re-inclure l'information déjà présente. Pour diminuer ce problème il faut une stratégie de diminution de la redondance. Notre système ne possède pas un traitement linguistique et la stratégie d'anti-redondance consiste à comparer les valeurs d'énergie des phrases candidates et leur longueur. Nous supposons que (dans de grands corpus) la probabilité que 2 phrases aient les mêmes valeurs d'énergie est très faible. Ainsi, nous avons éliminé la présence de doublons (phrases avec exactement la même valeur d'énergie). Peut-on aller encore plus loin et détecter avec ce même critère des phrases égales à quelques mots près ? Pour le tester, on considère que si 2 phrases partagent la plus grande partie de leurs mots, elles apportent la même information. On construit donc le résumé avec la phrase la plus énergétique (en valeur absolue), puis la suivante dans le score (la candidate) fera partie du résume si |E  E |  . E est l'énergie de la phrase déjà présente.  La 3ème phrase candidate fera partie du résumé si  |E  E |   et si |E  E |  . Les énergies E et E sont considérées comme celles des phrases de référence. En général, une phrase candidate i sera ajoutée au résumé, si pour chaque phrase de référence (i  1) :  |E   E | = E  ; i = 2, 3, ... (7) Le cas contraire signifie que les énergies sont très proches avec une haute probabilité de redondance. On présente sur la figure 2 à gauche les valeurs du rappel du produit R -2 × SU4 pour différentes valeurs de E. Le meilleur résultat sur les corpus DUC'05-07 est obtenu avec E  0, 003. Cela correspond aux phrases à 2 mots près. Une autre stratégie permettant de  diversifier le contenu, consiste à écarter du résumé les phrases longues (dans les documents il y  a des phrases de taille  à celle du résumé demandé). On a défini la taille maximale de phrase comme k × M où M = nombre moyen de mots par phrase dans les documents originaux. Nous avons fait varier k par petits pas en mesurant à chaque moment le produit de R -2 × SU4. Le comportement est montré sur la figure 2 à droite. Le meilleur résultat est avec k  1, 6. Nous avons fixé k, puis le seuil d'énergie E = 0, 003 en maximisant le produit R -2 ×SU4. En DUC'07 il y avait 2 baselines : la 1ère est tirée au hasard. La 2ème est un système de résumé générique. La figure 3 montre la position d'Enertex comparé aux participants après les campagnes DUC'05-07. Le cosinus obtient des performances R étonnament hautes, mais qui peuvent donner lieu à des résumés avec beaucoup de redondance, car toutes les phrases selectionnées sont proches de la thématique. Par contre l'énergie textuelle capture l'information entre 2 phrases calculéé parmi toutes les autres. De ce fait, la similarité tient en compte pas uniquement du nombre de mots partagés (le recouvrement et le cosinus sont des mesures locales) mais des interactions indirectes (chemins de longueur 2).  Plusieurs stratégies ont été développées pour segmenter thématiquement un texte. On trouve  PLSA (Brants et al., 2002) qui estime les probabilités d'appartenance des termes à des classes sémantiques, des méthodes s'appuyant sur des modèles de Markov (Amini et al., 2000), sur une classification des termes (Caillet et al., 2004; Chuang & Chien, 2004) ou sur des chaînes lexicales (Sitbon & Bellot, 2005). Plus récemment, (Ferret, 2007) a proposé l'identification  préalable des sujets présentes dans le document comme stratégie pour améliorer la détection de  ruptures thématiques. L'identification de sujets est faite à partir d'une analyse contextuelle basée sur la co-occurrence de mots. L'idée est que si deux segments n'ont pas une forte cohésion lexicale entre eux, mais ils apparaissent dans le même contexte, alors ils appartiennent au même sujet et la rupture thématique n'existe pas. Dans ce travil, nous avons utilisé la matrice d'énergie E (5). Chaque ligne de cette matrice produit un spectre qui répresente l'interaction de la phrase i avec les autres. La figure 4 montre les spectres de quelques phrases d'un texte composé de deux thématiques. Étant donné que l'énergie textuelle détecte et pondère le voisinage d'une phrase, on constate une similarité entre les courbes de l'une (en gras) et de l'autre thématique (en pointillées). Pour comparer les spectres nous avons utilisé (Fernández et al., 2007a) le coefficient de concordance  de Kendall (Siegel & Castellan, 1988) et le calcul de sa p valeur qui permettent de définir un test statistique de concordance entre 2 juges qui classent un ensemble de P objets. Nous avons utilisé ce test pour trouver les frontières thématiques entre segments. Ces ruptures entre segments sont bien détectées si le voisinage commun entre les phrases est bien repéré. Mais il se trouve que des phrases chevauchant les thématiques présentent des courbes d'énergie que le test du  de Kendall s'avère incapable de distinguer. C'est le cas du spectre de la phrase 23 de la figure 4. Pour diminuer cet effet nous avons proposé (Fernández et al., 2007b) une variation du test de Kendall avec l'utilisation d'une fenêtre glissante : la phrase centrale est comparée aux autres dans la fenêtre Cette stratégie a permis une meilleure détection des ruptures. Mais nous pensons qu'on peut faire mieux. Dans ce travail nous introduisons une stratégie portant directement sur la modification des spectres des courbes : le lissage par un paramètre de bruit  qui peut être assimilé, en termes physiques, à l'inverse d'une température T .  Décroissance exponentielle : distance et température  La figure 4 montre que les spectres qui expriment correctement leur appartenance à une thématique ont une forme décroissante par rapport à un maximum. Ce maximum correspond à l'expression d'une forte interaction entre un couple de phrases. À partir de ce point maximal, les autres interactions diminuent rapidement jusqu'à la fin de la thématique. Cette décroissance de l'énergie textuelle peut être contrôlée avec un facteur exp où r est la distance entre la phrase µ et la phrase voisine qui présente la plus haute interaction avec elle et T un paramètre de bruit présent dans les spectres . La figure 5 montre le lissage induit dans les spectres pour deux  phrases de la figure 4 par le facteur  exp (la phrase 23 est difficile à classer en fonction de ses pics). Nous avons diminué T progressivement afin d'analyser l'évolution du chevauchement des courbes. Cette diminution lisse les courbes de façon efficace : à T  8 le bruit de la courbe 23 est réduit et un classement correct a été obtenu. Le spectre de la phrase 10 a aussi été lissé sans perte d'information. Nous faisons l'hypothèse que avec ce lissage, le test de concordance  La mesure -Front   (Pevzner & Hearst, 2002) ont montré que WD est peu sensible aux variations de la taille de  segments et plus équilibré que d'autres mesures dans la pénalisation des erreurs. Cependant elle a ses faiblesses. WD ne peut pas être assimilée à un taux d'erreur (sa valeur peut être > 1) et elle n'est qu'un élément de comparaison de la fiabilité des méthodes et non un paramètre absolu de sa qualité (Sitbon & Bellot, 2004). De plus, nous avons trouvé qu'une même valeur de WD pouvait correspondre aux segmentations différentes du document. Nous introduisons ici la mesure -Front qui calcule la distance euclidienne d () entre les vecteurs A et B de dimension P (nombre des phrases du document) : A correspond aux frontières véritables et B à celles détectées. La valeur de la composante i est le nombre de phrases séparant la phrase i de la frontière la plus proche (figure 7). Le facteur de normalisation est calculé avec le vecteur nul : ne contenant aucune frontière sauf les extrêmes. Plus la valeur -Front est basse, mieux la segmentation a été réalisée.  -Front(A,B)  = d (A,B) d (A,C) (8)  On observe au tableau 1 que la valeur de T pour la meilleur segmentation dépend de la longueur   du document. Plus la taille du segment est grande (plus le document est long) plus la valeur T  est elevée. Les deux mesures ne sont pas toujours en accord. En français WD obtient la valeur la plus haute pour des segments de taille 9-11 et -Front pour 3-5. Cette différence peut être due  au nombre de véritables frontières trouvées : -Front considère plus finement ce facteur. Les  méthodes cités rapportent des meilleures performances en anglais qu'en français, peut être dû aux différences structurales et de répétition de mots entre ces langues. Cependant nos résultats sont comparables dans les 3 langues. Cette stabilité découle du calcul d'interactions des mots combiné au processus de comparaison de segmentes. (Ferret, 2007) constate en partie cet effet.  
