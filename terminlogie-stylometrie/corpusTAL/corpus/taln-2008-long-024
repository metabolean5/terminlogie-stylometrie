Il y a un certain nombre d'idées fausses qui circulent parmi les chercheurs en TA, et freinent à notre   avis les progrès dans ce domaine. La première est que la plupart des systèmes opérationnels utilisent la  TA statistique, alors que la plupart (voir le Compendium (Hutchins & al. 2005) publié par l'EAMT)  utilisent  des  méthodes  « expertes »  (« à  règles »,  mais  pas  seulement  à  règles).  L'autre  est  que  les  systèmes  utilisant  un  « pivot  interlingue »,  évidemment  très  adapté  à  la  communication  multilingue,  sont  nécessairement  « à  règles »  (TAFR,  en  anglais  RBMT  ou  « rule-based  MT »),  et  donc  très  coûteux à construire (ce « donc » est faux aussi...).   Il ne faut pas faire l'amalgame entre l'architecture linguistique d'un système de TA, caractérisée par les  représentations  intermédiaires  qu'il  utilise  durant  le  processus  de  traduction,  et  son  architecture  computationnelle, caractérisée par les méthodes de calcul et les ressources utilisées dans ses diverses  « phases » transformant une représentation intermédiaire en sa suivante dans le processus.  Après une brève partie consacrée aux définitions des variantes de ces architectures, nous montrerons  que,  pour  à  peu  près  chaque  architecture  linguistique,  on  trouve  des  systèmes  utilisant  diverses  architectures  computationnelles.  De  plus,  une  bonne  partie  des  systèmes  utilisent  plusieurs  architectures  computationnelles  dans  leurs  différentes  phases.  Nous  essaierons  enfin  de  dégager  quelques indications sur les choix d'architecture appropriés aux diverses situations traductionnelles et  des ressources disponibles, en termes de dictionnaires, de corpus, et de compétences humaines.   Ces architectures correspondent aux « chemins » dans le fameux « triangle de Vauquois ».       Les systèmes directs n'utilisent que deux représentations, le texte d'entrée et le texte de sortie. Pour les  langues  ayant  des  systèmes  d'écriture  à  séparateurs  de  mots  ou  de  syllabes,  le  texte  d'entrée  n'est  souvent  pas  strictement  le  flot  de  caractères  tel  quel,  mais  une  suite  de  « mots  typographiques »  séparés  grâce  à  des  règles  simples.  Les  systèmes  semi-directs  ont  une  phase  de  segmentation  ou  d'analyse morphologique, voire morphosyntaxique, et une phase de génération morphologique. C'est le  cas des systèmes de « première génération » (russe-anglais aux USA et anglais-russe en URSS dès les  années 1950), et un certain nombre de systèmes commerciaux actuels sont toujours de ce type.  Il existe au moins 7 variantes des systèmes à transfert. La structure obtenue en fin d'analyse peut être  syntagmatique (basée  sur  des  constituants la  plupart  du temps  connexes),  ou  bien  dépendancielle,  et  dans  ce  cas  surfacique  (fonctions  syntaxiques  comme  sujet,  objet  direct,  épithète,  attribut...)  ou  profonde (relations sémantiques comme agent, patient, cause, concession...). Les systèmes à transfert  profond fondés sur les théories de Tesnière, puis de l'Ecole de Prague et de celle de Moscou, utilisent  des représentations logico-sémantiques distinguant les arguments des circonstants.                                                              On  dit  qu'il  y  a  « transfert  lexical »  quand  on  passe  directement  de  « l'espace  lexical »  de  la  langue   source à celui de la langue cible. Par « espace lexical », on entend tout le système lexical, qui va des  « formes » de surface aux « acceptions », en passant par les lemmes et éventuellement par les « unités  lexicales » (familles dérivationnelles) ou « prolexèmes » (les mêmes, un peu élargies).  Le « pivot hybride » (terme dû à Shaumyan) des systèmes du CETA des années 1965-70 était un type  de représentation utilisant des attributs et relations interlingues, et des unités lexicales de chacune des  langues. Ces systèmes étaient donc à transfert simple, alors qu'on a un double transfert en « pivot ».  Les structures multiniveau de Vauquois sont basées sur un graphe syntagmatique abstrait (suppression  des auxiliaires, regroupement de lexèmes discontinus comme give...up, etc.), lexicalisé (chaque noeud  interne  domine  un  « gouverneur »  lexical),  et  contenant  aussi  bien  les  informations  et  relations  profondes que celles de surface. De telles structures  sont « génératrices » des structures mononiveau  usuelles, et offrent une sorte de « filet de sécurité ».   Les systèmes à véritable interlingua (comme ATLAS-II de Fujitsu ou PIVOT/Crossroads de NEC, ou   KANT/CATALYST  de  CMU/Caterpillar,  ou  UNL,  ou  MASTOR-1  d'IBM,  ce  dernier  en  TA  de  parole) utilisent 3 espaces lexicaux, car un véritable interlingua possède son propre vocabulaire, même  si ce vocabulaire est construit comme union des acceptions  d'un certain nombre de langues, comme  UNL  (Uchida  1996,  2004).  Dans  les  systèmes  de  TA,  il  existe  des  interlinguas  « linguisticosémantiques »  (comme  KANT,  ULTRA,  UNL)  dont  les  « lexèmes »  sont  construits  à  partir  des  lemmes  et  des  lexies  de  dictionnaires  d'une  ou  plusieurs  langues  naturelles,  et  des  interlinguas  « sémantiques » ou « sémantico-pragmatiques », dont les lexèmes sont construits à partir des entités,  propriétés, actions et processus  d'un domaine précis  et d'un  ensemble  de  tâches  bien  identifiées  (par  exemple, réservation touristique).  Enfin, si la plupart des systèmes de TA ont comme « unité de traduction » le « segment » (phrase ou  titre) des systèmes d'aide au traducteur utilisant des mémoires de traductions, certains ont des unités de  traduction  de  l'ordre  de  la  page  (Ariane-G5),  ce  qui  permet  de  mieux  traiter  certains  phénomènes  comme la concordance des temps et de résoudre des anaphores hors du contexte de la phrase.   Pour  ce  qui  est  des  processus  automatiques,  on  distingue  entre  méthodes  expertes  et  méthodes   empiriques. Il y a aussi des distinctions à faire si le processus de traduction est interactif.   1.2.1  Méthodes « expertes »   Les  méthodes  « expertes »  sont  plus  ou  moins  procédurales  ou  déclaratives,  et  font  appel  à  de  la  programmation  directe  ou  fondée  sur  des  « modèles  de  calcul »  abstraits,  d'où  l'utilisation  de  LSPL  (langages spécialisés pour a programmation linguistique). On a en bref :     la programmation directe dans un  langage algorithmique  classique  (souvent  employée  au niveau   des traitements typographiques ou morphologiques).     la programmation directe dans un langage de haut ou très haut niveau (Lisp, Prolog) offrant des   structures  de  données  et  de  contrôle  plus  adaptées  à  la  programmation  linguistique,  mais  demandant une grande expertise en programmation.     la  programmation  dans  des  LSPL  d'automates  (comme  les  transducteurs  finis,  les  ATN  ou  les   transformateurs d'arbres, abusivement dits « grammaires » transformationnelles).     la  programmation  dans  des  formalismes  de  grammaires  déclaratives  (ou  presque)  comme  LFG,   GPSG, HPSG, ou TAG.                                                                                                                                                                         Il  est  abusif  de  parler  de  systèmes  « à  règles »  pour  les  deux  premiers.  Ainsi,  Systran  utilise  des  automates  (transducteurs  d'états  finis)  pour  l'analyse  morphologique,  tandis  que  l'analyse  syntaxique  n'est pas faite par « règles », mais par un programme instanciant un schéma procédural fixe (écriture  de « macros » déterminant des décisions locales par examen d'une « fenêtre courante » sur un graphe  sans boucle représentant la phrase).     1.2.2  Méthodes empiriques   Ce sont les méthodes fondées sur les corpus :      TA statistique (SMT) et TA statistique à syntagmes (PSMT, ou « phrase-based » SMT),     TA fondée sur les exemples (EBMT), avec 3 variantes.  Notons  que  « TA  statistique »  est  un  assez  mauvais  terme,  car  on  devrait  plutôt  parler  de  TA  « probabiliste ». En effet, un « modèle de langage » est une collection de probabilités estimées d'après  des comptages sur de gros ou très gros corpus.   La  différence  essentielle  entre  SMT  et  EBMT  est  que,  en  EBMT,  les  exemples  sont  utilisés   directement  durant  le  processus  de  traduction,  tandis  que  la  SMT  utilise  les  résultats  d'une  sorte  de  gigantesque « compilation » de l'ensemble des exemples (corpus aligné).  Les variantes de l'EBMT sont les suivantes :     En EBMT classique, on étend les techniques de recherche de segments voisins des systèmes d'aide   aux  traducteurs  avec  mémoire  de  traductions,  et  on  propose,  pour  les  mots  différenciant  le  segment  à  traduire  et  le  segment  trouvé,  des  remplacements  venant  d'autres  exemples  ou  de  dictionnaires. Le système Similis(dérivé de (Planas 1998)) d'aide au traducteur en est proche.     En EBMT par analogie (Lepage & Denoual 2005), si S1 est le segment à traduire (en langue L1),   on  cherche  les  « rectangles  analogiques »    tels  qu'on  dispose  des  exemples  de  traduction    et  on  résout  en    (dans  la  langue  L2)  l'équation  analogique  . On obtient en général plusieurs traductions  , qu'on filtre pour la fluidité  par un modèle n-gramme. Si on ne trouve pas de tel rectangle, on résout en   (dans la langue L1)  l'équation    et  on  continue  récursivement.  Il  n'y  a  donc  pas  de  « décomposition  en  morceaux qui se correspondent » puis de « recomposition ».     Dans  le  système  EBMT  par  exemples  de  correspondances  structurées  de  Al-Adhaileh  et  Tang   (USM,  Penang),  on  utilise  un  corpus  parallèle  annoté  par  des  S-SSTC  (correspondances  chaînearbre  structurées  synchronisées).  La  traduction  se  fait  par analyse-synthèse.  Une  correspondance   est  élémentaire  ou  composée  (=  .  Quand on en trouve une car on a identifié un morceau C1 du segment S1 à traduire, ou bien les  correspondances la constituant, on a d'un seul coup les 3 autres éléments et leur synchronisation.    Voici maintenant une étude synthétique (non exhaustive) des architectures computationnelles utilisées   dans  des  systèmes  de  TA  basés  sur  11  architectures  linguistiques  différentes.  Pour  la  clarté,  nous  utilisons des tableaux, organisés de la  façon la plus homogène possible. Il n'a  malheureusement pas  été possible de suivre la suggestion d'un relecteur, et de faire un seul grand tableau croisant les deux  architectures,  car  trop  de  systèmes  utilisent  différentes  architectures  computationnelles  dans  différentes phases du traitement. Pour des raisons de place, il n'a pas non plus été possible de mettre  autant  de  références  qu'on  l'aurait  souhaité.  D'un  autre  côté,  les  références  sur  les  systèmes  opérationnels (commerciaux comme Systran, ATLAS, The Translator, Honyaku-no-oo-sama, ProMT,  Softissimo,  Tracy,  PIVOT/Crossroads,  ALTFlash,  METAL/Compendium,  LanguageWeaver,  etc.,  et  non  commerciaux  ou  semi-commerciaux  comme  PAHO-MTS,  ALT/JE  ou  Google  Translator)  sont  très rares et souvent anciennes. Le « Compendium » (Hutchins & al. 2005) est une source importante,  mais ne donne pas de détails précis sur la façon dont les systèmes cités sont construits.   Le  plus  souvent,  ces  systèmes  sont  « empiriques »,  mais  certains  utilisent  une  approche  « experte »,  comme ATLAS-I (différent de ATLAS-II).   GlobaLink a été fait à partir d'une copie de Spanam-1. Spanam-2 est de type expert (ATN).      Les systèmes actuels de Google sont (sans doute) plus PSMT (phrase-based SMT) que SMT.   Les systèmes récents de type PSMT de LanguageWeaver sont sans doute aussi de ce type.    Systran  est  très  ancien  (1966),  mais  depuis  1990  environ  il  intègre  des  FST  pour  les  traitements   morphologiques, et les macros utilisées pour la suite du traitement sont développées en C et plus en  assembleur.  Dans  JETS  (ancêtre  de  Honyaku  no  oo-sama,  actuellement  commercialisé  par  IBMJapon), les dépendances sont les « cas profonds » correspondant aux particules casuelles du japonais.  La différence avec les systèmes précédents est que le transfert produit une structure de même nature  que ce que produirait l'analyse de l'unité de traduction cible. Cela permet éventuellement de composer  deux  systèmes  de  TA  en  perdant  beaucoup  moins  d'information  et  en  introduisant  beaucoup  moins  d'erreurs qu'en mettant bout à bout deux systèmes complets, i.e. en passant par un « pivot textuel ».   LMT (MacCord, IBM), est rangé ici car les « slots » correspondent à des fonctions syntaxiques.    Passer  d'une  architecture  à  transfert  descendant  à  celle  de  transfert  « horizontal »  a  été  très  difficile   (communication  personnelle  de  K. Eberle  de  Linguatec  à  COLING-2000).  Cela  a  été  aussi  tenté  sur  METAL (par Siemens puis Sietech), mais sans succès.  Ici,  le  transfert  produit  une  structure  multiniveau  « génératrice »  dans  laquelle  les  informations  non  interlingues correspondent à celles de la langue source de façon « contrastive », et sont à utiliser par le  générateur  comme  des  préférences  ou  des  ordres  en  fonction  des  valeurs  de  certains  attributs  « tactiques ».  La  première  phase  de  l'étape  de  génération  consiste  alors  à  « sélectionner  une  paraphrase » en recalculant les informations de surface.  On pourrait ajouter les systèmes du CETA (1962-70), à « pivot hybride », décrit plus haut.   Il s'agit de systèmes utilisant un interlingua muni d'un vocabulaire « autonome ».    Les  graphes  UNL  sont  « linguistico-sémantiques ».  Le  vocabulaire  (UW)  est  l'union  des  acceptions   des différentes langues traitées, comme dans ULTRA, mais les relations sémantiques et le traitement  des idiomes sont liés à l'anglais (et tant mieux, car les langues voient assez souvent différemment les  relations sémantiques dans des énoncés synonymes).    Ces systèmes sont les seuls à faire de la « compréhension explicite », leur interlingua étant « projeté »   dans une ontologie , soit de façon séparée, soit de façon interne.  L'IF (interface format) réfère à une ontologie implicite, pas explicite.    Le tableau suivant donne une estimation des ressources nécessaires pour construire un système de TA   en fonction de la difficulté de la tâche, grossièrement estimée à partir de la taille moyenne des phrases.  Les coûts sont donnés ici en homme*année (h*a), M veut dire « million », et K « mille ».     Pour la TA empirique, il s'agit de la taille du corpus, en mots, pages (de 250 mots), phrases, et du   temps  humain  de  préparation  de  ce  corpus.  S'il  s'agit  de  traduction,  nous  utilisons  le  taux  professionnel de 1h/page (avec la révision, ce serait 1h20  par page).  S'il  s'agit  d'annotation, les  coûts  ne  sont  la  plupart  du  temps  pas  publiés,  et  nous  utilisons  des  informations  dont  nous  disposons par communications personnelles. Le coût par page est bien plus élevé, mais le corpus  peut être beaucoup plus petit, et finalement bien moins coûteux, pour de meilleurs résultats.     Pour la TA experte, il s'agit de la taille des dictionnaires et des grammaires, et du travail d'experts   humains. Contrairement à ce qu'on lit dans de nombreux cours sur la TA qu'on peut glaner sur le  Web, ce coût est souvent très surévalué, et pas seulement par les tenants des méthodes empiriques.    1.  Il  est  clair  que,  plus  les  corpus  sont  « bruts »,  plus  ils  doivent  être  grands.  Même  à  raison  de   15h/page de travail humain, il semble intéressant d'utiliser une méthode comme celle de l'USM à  Penang, car on n'a besoin que de 1000 pages et d'un gros dictionnaire assez simple.  2.  D'autre part, la SMT (et la PSMT) sont en fait adaptées à des « niches de riches », tout comme la  TA « experte » pour sous-langages. En effet, il y a très peu de corpus parallèles disponibles de 200  à  800  K  pages !  Du  point  de  vue  des  corpus,  les  différences  entre  couples  de  langues  « bien  dotés » et « mal dotés » sont encore plus grandes qu'en ce qui concerne les dictionnaires.  3.  Créer de très gros corpus parallèles à partir de zéro est 2 à 3 fois plus coûteux que de construire un  grand système de TA par approche experte (procédurale et/ou à automates et grammaires).  4.  L'architecture  linguistique  par  « pivot  interlingue »  peut  utiliser  n'importe  quel  paradigme  computationnel, qu'il soit statistique, analogique, à règles, ou hybride.   5.  En  dernier  ressort,  le  choix  de  l'architecture  linguistique  et  de  l'architecture  computationnelle   dépend des ressources disponibles en termes de corpus préalablement traduits, et d'humains plus  ou moins experts. Les types d'expertise recherchée sont, par ordre de difficulté croissante (estimée  via  le  temps  de  formation  et  la  relative  rareté  des  experts) :  la  traduction,  la  post-édition,  la  correction d'annotations, l'annotation à partir  de rien,  la terminologie,  la  lexicographie  complexe   (vocabulaire général et tournures), l'écriture de grammaires assez déclaratives, la programmation   par automates dans des LSPL adaptés, et enfin la programmation directe.    Nous  avons  donc  montré  que  les  architectures  linguistiques  et  computationnelles  des  systèmes  de   traduction  automatique  sont  indépendantes,  au  sens  où  on  peut  utiliser  n'importe  quelle  architecture  computationnelle pour réaliser n'importe quelle phase de traitement dans une architecture linguistique  donnée, non seulement en théorie, mais en pratique, comme l'illustre la variété des systèmes cités en  exemple. Nous avons aussi donné une évaluation des tailles et des coûts de construction des ressources  utilisées  par  différents  types  de  systèmes  de  TA,  ce  qui  donne  quelques  éléments  pour  le  choix  de  l'architecture  linguistique  et  computationnelle  d'un  système  à  créer,  en  fonction  des  situations  traductionnelles  et  des  ressources  disponibles,  en  termes  de  dictionnaires,  de  corpus,  et  de  compétences humaines.  Cette  réflexion  ouvre  sur  une  perspective  plus  générale  et  « sociétale ».  Si  l'on  veut  surmonter  la  « barrière  linguistique »  entre  toutes  les  langues,  on  ne  pourra  pas  se  contenter  de  construire  des  systèmes  de  TA  entre  l'anglais  et  les  autres  langues,  même  pas  pour  le  tchat  entre  deux  langues  différentes de l'anglais. En effet, l'anglais intermédiaire serait nécessairement trop « grossier », entaché  d'erreurs,  et  porteur  d'ambiguïtés  nouvelles  en  sus  des  anciennes  (celles  de  la  langue  source).  La  plupart  des  locuteurs  (ou  simplement  lecteurs  « passifs »)  seront  de  plus  toujours  bien  moins  compétents et à l'aise en anglais que dans leur langue.   Il  faudra  donc  construire  des  systèmes  fondés  sur  des  interlingues,  soit  « sémantico-pragmatiques »  (comme  l'IF  de  CSTAR,  Nespole!  ou  MASTOR-1)  s'il  s'agit  de  tâches  et  de  domaines  restreints  et  bien  identifiés,  soit  « linguistico-sémantiques »  (comme  UNL).  Cela  sera  d'autant  plus  nécessaire  qu'on  voudra  intégrer  ces  systèmes  au  « Web  sémantique »,  car  il  faudra  alors  demander  aux  internautes  d'aider  les  systèmes  d'annotation,  sans  doute  par  le  même  type  de  « désambiguïsation  interactive »  que  celui  qui  permet  de  compenser  la nécessaire  « rusticité »  (ou  la  « mauvaise  qualité  intrinsèque ») des systèmes de TA « tout terrain » quand on veut les utiliser en « tout automatique ».  Il ressort de ce qui précède qu'il devrait être possible de construire des systèmes de TA entre toutes les  langues,  passant  par  un  niveau  sémantique  comme  UNL,  non  seulement  par  des  approches  « expertes » comme c'est le cas actuellement, mais par des approches empiriques moins coûteuses et  moins  longues  en  développement,  si  toutefois  on  disposait  de  corpus  adéquats  de  taille  suffisante.  D'autre  part,  à  la  lumière  des  développements  récents  en  alignement  et  en  TA  statistique,  de  tels  corpus  devraient  pouvoir  être  construits  par  « transitivité »,  en  alignant  des  corpus  parallèles  et  des  corpus annotés en IL (en UNL par exemple) s'ils ont au moins une langue en commun.    
