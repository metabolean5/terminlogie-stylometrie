Le développement manuel d'un lexique précis et à large couverture est une tâche fastidieuse,  complexe et sujette à erreurs nécessitant une coûteuse expertise humaine. Les développements manuels de lexiques n'atteignent généralement pas les objectifs attendus et progressent très lentement une fois un certain niveau de couverture et de qualité atteint. Cette tâche manuelle peut cependant être simplifiée et améliorée par l'utilisation d'outils automatisant les tâches d'acquisition et de correction. Nous présentons un ensemble combiné de techniques permettant de détecter les entrées manquantes, incomplètes ou erronées d'un lexique et de proposer des corrections. La chaîne logique du processus global se résume ainsi :  1. Donner en entrée à un analyseur syntaxique un grand nombre de phrases non-annotées  considérées comme respectueuses de la langue, afin d'attribuer un échec d'analyse aux manques de l'analyseur et non aux textes qu'il reçoit en entrée .  2. Pour chaque phrase non-analysable, tenter de déterminer automatiquement si l'échec  d'analyse est dû à des manques de la grammaire ou du lexique utilisés par l'analyseur.  3. Suspecter des entrées lexicales d'être manquantes, incomplètes ou erronées.   4. Générer des hypothèses de correction en observant les attentes de la grammaire vis à vis  des formes suspectées lors des analyses de phrases dans lesquelles elles apparaissent.  5. Évaluer et classer les hypothèses de correction afin de procéder à une validation manuelle.  Bien que tous nos exemples et résultats soient liés à la langue française, cet ensemble de techniques est indépendant du système, c.a.d, il est facilement adaptable pour la plupart des étiqueteurs syntaxiques, classifieurs d'entropie, lexiques et analyseurs profonds existants, et par conséquent, à la plupart des langues informatiquement décrites.  Cet ensemble de techniques est l'un des points de départ du récent projet Victoria  , dont le but est de développer un ensemble d'outils permettant la construction efficace de ressources morphologiques, lexicales et grammaticales. Ces travaux font suite aux travaux présentés dans (2007a; 2007b; 2008) où des modèles plus simples avait été décrits.  Pour des raisons de clarté, les résultats pratiques de chaque étape ainsi que les améliorations  possibles sont donnés conjointement à sa présentation. Nous commençons donc par décrire le contexte pratique de nos expériences (sect. 2). Nous décrivons ensuite chaque étape énumérée ci dessus (sections 3,4,5,6,7) et concluons (sect.8).  Nous utilisons un corpus journalistique français non-annoté extrait du monde diplomatique. Ce  corpus contient 280 000 phrases de 25 mots ou moins, totalisant 4,3 millions de mots.  Le lexique utilisé que l'on cherche a amélioré se nomme le Lefff  . Ce lexique morphologique et syntaxique à large couverture du français contenant plus de 600 000 entrées.  Deux analyseurs syntaxiques sont utilisés afin de générer des corrections :   - FRMG (French Meta-Grammar) se base sur une méta-grammaire abstraite avec des arbres  hautement factorisés (Thomasset & Villemonte de La Clergerie, 2005) compilée en un analyseur hybride TAG/TIG grâce au système D AL . - S LFG-F (Boullier & Sagot, 2006) est une grammaire LFG profonde efficace non probabiliste compilée en analyseur LFG par S L , un système basé sur S . Nous utilisons aussi de façon ponctuelle l'étiqueteur syntaxique MrTagoo (Molinero et al., 2007; Graña, 2000) et le classifieur d'entropie MegaM (Daumé III, 2004).  Nous partons des résultats d'analyse syntaxique d'un grand nombre de phrases. Certaines phrases  ont été pu être analysées, d'autres non. Les phrases analysables sont considérées comme couvertes lexicalement et grammaticalement (même si les analyses obtenues ne coïncident pas toujours avec leur sens véritable). Les phrases non-analysables sont par contre non couvertes lexicalement et/ou grammaticalement. Afin de générer des corrections pour un lexique, il nous est préférable d'isoler les phrases qui ne sont non-analysables que pour des raisons lexicales. Pour ce faire, nous cherchons d'abord à identifier les phrases grammaticalement non-analysables.  Cette étape est réalisée grâce à un classifieur d'entropie, c.a.d, un outil statistique qui permet  de calculer une adéquation (une entropie) entre les données qu'il reçoit à l'évaluation et les données sur lesquelles il a été entraîné (Daumé III, 2004).  Dans ce but, nous profitons du fait que les constructions syntaxiques sont plus fréquentes et  bien moins diverses que les formes lexicales. Celles non couvertes tendent donc à être récurrentes et systématiques dans les phrases grammaticalement non-analysables. Afin d'identifier ces constructions problématiques, nous entraînons un classifieur d'entropie de la sorte : - nous réduisons toutes les phrases à des séquences de 3-grams obtenues soit à partir des catégories syntaxiques pour les mots des catégories ouvertes (c.a.d., verbes, adjectifs, etc.) et soit à partir de formes lexicales pour les mots des catégories fermées (prépositions, déterminants, etc.) auxquelles nous rajoutons des marqueurs de début et de fin de phrase. - nous associons à chaque séquence une classe (analysable/non-analysable) correspondant au résultat de l'analyse de la phrase dont cette séquence a été extraite. Pour "je(cln) mange(v) une(det) pomme(nc)" nous générons donc une séquence de 3-grams d'entraînement: <deb-je-v> <je-v-une> <v-une-nc> <une-nc-fin> dont la classe est analysable.  Le classifieur différencie donc, à partir des  3-grams qui les composent, les phrases qui paraissent être grammaticalement analysables de celles qui ne le sont pas. Les phrases non-analysables déclarées comme grammaticalement analysables sont alors considérées comme lexicalement non-analysables.  Il est à noter que l'entraînement n'est pas optimal à cause de deux aspects. Premièrement, la  catégorie de chaque mot dans les phrases est obtenue par le biais d'un étiqueteur syntaxique. Les étiqueteurs ne sont clairement pas des outils parfaits. Cependant, leurs erreurs sont grammaticalement aléatoires car elles dépendent avant tout des formes lexicales rencontrées. Ce caractère aléatoire permet donc aux erreurs de ne pas trop perturber la cohérence globale de l'entraînement du classifieur d'entropie. Deuxièmement, les phrases non-analysables données en entraînement ne sont pas toutes grammaticalement non-analysables, certaines sont seulement lexicalement non-analysables. On l'entraîne donc en partie à considérer injustement des phrases comme grammaticalement non-analysables. Cependant, les calculs sur les 3-grams pré sents dans ces phrases injustement catégorisées sont contrebalancés par leur présence logique  dans des phrases analysables.  Pour évaluer cette technique, nous avons ôté 5% des phrases analysables à l'entraînement et  avons observé si le classifieur les déclare comme analysables. Les taux de précision avant la première session de correction, puis après la première, seconde et troisième session étaient respectivement de 92,7%, 93,8%, 94,1% et 94,9%. La précision du classifieur augmente logiquement car, après chaque session, certaines phrases dont l'analyse échouait pour des raisons lexicales deviennent analysables et ne perturbent donc plus l'entraînement. La génération des séquences de 3-grams étant la même pour l'ensemble des phrases, ces taux de précision devraient s'appliquer de façon équivalente aux phrases grammaticalement non-analysables. Finalement, le taux d'erreur de (pour l'instant) 5,1% est un manque considéré comme acceptable étant donné l'impact positif que l'étape de filtrage a sur nos techniques de détection. Puisqu'il n'y a pas de raison pour qu'une forme particulière se retrouve plus que de raison dans des phrases classifiées incorrectement, il est possible de contrebalancer la perte de certaines phrases par une simple augmentation de la taille du corpus donné en entrée. La détection d'entrées lexicales douteuses est réalisée par le bais de deux techniques complémentaires qui identifient des formes et les associent à des phrases dont elles sont suspectées d'être responsables de l'échec d'analyse.  Nous appelons information lexicale de courte portée toute information pouvant être déterminée  par un étiqueteur syntaxique. Pour l'instant, nous ne considérons que la catégorie syntaxique.  Afin de détecter les problèmes lexicaux concernant ce type d'information, nous utilisons un  étiqueteur syntaxique configuré de façon particulière dont nous court-circuitons ponctuellement le lexique interne afin de le forcer à considérer comme inconnue, une à la fois, chaque forme d'une phrase. Nous nous reposons donc sur sa capacité à s'inspirer du contexte d'une forme pour supposer l'étiquette la plus probable. Les informations portées par ces étiquettes supposées sont ensuite comparées aux informations existantes dans le lexique. Si ces informations sont manquantes et concernent des classes ouvertes, la forme correspondante est déclarée comme suspecte. Appliquée aux catégories syntaxiques, cette technique nous permet de détecter les homonymes manquants d'un lexique en plus des formes totalement inconnues.  Bien entendu, les étiqueteurs commettent des erreurs, surtout lorsqu'on courcuite ainsi leurs  lexiques internes. La précision de l'étiqueteur modifié pour n'importe quel type de forme (même celles appartenant aux classes fermées) sur 5% des phrases non utilisées à l'entraînement est de 47,34%. Le nombre de faux positifs est donc très important. En étudiant les résultats, nous avons pu observer le caractère systématique de certaines erreurs. Par exemple, un nom propre est souvent considéré comme un nom commun, un participe passé comme un adjectif etc. Pour réduire le nombres de formes suspectées incorrectement, nous avons développé quatre surcouches profitant du fait que nous ne travaillions pas à l'échelle d'une phrase solitaire (comme le font généralement les étiqueteurs) mais d'un ensemble de phrases.  La première surcouche choisie simplement l'étiquette la plus fréquement donnée à une forme.   La deuxième surcouche calcule des patrons de réponses de l'étiqueteur par type d'étiquette et  par fréquence d'apparition de la forme (indexées sur les valeurs entières du logarithme népérien). On cherche donc durant l'entraînement à savoir, par exemple, combien de fois un nom propre est déclaré par l'étiqueteur comme nom propre, comme nom commun, comme adjectif, etc. A l'évaluation, nous calculons une affinité entre les nouveaux ensembles de réponses données par l'étiqueteur pour chaque forme et les patrons calculés à l'entraînement et on attribue ensuite à la forme l'étiquette du patron avec lequel elle a le plus d'affinité. Le calcul d'affinité Af f et le choix du meilleur patron Best se calculent ainsi :  Af f  = abs (P at  Rep ), Best = max(Af f  log (Occ ))  P at  et Rep sont la part d'une étiquette eti, et Occ est le poids d'émission du patron égale à la somme des occurrences des formes qui ont permis sa construction.  La troisième surcouche applique la même idée mais laisse le calcul d'affinité à un classifieur  d'entropie. Le classifieur est entraîné à reconnaître des patrons et à les associer à une classe représentant une étiquette et un index népérien.  La dernière surcouche s'appuie sur les trois premières pour réaliser un « vote à la crédibilité »où  l'« opinion » de chaque surcouche est valorisée à partir des taux d'erreurs par type de réponse.  Le défaut majeur de ces surcouches est qu'actuellement, elles considèrent que chaque forme  représente un seule lemme, ce qui est faux bien que vrai dans la grande majorité des cas.  L'étiqueteur est alors entraîné avec 50% des phrases du corpus, l'entraînement des surcouches  se réalise ensuite sur les réponses fournis par l'évaluation de l'étiqueteur sur 47,5% des phrases et leur évaluation sur 2,5% des phrases restantes. Les précisions respectives sur ces 2,5% de phrases de l'étiqueteur, de la première, seconde, troisième et quatrième surcouche sont respectivement de 40,17%, 43,6%, 77,61%, 74,09% et 89,78%. L'application de ces surcouches permet donc de passer d'une précision originelle de 47,34% de l'étiqueteur modifié à 89,78%, réduisant ainsi fortement le nombre de faux positifs.  Dans une première version basée uniquement sur l'étiqueteur sans surcouche et considérant  toute les formes comme inconnues en même temps, nous avons pu identifier 182 lemmes manquants. Cette nouvelle version nous a permis d'en trouver 358 autres. Le tout correspond a un total de 1168 formes lexicales, pour la plupart adjectifs ou noms propres manquants. Cette technique de détection de défauts lexicaux, décrite dans (Sagot & Villemonte de La Clergerie, 2006; Sagot & de La Clergerie, 2008), repose sur les hypothèses suivantes : - Si une forme lexicale apparaît plus souvent dans des phrases non-analysables que dans des phrases analysables, il est raisonnable de la suspecter d'être incorrectement décrite dans le lexique (van Noord, 2004). - Le taux de suspicion peut être renforcé si la forme apparaît dans des phrases non-analysables à côté d'autres formes présentes dans des phrases analysables.  L'avantage de cette technique par rapport à la précédente est sa capacité à prendre en compte  tout type d'erreurs lexicales. Cependant, puisque qu'elle part du précepte que toute phrase nonanalysable ne l'est que pour des raisons lexicales, la qualité de la liste de suspects fournie dépend directement de la qualité de la grammaire utilisée. En effet, si une forme spécifique est particulièrement liée à une construction syntaxique non couverte par la grammaire, on la retrouvera souvent dans des phrases non analysables et elle sera alors injustement suspectée. Nous atténuons ce problème de deux façons. Premièrement, nous excluons du calcul statistique toutes les phrases considérées comme grammaticalement non-analysables. Deuxièmement, comme cela a déjà été fait dans (Sagot & Villemonte de La Clergerie, 2006), nous combinons les résultats d'analyse fournis par différents analyseurs reposant sur des formalismes et grammaires différents, et donc avec des manques grammaticaux différents.  Cette technique nous a permis de détecter 72 lemmes décrits de façon incomplète correspondant  à un total de 1693 formes lexicales, pour la plupart des verbes. Pour l'instant, les deux techniques de détection identifient des formes lexicales. Il serait intéressant de monter au niveau du lemme en appliquant en post-traitement l'idée décrite dans (Sagot, 2005) où la validité d'un lemme est favorisée ou pénalisée suivant la présence ou l'absence de ses formes lexicales.  Autre amélioration possible, l'efficacité/l'intérêt de ces techniques ne sont valorisés que par  les corrections qu'elles ont permis de réaliser. Il serait intéressant d'établir une métrique afin d'évaluer la qualité des formes suspectes fournies. Cette métrique permettrait aussi de quantifier formellement l'impact positif de l'étape de filtrage sur l'étape de détection. Le classement à chaque session des formes corrigées pourrait être un point de départ.  La génération de corrections lexicales à partir du contexte grammatical a été utilisé pour la  première fois en 1990 (Erbach, 1990). Elle suit l'idée suivante : suivant la qualité du lexique et de la grammaire, la probabilité que ces deux ressources soient simultanément erronées au sujet d'une forme donnée dans une phrase donnée peut être faible. Si une phrase ne peut pas être analysée à cause d'une forme suspecte, cela implique que les deux ressources n'ont pas pu s'accorder sur le rôle que la forme peut avoir dans la phrase. Puisque que le problème est d'origine lexical, il est possible de générer des corrections en étudiant les attentes de la grammaire pour chaque forme suspectée lorsqu'elle analyse les phrases qui leur sont associées. De manière métaphorique, on « demande » à la grammaire son opinion sur les formes suspectées. Originellement, les formes suspectes étaient déterminées manuellement puis, à partir de 2006 (van de Cruys, 2006; Yi & Kordoni, 2006; Nicolas et al., 2007a; Nicolas et al., 2007b) , cette idée a été combinée avec des techniques de fouille d'erreurs telles que (van Noord, 2004; Sagot & Villemonte de La Clergerie, 2006; Sagot & de La Clergerie, 2008). Pour générer des corrections, nous nous approchons au mieux de des analyses que la grammaire aurait permises avec un lexique sans erreur. Puisque nous pensons que les informations lexicales associées à la forme suspecte ont coupé le chemin vers une possible analyse, nous diminuons les restrictions imposées par les informations lexicales : pendant l'analyse, chaque fois qu'une information lexicale associée à une forme suspectée est vérifiée, le lexique est court circuité et toutes les contraintes sont considérées comme satisfaites. La forme devient alors tout  ce que peut souhaiter la grammaire. En réalité, cette opération est effectuée en échangeant les formes suspectes dans les phrases associées par des formes sous-spécifiées appelées jokers. Si une forme a été correctement suspectée, et si c'est l'unique cause d'échec de certaines analyses de phrases, remplacer cette forme par un joker permet aux phrases de devenir analysables. Dans ces nouvelles analyses, des entrées « instanciées » du joker sont partie prenante des structures grammaticales produites en sortie. Ces entrées instanciées représentent les informations manquantes du lexique, nous les traduisons donc au format du lexique afin d'établir les corrections.  Comme expliqué dans (Barg & Walther, 1998), l'utilisation de jokers totalement sous-spécifiés  peut introduire une ambiguïté trop grande dans le processus d'analyse. Cela entraîne souvent des échecs d'analyse pour des contraintes de temps ou de mémoire (pas de corrections), ou des analyses surgénérative (trop de corrections). Contrairement à (Yi & Kordoni, 2006), où les auteurs utilisent les jokers totalement spécifiés les plus probables, nous n'ajoutons que peu d'information lexicale aux jokers et nous nous reposons sur la capacité de nos analyseurs à gérer des formes sous spécifiées. Pour des raisons pratiques, nous avons choisi d'ajouter aux jokers une catégorie syntaxique. L'ambiguïté introduite reste conséquente et aboutit généralement à un nombre important de corrections. Néanmoins cet aspect peut être facilement contrebalancé pour peu qu'il y ait assez de phrases non-analysables associées à une forme suspecte (voir sect 6). La catégorie syntaxique ajoutée aux jokers dépend de la technique de détection utilisée pour suspecter la forme. Lorsque nous utilisons la détection basée sur un étiqueteur, nous générons des jokers avec des catégories syntaxiques en accord avec les étiquettes fournies pour la forme. Quand nous utilisons l'approche de détection statistique, nous produisons des jokers avec les catégories syntaxiques déjà présentes dans le lexique pour la forme suspectée.  Le lecteur peut noter qu'un joker inadéquat peut parfaitement mener à de nouvelles analyses  et donc permettre la génération de corrections incorrectes. Nous commençons donc par séparer les corrections suivant le joker qui a permis leur génération.  Classification mono-analyseur.  À l'échelle d'une seule phrase, rien de permet de différencier les corrections valides des corrections erronées dont la génération résulte de l'ambiguïté introduite par les jokers. Cette ambiguïté ayant permis à l'analyse d'emprunter des règles de grammaires qu'elle n'aurait pas du. Le type de correction erronée dépend donc des règles de grammaires empruntées, c.a.d, de la structure syntaxique véritable de la phrase. Si la forme corrigée appartient à une catégorie ouverte, elle a de forte chance de pouvoir se retrouver au sein de structures variées. Par conséquent, plus le nombre de phrases est élevé, plus les structures syntaxiques au sein desquelles la forme est présente sont variées et plus les corrections erronées ont tendance a se disperser. Les correction valides, au contraire, tendent à être récurrentes.  Nous considérons donc toutes les corrections d'une forme w issue d'une même phrase comme  un groupe de corrections. Chaque groupe reçoit un poids P = c variant selon sa taille n, avec c une constante numérique entre ]0, 1[ proche de 1. Plus le groupe est grand, plus bas sera son poids car plus forte sera la probabilité qu'il soit la conséquence de squelettes syntaxiques permissifs. Chaque correction  du groupe reçoit ensuite un poids p = = . Tous les poids d'une correction sont finalement additionnés afin de calculer le poids global s =  p .  Classification multi-analyseur.  Étant donné que les corrections erronées générées dépendent des règles de grammaire empruntées durant les analyses, l'utilisation des résultats provenant de plusieurs analyseurs avec des grammaires différentes permet d'accentuer leur dispersion, alors que les correction pertinentes restent habituellement stables. Des corrections sont donc considérées comme moins pertinentes si elles ne sont pas proposées par l'ensemble des analyseurs. Nous obtenons donc séparément les corrections de chaque analyseur comme décrit ci dessus et fusionnons les résultats à l'aide d'une simple moyenne harmonique.  Contrairement à (van de Cruys, 2006; Yi & Kordoni, 2006), nous privilégions une approche  semi-automatique impliquant une étape de validation manuelle. Lors de la validation manuelle, nous avons identifié trois situations possibles.  Soit il n'y a pas de corrections : la détection des formes suspectes a été inadéquate ou la forme  suspectée n'est pas l'unique raison des échecs d'analyse associés. Soit il y a des corrections pertinentes : la forme a été correctement détectée, la forme est l'unique raison de (certains) échecs d'analyse associés. Soit il n'y a que des corrections erronées : l'ambiguïté introduite par les jokers a ouvert la voie vers des analyses erronées fournissant des corrections erronées. Si la grammaire ne couvre pas toutes les structures syntaxiques possibles, il n'y a aucune garantie qu'il y ait des corrections pertinentes produites.  Les résultats donnés dans (van de Cruys, 2006) démontrent clairement cet aspect : on peut y voir  que pour les catégories syntaxiques complexes comme les verbes, il est impossible d'appliquer un tel ensemble de techniques de façon automatisée sans nuire à la qualité du lexique. Si le but du processus de correction est d'améliorer la qualité du lexique et non pas d'augmenter artificiellement sa couverture, un tel processus devrait toujours être semi-automatique.  Comme nous l'expliquons plus loin, la validation manuelle n'est pas un très lourd tribut à payer.  De plus, elle ouvre la possibilité suivante : les lemmes sémantiquement reliés d'une même catégorie syntaxique tendent à avoir des comportements syntaxiques similaires. Cette similarité pourrait être utilisée pour attirer l'attention du correcteur ou même générer des corrections pour des formes non rencontrés/détectés.  Voici quelques exemples de correction validées :  - israélien, portugais, parabolique, pittoresque, minutieux étaient des adjectifs manquants ; - revenir ne traitait pas les constructions telles que revenir vers ou revenir de ; - se partager ne traitait pas les constructions telles que partager (quelque chose) entre ; - aimer était décrit comme attendant obligatoirement un COD et un attribut ; - livrer ne traitait pas les constructions telles que livrer (quelque chose) à quelqu'un. Le Tableau 1 donne les résultats de 4 sessions de correction. Les première et troisième sessions ont été réalisées avec la technique statistique de détection. La seconde avec une version brute de la technique de détection basée sur un étiqueteur et la quatrième avec la version décrite précédemment (voir sect 4).  Après ces quelques sessions, les techniques de détections nous fournissent encore des formes  suspectes mais nous n'obtenons plus de nouvelles corrections valides. Cela peut s'expliquer par plusieurs raisons. Bien que peu probable, les phrases non analysables restantes peuvent possé der deux formes erronées ; l'introduction d'un seul joker ne suffit donc pas à rendre la phrase  analysable. On peut aussi penser que les couvertures de nos grammaires sont insuffisantes, elles ne sont donc pas en mesure de nous fournir de nouvelles corrections. Cette dernière explication est privilégiée car, après la dernière session, l'étape de filtrage des phrases non analysables a classifiée l'essentiel des phrases restantes comme grammaticalement non analysables. Des sessions de correction futures n'auront donc de sens qu'après des améliorations des grammaires ou l'application à de nouveaux corpus. Cependant, cette constatation nous met en mesure de produire des corpus globalement représentatifs de manques grammaticaux. Si une technique était capable d'utiliser ce corpus pour suggérer des corrections grammaticales, la mise à jour de la grammaire nous permettrait de générer à nouveau des corrections pour le lexique. Ce qui à nouveau nous permettrait de générer un corpus représentatif des manques de la grammaire et ainsi de suite. Il serait alors possible de mettre au point un processus itératif améliorant alternativement et incrémentalement la grammaire et le lexique. Le modèle d'entropie construit par le classificateur pourrait être un bon point de départ pour établir les manques d'une grammaire.  Pour résumer nos résultats, nous avons déjà détecté et corrigé 612 lemmes correspondant à 2861  formes. Il est important de noter que ces corrections ont été obtenues après seulement quelques heures de travail manuel. L'aspect semi-automatique de notre approche n'est donc pas un très lourd tribut à payer.  Depuis ses premières versions (Nicolas et al., 2007a; Nicolas et al., 2007b), cet ensemble de  techniques a fortement évolué et les résultats obtenus démontrent sa cohérence et sa viabilité. Les améliorations prévues devraient renforcer ces résultats et accroître l'efficacité globale. Une effort important sera de formaliser l'intérêt et l'efficacité des techniques par des métriques qui, à ce jour, n'existent pas pour ce type de problème.  Pour conclure, cet ensemble de techniques présente actuellement trois avantages importants :   1. Il prend en entrée du texte « non-annoté » produits quotidiennement par des sources  journalistiques ou techniques facilement accessibles à travers des initiatives tel que le projet français Passage , qui juxtapose des fragments du Wikipedia français, de sources Wiki français, du journal régional L'Est Républicain, d'Europarl et de JRC Acquis.  2. Il permet d'améliorer de façon significative un lexique morphologique et syntaxique à   large couverture en peu de temps.  3. Enfin, son application répétée sur un corpus peut rendre ce corpus représentatif des manques de la grammaire utilisée. Un tel corpus pourrait être un point de départ pour le développement d'un processus dédié à l'amélioration d'une grammaire.  
