La tâche, aujourd'hui considérée comme fondamentale, de reconnaissance d'entités nommées, présente des difficultés spécifiques en matière d'annotation. Nous les précisons ici, en les illustrant par des expériences d'annotation manuelle dans le domaine de la microbiologie. Ces problèmes nous amènent à reposer la question fondamentale de ce que les annotateurs doivent annoter et surtout, pour quoi faire. Nous identifions pour cela les applications nécessitant l'extraction d'entités nommées et, en fonction des besoins de ces applications, nous proposons de définir sémantiquement les éléments à annoter. Nous présentons ensuite un certain nombre de recommandations méthodologiques permettant d'assurer un cadre d'annotation cohérent et évaluable. Today, the named entity recognition task is considered as fundamental, but it involves some specific difficulties in terms of annotation. We list them here, with illustrations taken from manual annotation experiments in microbiology. Those issues lead us to ask the fundamental question of what the annotators should annotate and, even more important, for which purpose. We thus identify the applications using named entity recognition and, according to the real needs of those applications, we propose to semantically define the elements to annotate. Finally, we put forward a number of methodological recommendations to ensure a coherent and reliable annotation scheme. annotation, reconnaissance d'entités nommées. annotation, named entities extraction.  Si l'extraction d'entités nommées (EN), apparue au milieu des années 1990 à la faveur des  dernières conférences MUC , fait aujourd'hui figure d'incontournable en Traitement Automatique des Langues (TAL), l'annotation de corpus qui la sous-tend est encore peu étudiée en tant  que telle. Les enjeux de l'annotation manuelle sont pourtant importants. Qu'il s'agisse de la  performance des systèmes mis au point (à partir d'un travail de modélisation ou d'apprentissage automatique sur corpus), de l'évaluation de ces derniers, ou encore de la bonne réponse apportée à des besoins applicatifs, l'annotation de corpus est une composante fondamentale. Au coeur des divers processus constituant la reconnaissance d'entités nommées (REN), nous souhaitons examiner de plus près la problématique de l'annotation manuelle, laquelle conduit à s'interroger sur ce que sont les entités nommées et ce à quoi elles servent.  Nous présentons cette pratique établie qu'est aujourd'hui l'annotation d'EN (2) puis nous en  détaillons les principales difficultés (3), aussi bien sur des textes de langue générale que de spécialité. Nous examinons ensuite les applications dans lesquelles les EN sont utilisées et en déduisons les différents types d'annotation (4). Enfin, nous proposons des recommandations méthodologiques permettant d'assurer un cadre d'annotation cohérent et évaluable (5). La reconnaissance d'entités nommées est une tâche bien connue : initiée il y a une vingtaine d'années à l'occasion des conférences américaines MUC, elle fut rapidement reprise lors d'autres campagnes d'évaluation, suscitant des travaux toujours plus nombreux. Sans revenir ici sur le " succès" de cette tâche (Nadeau & Sekine, 2007), on peut retracer son évolution selon trois directions principales. La première correspond à des travaux dans le domaine " général ", avec la poursuite de la tâche définie par MUC pour d'autres langues que l'anglais, selon un jeu de catégories plus ou moins revisité et pour annoter des entités dans des corpus de nature journalistique essentiellement . La seconde direction concerne des travaux dans des domaines dits " de spécialité ", avec la reconnaissance d'entités dans les domaines de la médecine, de la chimie ou de la microbiologie. Il fut ainsi proposé de reconnaître des noms de gènes, de protéines, etc. dans de la littérature spécialisée, lors des campagnes JNLPBA (Kim et al., 2004) et BioCreAtIvE (Hirschman et al., 2005). La dernière direction, transversale aux domaines général et de spécialité, correspond à des travaux sur la désambiguïsation : résolution de métonymie des EN dans SemEval2007 (Markert & Nissim, 2007) et désambiguïsation de noms de personnes (Artiles et al., 2007) . A l'occasion de chacune de ces campagnes, des corpus furent constitués et annotés manuellement. De manière générale, ces corpus annotés servent à la mise au point d'outils d'annotation automatique. &#34;Mise au point&#34; est à entendre ici au sens large : il s'agit de décrire le plus précisément possible ce que les systèmes doivent faire pour guider le travail d'écriture des règles sur lesquelles ils reposent, pour apprendre automatiquement ces règles de fonctionnement ou des critères de décision et, enfin, pour évaluer les résultats obtenus en les confrontant à une analyse de référence. Le processus d'annotation met en jeu deux acteurs, un annotateur et un texte, et aboutit à une annotation du texte dont la qualité doit répondre à une méthodologie et supporter un protocole d'évaluation, et le contenu suivre un guide d'annotation. Pour le domaine général, les campagnes MUC, CoNLL et ACE ont travaillé avec des corpus issus de la presse. Ces campagnes semblent avoir porté attention au processus d'annotation manuelle des EN, avec la rédaction de guides d'annotation et des calculs d'accord interannotateurs (mais non intra-annotateur), procédant par allers-retours entre le corpus à annoter et  le guide d'annotation à réviser, mais des points d'hésitations ont perduré quant à l'annotation,  dus principalement à " different interpretations of vague portions of the guidelines " (Sundheim, 1995) où à des phénomènes de superposition de sens (Doddington et al., 2004). Dans les domaines de la biologie et de la biomedecine, des textes des bases de données de publications scientifiques (PubMed et MedLine ) ont été annotés : on observe que les guides d'annotation - quand ils existent - laissent des zones d'ombre quant à la manière d'annoter les entités et que peu d'études ont porté sur la qualité de l'annotation. Que ce soit pour les corpus GENIA (Kim et al. , 2003), PennBioIE (Kulick et al., 2004) ou GENETAG (Tanabe et al., 2005), aucun accord inter ou intra-annotateur n'est rapporté. A la fin de leur expérience d'annotation, les auteurs de (Tanabe et al., 2005) constatent que " a more detailed definition of a gene/protein name, as well as additional annotation rules, could improve interannotator agreement and help solve some of the tagging inconsistencies ".  On observe par ailleurs des pratiques d'annotation manuelle et une réflexion méthodologique  intéressantes dans d'autres domaine du TAL, notamment dans les travaux issus de la communauté parole (Gut & Bayerl, 2004). La campagne EVALDA MEDIA (Bonneau-Maynard et al., 2005), par exemple, a mis en oeuvre une annotation manuelle de corpus en deux passes, la première sur un échantillon pour faire le point sur les éventuels désaccords entre annotateurs, et une seconde grandeur nature, après réajustement des guides d'annotation.  Pour les corpus de langue générale, un certain nombre de difficultés sont identifiées (Ehrmann,  2008). La première concerne le choix des catégories et la détermination de ce qu'elles recouvrent. En effet, au-delà de la triade "universelle" définie par MUC ( , et ), l'inventaire des catégories à annoter est difficile à stabiliser et à définir. Prenons l'exemple de la catégorie : s'il est évident qu'un nom d'individu tel que Lionel Jospin est à annoter à l'aide de cette catégorie, que faut-il faire des Kennedys, de Zorro, des Démocrates ou de St. Nicolas ? Pour les autres catégories, il est également difficile de choisir la granularité des catégories et de déterminer ce qu'elles recouvrent vraiment. Un autre type de difficuté concerne la sélection des mentions à annoter ainsi que la délimitation des frontières des EN. A titre d'exemple, considérons l'EN "Barack Obama" et les diverses unités lexicales suivantes permettant d'y faire référence : Barack Obama, Monsieur Obama, le Président des Etats-Unis, le nouveau président , il. Faut-il annoter les noms propres uniquement, ou peut-on également considérer les descriptions définies permettant d'identifier cette personne, voire les pronoms qui, contextuellement, y renvoient ? Et que fait-on des différents attributs accompagnant l'EN ("monsieur" et "président") ? De nombreux autres cas, plus ou moins compliqués (la Maire du 7e arrondissement de Paris et Garde des Sceaux Rachida Dati ) se rencontrent en corpus. Dans le même ordre d'idée, des phénomènes de coordination et d'imbrication peuvent poser problème aux annotateurs (une ou plusieurs entités pour Bill et Hillary Clinton et l'Université de Corte ?). Enfin, une dernière difficulté résulte de phénomènes de pluralité référentielle, avec des EN homonymes (Orange ville et Orange compagnie) et des glissements métonymiques, parfois difficiles à distinguer (France en tant que lieu géographique, gouvernement ou équipe sportive). Même si elles sont surmontables grâce aux guides d'annotation, ces difficultés entraînent un coût supplémentaire et une baisse de qualité de l'annotation.  Notre expérience en microbiologie montre que ces difficultés sont plus aiguës encore dans les  langues de spécialité. Une expérience d'annotation a été réalisée à l'INIST pour le programme Quaero, en collaboration avec l'équipe MIG. Un corpus anglais de 499 notices PubMed (titres et résumés, soit environ 110 000 "mots"), pré-annotées par application d'un dictionnaire et d'un anti-dictionnaire, a été fourni à deux experts de l'INIST, dont le travail a consisté en une révision de ces pré-annotations. La principale difficulté rencontrée a concerné la distinction qui était demandée entre noms propres et noms communs, la limite morphologique entre les deux étant peu marquée dans ces domaines où les noms communs sont souvent reclassés comme &#34;noms propres&#34;, comme en atteste la présence de ces noms dans des nomenclatures ("small, acid-soluble spore protein A" est ici un cas extrême) ou les phénomènes d'acronymisation (on trouve par exemple &#34;across the outer membrane (OM)&#34;). Dans ces cas, la consigne donnée aux annotateurs était de se référer à des listes d'autorité, telle que Swiss-Prot , ce qui entraîne une perte de temps conséquente. La délimitation des frontières des éléments à annoter a elle-aussi soulevé de nombreux questionnements, les annotateurs se demandant ce qu'il fallait inclure ou non dans le segment annoté. On peut ainsi choisir d'annoter "nifh messenger RNA&#34; si on considère que la mention de l'état &#34;messenger RNA&#34; entre dans la détermination de la référence, ou seulement "nifh", si on considère que le nom propre suffit à construire la référence. Le typage sémantique choisi a aussi posé problème aux annotateurs, notamment pour les éléments génétiques mobiles, comme les plasmides ou les transposons. En effet, ceux-ci devaient être annotés dans les taxons et non dans les genes alors que ce sont des fragments d'ADN, donc des parties de génome. Une directive particulièrement perturbante pour les annotateurs a été d'annoter l'acronyme "KGFR" comme nom propre et sa forme développée "keratinocyte growth factor receptor" comme nom commun. Ce type de consigne, préconisée au départ pour entraîner plus efficacement les outils de REN (Nédellec et al., 2006), est difficile à appréhender et aurait dû être mieux documentée.  Ces problèmes se traduisent par un coût d'annotation élevé, des guides d'annotation de taille  trop grande par rapport au corpus et trop d'hésitations de la part des annotateurs, ce qui induit des incohérences et une qualité moindre de l'annotation. Cette expérience nous a ainsi permis de reposer la question de ce que les annotateurs doivent annoter et surtout, pour quoi faire. Pour mieux comprendre &#34;quoi annoter dans quel texte&#34;, nous revenons sur les critères linguistiques qui permettent de définir la notion d'EN : l'importance de ces critères varie d'une application à l'autre et les annotations résultantes en dépendent.  (Ehrmann, 2008) propose une analyse linguistique de la notion d'EN qu'elle présente comme  une &#34;création&#34; du TAL. Dans ce qui suit, nous reprenons la distinction introduite dans (LDC, 2004) : les EN sont des &#34;mentions&#34; qui renvoient à des &#34;entités&#34; du domaine, ces mentions pouvant relever de différentes catégories linguistiques : des noms propres (&#34;Rabelais&#34;), mais aussi les pronoms (&#34;il&#34;), et plus largement des descriptions définies (&#34;le père de Gargantua&#34;). On peut identifier plusieurs critères définitoires des EN. Unicité référentielle L'une des caractéristiques principales des noms propres est leur fonctionnement référentiel : un nom propre renvoie à une entité référentielle unique, même si cette unicité est contextelle. A la différence de (Poibeau, 2005), nous considérons que cette propriété est essentielle dans l'utilisation que fait le TAL des EN.  Autonomie référentielle Les EN sont de surcroît autonomes du point de vue référentiel. C'est  évident dans le cas du nom propre qui permet à lui seul l'identification du référent, tout au moins dans une situation de communication donnée (Eurotunnel). Le cas des descriptions définies (l'opérateur du tunnel sous la Manche) est un peu différent : si elles suffisent à identifier le référent, c'est par le truchement de connaissances extérieures.  Stabilité dénominative Les noms propres sont également des dénominations stables. Même s'il  y a des variations (Angela Merkel/Mme Merkel/A. Merkel), elles sont plus régulières et moins nombreuses que pour les autres syntagmes nominaux .  Relativité référentielle L'interprétation se fait toujours relativement à un modèle du domaine,  qui peut être implicite dans les cas simples (on suppose une connaissance partagée de ce qu'est une personne ou un pays) mais qui doit être explicité dès que la diversité des entités à prendre en compte s'accroît (il faut au moins une typologie pour les catégoriser).  La REN voit le jour dans un cadre d'extraction d'information où il s'agit d'identifier les actants  de certaines situations (des attentats aux interactions géniques), cette situation étant décrite par un formulaire à instancier avec les informations extraites du texte. La sémantique sous-jacente est clairement référentielle : on identifie dans le texte les &#34;mentions&#34; des &#34;entités&#34; qui jouent un rôle dans les situations considérées (LDC, 2004). Dans les systèmes de questions/réponses, les EN jouent le même rôle mais on a souvent recours à des typologies plus fines . Les EN sont également utilisées comme &#34;descripteurs&#34; dans de nombreuses applications d'indexation. On a des index de noms propres dans certains ouvrages et moteurs de recherche et on sait qu'une importante proportion des requêtes adressées aux moteurs de recherche sont des EN. On surligne les EN pour aider la lecture ou la navigation dans de gros volumes documentaires. On utilise enfin la REN pour construire et mettre à jour des nomenclatures (Tran & Maurel, 2006) utilisées pour l'indexation, la traduction, etc. Dans ce deuxième type d'application aussi, la sémantique sous-jacente est référentielle : si les EN sont de bons descripteurs, c'est qu'elles fonctionnent comme des ancres référentielles qui permettent de situer ce à quoi le texte fait référence . En revanche, on s'intéresse exclusivement aux mentions de type nom propre. Troisième type d'applications, les EN sont utilisées pour l'intégration de données. Cela concerne à la fois l'analyse des bases documentaires (suivi de thèmes, découverte de communautés de pratiques (Li & Liu, 2005) et l'articulation des documents avec d'autres sources de connaissances (bases de données, bases d'images, etc.) pour interroger les unes et les autres de manière homogène et naviguer facilement de l'une à l'autre (Dragos & Nazarenko, 2009). On s'appuie alors sur les EN référencées dans les bases de données pour établir des liens entre les différentes sources. Pour ces tâches d'intégration, on se contente des EN qui figurent dans les nomencla tures, l'objectif étant d'articuler les sources entre elles et pas d'en décrire le contenu : il suffit  de savoir qu'une dépêche parle de Nelson Mandela, inutile de trouver toutes les mentions qui en sont faites dans le texte.  L'anonymisation des documents est un quatrième champ d'application (Plamondon et al., 2004).  On veut éviter qu'on puisse identifier des entités (des personnes, notamment) à partir des mentions qui en sont faites dans le texte. Il faut repérer toutes les formes de mentions (la ville qui a reçu la première bombe atomique ) et pas seulement les noms propres.  Dernière famille d'applications, la REN est utilisée pour &#34;peupler&#34; des ontologies. Il s'agit alors  de modéliser un domaine, et le modèle formel des ontologies impose de distinguer les entités du domaine, qui sont représentées comme des instances, des concepts ou classes auxquelles ces instances se rattachent. La REN est alors utilisée pour enrichir la structure conceptuelle avec des instances de concepts ou de rôles (relations entre instances) (Amardeilh et al., 2005). Selon les cas, on privilégie les entités nommées ou on s'intéresse à toutes les mentions d'entités. Dans le premier cas, l'ontologie résultante peut être vue comme une nomenclature hiérarchisée ou comme un thesaurus formalisé. Dans le second, on cherche à modéliser un domaine en identifiant les entités qui le composent et les relations qu'elles entretiennent. Dans les deux cas, le typage des EN est essentiel parce qu'on doit relier l'instance nommée à un concept de l'ontologie. Les critères définitoires cités en 4.1 ne jouent pas tous le même rôle pour toutes les applications. Dans certains cas (indexation et intégration de connaissances), on s'intéresse à des entités référentielles qui sont désignées par des descripteurs stables et non ambigus. Ce sont donc les EN de type noms propres ou &#34;catalogables&#34; qui sont à retenir et il est important de les normaliser pour s'affranchir des variations qui peuvent apparaître malgré leur stabilité référentielle. Pour ce type d'application, l'essentiel n'est pas de repérer toutes les mentions de telle entité dans un document mais de repérer que ce document mentionne telle entité. Il faut donc privilégier la précision sur le rappel de l'annotation. A l'autre extrême, on trouve les tâches d'extraction d'information et de modélisation du domaine où toutes les mentions sont importantes à repérer, y compris celles qui sont des descriptions définies (il faut d'ailleurs repérer des relations de coréférence entre les mentions qui ne sont pas suffisamment autonomes référentiellement). La figure 1 montre l'impact de ces deux perspectives d'annotation sur les résultats de l'annotation d'un tout petit exemple .  Comme il est impossible de repérer les mentions de toutes les entités référentielles, le modèle  du domaine détermine quelles sont les entités &#34;d'intérêt&#34; et la limite entre ce qui doit ou non être annoté. Illustrons ce point sur un exemple. Quand un directeur des ressources humaines s'intéresse aux grilles salariales de son organisation, il raisonne sur des catégories de personnels et pas sur les personnes physiques que sont les employés. Cela se reflète dans le modèle du domaine : les différentes catégories de personnes (techniciens, ingénieurs, etc.) sont modélisées comme des instances rattachées au concept et les personnes physiques ne sont pas représentées. A l'inverse, quand il s'occupe des fiches de paye et de la progression des employés, il s'intéresse aux individus. Dans ce cas, le modèle doit considérer les personnes comme instances et les catégories de personnels comme des concepts. Le même problème se  rencontre en biologie où la mention du gène G peut renvoyer au gène G de l'espèce E, au gène  G d'un individu de l'espèce E ou à un gène particulier parmi les gènes G de cet individu.  Modéliser suppose de faire des choix explicites là où les textes peuvent être flous et mêler les  points de vue. Il est donc impossible d'annoter les EN d'un texte indépendamment d'un modèle de référence. Dans le cas de l'expérience décrite plus haut, le modèle était, comme souvent, simplement décrit par une liste de concepts : il fallait nommer les gènes et protéines, mais aussi leurs familles, compositions, et composants. Par ailleurs, comme la REN doit servir à modéliser le réseau d'interactions entre gènes, il était essentiel de repérer toutes les mentions des entités considérées, chacune pouvant contribuer à enrichir sa modélisation.  Annoter est difficile. Il faut donc guider le travail des annotateurs, ce qui passe par des guides  et des outils d'annotation, mais aussi par l'évaluation de la qualité des annotations.  Guides d'annotations Comme le type d'annotation à produire dépend de ce qu'on cherche à  annoter et de ce à quoi cette annotation doit servir, il est essentiel de fournir aux annotateurs des guides (ou conventions) d'annotation qui indiquent ce qu'il faut annoter plutôt que comment annoter. Trop souvent en effet, les contraintes de faisabilité prennent le pas sur les critères sémantiques ce qui brouille l'objectif pour les annotateurs. Par ailleurs, il est important de prendre la mesure de la tâche dans toute sa complexité sans exclure a priori ce qui serait douteux ou trop difficile à reproduire automatiquement. C'est même l'intérêt de l'annotation manuelle que de donner une idée de l'ampleur de la tâche d'annotation. Il faut donner aux annotateurs une vision claire de l'application visée. Cette vision doit s'appuyer sur un modèle de référence explicite, du type de celui donné dans la campagne GENIA, avec des définitions précises et des explications sur les choix méthodologiques réalisés (catégories, typage sémantique, etc.). Des exemples peuvent être ajoutés à titre d'illustration mais ils ne doivent pas se substituer à la définition des objectifs. Ces informations permettont de limiter les incompréhensions, mais aussi de responsabiliser et de motiver les annotateurs en leur donnant accès à la logique sous-jacente. On entre ainsi dans une démarche didactique, permettant de passer d'un "rapport de père à un rapport de pair&#34; (Akrich & Boullier, 1991), ce qui est d'autant plus nécessaire que l'annotation porte sur des corpus spécialisés, les annotateurs étant  des experts à qui on a intérêt à donner la plus grande autonomie possible. Dans certains cas, la  tâche d'annotation étant trop complexe, on doit se restreindre à une annotation exploitable pour l'apprentissage automatique - en se limitant par exemple aux noms propres - mais cela doit être fait de manière explicite pour que les annotateurs aient une vue claire des choix à faire. Outils d'annotation Le travail d'annotation doit être outillé. On utilise des dispositifs techniques pour annoter les textes mais aussi pour préparer le travail d'annotation par une préannotation (projection d'un dictionnaire, par exemple). Ces outils sont cependant à manier avec précaution du fait des biais qu'ils introduisent. Dans l'expérience que nous avons menée, le corpus d'annotation avait été pré-annoté par projection d'un dictionnaire de noms de gènes et de protéines, pour alléger le travail des annotateurs, mais cette pré-annotation a influencé l'annotation. Elle a introduit un biais en faveur de la correction des pré-annotations, au détriment de la recherche de nouvelles EN. Une solution consiste à procéder en deux temps, en étiquetant à la main un échantillon du corpus, puis en le pré-annotant pour comparer les résultats et évaluer le biais introduit. On peut ensuite faire une annotation à plus grande échelle, précédée par une pré-annotation dont on connaîtra cette fois les biais.  En ce qui concerne le processus d'annotation lui-même, s'il existe aujourd'hui de nombreux  outils d'aide à l'annotation manuelle, peu sont effectivement disponibles, c'est-à-dire gratuits, téléchargeables et utilisables . On peut citer, entre autres, Callisto, MMAX2, Knowtator ou encore Cadixe qui a été utilisé dans notre expérience. Les fonctionnalités et l'expressivité du langage d'annotation doivent être adaptées à la tâche d'annotation visée : selon les cas, il faut typer des segments de textes ou les mettre en relation, on a des annotations concurrentes, disjointes ou superposables, etc. Dans notre expérience en microbiologie, par exemple, le fait de ne pas pouvoir annoter les coordinations d'EN a posé problème. Les critères ergonomiques sont également importants : on sait que les fonctionnalités difficiles d'accès ne sont pas utilisées, ce qui biaise là aussi les résultats. Dans notre cas, les annotateurs ont souvent négligé de mentionner leur incertitude parce qu'ajouter l'attribut correspondant à leurs annotations était malaisé. Évaluation de l'annotation Il est important de mesurer la qualité de l'annotation. (Gut & Bayerl, 2004) distingue l'accord inter-annotateur, qui permet de mesurer la stabilité de l'annotation, et l'accord intra-annotateur, qui donne une indication de la reproductibilité de l'annotation. S'il est important de calculer l'accord inter-annotateur, il n'est pas nécessaire de le réaliser sur tout le corpus, ne serait-ce que pour des raisons de coût. Il est en revanche conseillé de le calculer tôt, afin d'identifier les problèmes rapidement et de modifier l'annotation en conséquence. Il en va de même en ce qui concerne l'accord intra-annoteur. Un autre moyen d'évaluation consiste à faire ajouter à l'annotateur, si nécessaire, une note d'incertitude, de préférence typée, sur ses annotations. En l'absence de calcul de l'accord inter ou intra-annotateur pour cette annotation nous avons ainsi fait annoter les incertitudes des annotateurs a posteriori sur un sous-ensemble du corpus. Pour s'assurer de la représentativité de l'échantillon, les annotateurs ont identifié une typologie des fichiers du corpus (six "types"). Nous avons ensuite extrait 25 fichiers de types variés parmi les 499 notices du corpus (soit 5%). L'objectif de cette seconde validation étant d'évaluer la confiance des annotateurs en leur validation, nous avons choisi de profiter du tag uncertainty proposé dans Cadixe et de l'utiliser de manière systématique en cas de doute. Nous avons donc redéfini les types d'uncertainty en fonction de l'expérience des annotateurs. Cette évaluation n'a nécessité que quelques heures de travail et nous a permis de mieux qualifier et quantifier leurs doutes. Au final, sur 555 tags, les  annotateurs ont déclaré 113 uncertainty, soit environ 20% des tags. On observe que plus de 75%  des incertitudes concernent les noms communs de type bacteria, et que ces incertitudes sont très largement (77%) liées à une difficulté à distinguer les noms communs des noms propres.  Plus généralement, pour s'assurer à la fois de la qualité du guide d'annotation et des possibilités  d'évaluation, une bonne méthode consiste, en tout début de projet, à faire travailler les annotateurs chacun de leur côté sur un échantillon du corpus. Cela permet d'identifier rapidement les désaccords, puis de les trancher, soit en faisant intervenir un autre expert, soit par concertation des annotateurs. Ces décisions sont ensuite reportées dans le guide d'annotation.  Au final, une démarche rigoureuse et efficace d'annotation d'EN en corpus doit prêter attention  principalement à deux points. Au regard du contenu tout d'abord, il importe de se focaliser non pas tant sur comment annoter, mais sur quoi annoter, en fonction de l'application visée. Nous avons vu que chaque famille d'application spécifie un certain nombre de critères linguistiques, et que les mentions à annoter diffèrent relativement à ces derniers. Une fois spécifié ce qu'il faut annoter, il faut être prudent en termes de méthodologie et envisager dès le départ l'évaluation de l'annotation. Il est avantageux, du point de vue de la qualité de l'annotation, de réaliser un ou plusieurs galops d'essai, afin d'ajuster certains éléments du guide d'annotation et d'évaluer les biais éventuels introduits par l'outil d'annotation et/ou une mauvaise compréhension de la tâche par les annotateurs. Relativement aux enjeux et aux difficultés de l'annotation des EN en corpus, l'attention portée à la spécification du contenu et au respect d'une méthodologie d'annotation permet d'assurer un cadre de travail stable et cohérent à chacune des étapes de la tâche de REN. Nous comptons appliquer cette méthodologie dans le cadre des campagnes d'annotation de Quaero, en pharmacologie et en économie. Couvrant la terminologie et l'extraction de relations sémantiques, ces campagnes dépassent largement le cadre des EN, nous allons donc élargir notre méthode à ces applications.  Ce travail a été réalisé en partie dans le cadre du programme Quaero  , financé par OSEO, agence nationale de valorisation de la recherche. Nous en remercions les participants, en particulier l'équipe MIG de l'INRA. Nous remercions également F. Tisserand et B. Taliercio, les annotateurs experts de l'INIST.  
