Cet article montre comment calculer une interface syntaxe-sémantique à partir d'un analyseur   en  dépendance  quelconque  et  interchangeable,  de  ressources  lexicales  variées  et  d'une  base  d'exemples  associés à leur représentation sémantique. Chaque exemple permet de construire une règle d'interface. Nos  représentations  sémantiques  sont  des  graphes  hiérarchisés  de  relations  prédicat-argument  entre  des  acceptions lexicales et notre interface syntaxe-sémantique est une grammaire de correspondance polarisée.  Nous  montrons  comment  obtenir  un  système  très  modulaire  en  calculant  certaines  règles  par  « soustraction » de règles moins modulaires.  This  article  shows  how  to  extract  a  syntax-semantics  interface  starting  from  an  interchangeable  dependency  parser,  various  lexical  resources  and  from  samples  associated  with  their  semantic representations. Each example allows us to build an interface rule. Our semantic representations  are  hierarchical  graphs  of  predicate-argument  relations  between  lexical  meanings  and  our  syntaxsemantics interface is a polarized unification grammar. We show how to obtain a very modular system by  computing some rules by "subtraction" of less modular rules.  Interface syntaxe-sémantique, graphe sémantique, grammaires de dépendance, GUP  (Grammaire d'unification polarisée), GUST (Grammaire d'unification Sens-Texte)  Syntax-semantics Interface, Semantic Graph, Dependency Grammar, PUG (Polarized  Unification Grammar), MTUG (Meaning-Text Unification Grammar)   Une interface syntaxe-sémantique est un module qui prend en entrée les sorties d'un analyseur syntaxique   et  produit  des  représentations  sémantiques  correspondantes  qui  sont  des  graphes  de  relations  prédicatargument entre des acceptions lexicales. Le développement manuel d'un tel module peut être coûteux et il  est  périlleux  de  construire  une  interface  syntaxe-sémantique  qui  s'appuie  sur  les  sorties  d'un  analyseur  syntaxique  particulier  qui  peut  rapidement  devenir  obsolète.  Notre  objectif  est  donc  de  pouvoir  extraire  une  interface-sémantique  automatiquement  pour  n'importe  quel  analyseur  syntaxique.  Notre  idée  est  de  partir d'une base de phrases d'exemples associées à leur représentation sémantique, de traiter ces exemples  avec l'analyseur de notre choix et de calculer ainsi une grammaire permettant de faire la correspondance  stratégies d'extraction de grammaires sur corpus annotés par des méthodes statistiques, nous calculons une  grammaire formelle purement algébrique. Chaque exemple de notre base est conçu pour obtenir une règle  et  peut  être  vu  comme  une  demi-règle  contenant  la  partie  sémantique  et  dont  la  partie  syntaxique  est  calculée par l'analyseur syntaxique.   Notre représentation sémantique est un graphe de relations prédicat-argument entre les signifiés des unités   lexicales  et  grammaticales  d'une  phrase  (où  chaque  unité  lexicale  a  été  désambiguïsée  par  rapport  à  un  lexique de référence). Elle est directement inspirée des représentations sémantique et syntaxique profonde  de  la  Théorie  Sens-Texte  (Mel'uk  1988a ;  Candito  &  Kahane  1998 ;  Kahane  2002).  Il  s'agit  d'une  représentation  sémantique  du  contenu  linguistique  et  pas  d'une  sémantique  dénotationnelle  comme  les  représentations  sémantiques  basées  sur  la  logique.  Il  n'y  a  donc  pas  à  proprement  parler  de  calcul  de  valeurs  de  vérité  associées ;  par  contre,  ce  type  de  représentation  permet  des  calculs  de  paraphrases  (Mel'uk  1988b ;  Milievi  2007)  et  a  été  implémenté  avec  succès  pour  la  génération  de  textes  (Iordanskaja  et  al. 1988 ;  Bohnet &  Wanner 2001) ou la traduction automatique (Apresjan  et  al.  2003).  Des représentations similaires ont été proposées par d'autres auteurs sans référence explicite à la Théorie  Sens-Texte. Voir par exemple (Copestake 2009) ou (Bédaride & Gardent 2009).   Nous commencerons par présenter le formalisme utilisé pour l'écriture d'une interface syntaxe-sémantique   (section  1),  puis  les  principes  généraux  du  calcul  de  règles  grammaticales  ne  nécessitant  pas  de  connaissances lexicales (section 2). L'exploitation de ressources lexicales pour la production de nouvelles  règles sera esquissée (section 3). Nous terminerons en montrant  comment  réaliser l'articulation lexiquegrammaire  par  la  « soustraction »  de  règles  lexicales  à  nos  règles  grammaticales  (section  4).  Cet  article  porte essentiellement  sur les questions théoriques liées au calcul de l'interface syntaxe-sémantique. Une  implémentation non encore évaluée est en cours.   Pour  écrire  notre  interface  syntaxe-sémantique,  nous  utilisons  un  formalisme  générique,  la  Grammaire   d'Unification  Polarisée  (GUP)  (Kahane,  2004).  Ce  formalisme  permet  d'écrire  des  grammaires  de  correspondances  entre  graphes  et  a  déjà  été  proposé  pour  l'interface  syntaxe-sémantique  (Kahane  &  Lareau,  2005).  Ce  formalisme  permet,  à  l'image  de  TAG,  de  combiner  des  structures  élémentaires  afin  d'obtenir une structure complète. Les structures que nous souhaitons obtenir sont des couples formés d'un  arbre  de  dépendance  syntaxique  et  d'un  graphe  sémantique ;  nos  structures  élémentaires  sont  donc  des  fragments  d'arbre  syntaxique  associés  à  des  fragments  de  graphe  sémantique.  La  particularité  de  ce  formalisme est un contrôle rigoureux de ce que chaque règle consomme, à l'aide de polarités associées aux  objets manipulés par les règles. Le jeu de polarités le plus simple est constitué de deux polarités, que nous  appelons noir ( )  et blanc (). Chaque objet de la structure reçoit une polarité. Sont considérés comme des  objets  les  noeuds  (identifiés  avec  l'élément  lexical  qu'ils  portent),  les  dépendances  et  les  éléments  flexionnels  ayant  une  contribution  sémantique  (temps  verbal,  nombre  nominal,  etc.).  Les  règles  sont  combinées par identification des objets dont les étiquettes peuvent s'unifier et les polarités se combiner.   La Figure 1 présente un exemple d'interface syntaxe-sémantique en GUP. En haut à gauche se trouve la   phrase  Mary seems to  sleep  avec l'analyse syntaxique en dépendance qu'en propose le Stanford  Parser.  Les  dépendances  syntaxiques  sont  orientées  vers  la  gauche  (<nsubj)  ou  vers  la  droite  (xcomp>).  Nous  ajoutons des polarités blanches sur les dépendances ( ) et les mots ( ) indiquant que ces objets doivent être  consommés par des règles d'interface. Pour les verbes, une deuxième polarité blanche ( ) indique que la  flexion verbale doit aussi être consommée. En haut à droite se trouve le résultat attendu, c'est-à-dire un  graphe sémantique associé à la phrase et polarisé en noir puisque produit par l'interface. Pour assurer cette  correspondance,  nous  utilisons  les  trois  règles  qui  figurent  en  dessous. Ce  sont  des  règles  lexicales  consomme  la  totalité  des  dépendances  syntaxiques,  mais  ne  produit  qu'une  dépendance  sémantique.  La  deuxième  dépendance  sémantique  est  produite  par  la  règle  de  SLEEP.  Mais  pour  que  cette  règle  puisse  s'appliquer  il  est  nécessaire  que  la  règle  de  SEEM  restitue  une  dépendance  syntaxique.  Notons  pour  terminer que la règle de SEEM impose à son xcomp> d'être un verbe infinitif (Vinf) et consomme ainsi sa  polarité flexionnelle ( ).       Figure 1. Un exemple d'interface syntaxe-sémantique en GUP    Nous allons calculer la règle pour l'aspect progressif. Celui-ci est exprimé par BE + Ving et nous voulons   récupérer  au  niveau  sémantique  un  attribut  [aspect=  progressive]  sur  le  verbe.  Pour  apprendre  la  règle,  nous construirons un exemple de phrase avec un progressif (en l'occurrence Mary is sleeping) en indiquant  que pour is le lemme seul sera consommé ( ) et que pour sleeping la flexion seule sera consommée ( )  (voir Figure 3). Autrement dit, nous connaissons déjà la règle et nous pourrions l'écrire à la main. Notre  objectif est de l'adapter automatiquement et sans effort aux sorties de n'importe quel analyseur, d'autant  que les analyseurs varient non seulement dans les étiquettes qu'ils utilisent, mais aussi dans les structures  qu'ils manipulent. Dans le cas du progressif, nous ne savons pas si le sujet sera relié à l'auxiliaire ou au  verbe lexical.  Nos deux  analyseurs de  référence,  le Stanford Parser et  le  Link  Grammar, font des choix  différents. C'est pourquoi nous intégrons le sujet dans la règle et considérons donc une relation sémantique  <arg1 correspondante. Nous verrons dans la section 4 comment « retirer » cette information.        Figure 3. Calcul de la règle du progressif : exemple de départ (à gauche) et règles obtenues    pour le Stanford Parser (au centre) et pour Link Grammar Parser (à droite)   L'utilisation d'un lexique électronique permet le calcul automatique de règles. Par exemple, une ressource   telle  que  VerbNet  (pour  l'anglais)  ou  Dicovalence  (pour  le  français),  décrivant  des  cadres  de  sousrègles. Par exemple, le cadre give-13.1 de VerbNet est décrit de la façon suivante :   Cette  description  peut  être  utilisée  pour  créer  automatiquement  la  règle  lexicale  de  la  Figure  4,  avec  le   processus décrit dans (Chaumartin, 2005). Première étape : à partir de l'exemple donné par VerbNet et sa  description dans VerbNet la demi-règle sémantique est construite. Deuxième étape : l'exemple est analysé  par  l'analyseur  de  notre  choix  (ici  le  Stanford  Parser),  ce  qui  nous  fournit  une  règle  lexicale  pour  l'interface avec les résultats de cet analyseur.          Figure 4. Extraction d'une règle lexicale à partir du cadre give-13.1 de VerbNet :  exemple de départ construit à partir de VerbNet à gauche, règle obtenue pour le Stanford Parser à droite   Cette règle est par ailleurs utile pour lever des ambiguïtés lexicales et syntaxiques : en effet, d'une part, la   recherche  de  la  règle  la  plus  couvrante  dans  la  forêt  d'analyses  syntaxiques  produite  pour  une  phrase  donnée permet d'augmenter le score des analyses où la règle est applicable ; d'autre part, VerbNet précise  les  sens  du  verbe  compatibles  avec  le  cadre  de  sous-catégorisation,  et  impose  éventuellement  des  contraintes de sélection sur ses arguments.   Considérons  une  phrase  telle  que  They  were  lending  me  a  bicycle.  Nous  pouvons  y  appliquer  la  règle   grammaticale  du  progressif  (calculée  en  Section  2).  Mais  cette  règle  consomme  le  lien  sujet  et  nous  ne  pourrons pas ensuite appliquer la règle lexicale du verbe LEND que nous avons créée à partir de VerbNet  (Section 3). La solution habituelle à ce problème est celle adoptée par exemple par les grammaires TAG  consistant  à  produire  à  partir  de  la  diathèse  de  base  toutes  les  réalisations  possibles  (Candito  1999)  ou  (Bédaride  &  Gardent,  2009)  dans  un  formalisme  similaire  au  notre.  Il  en  résulte  un  lexique-grammaire  assez  volumineux  en  raison  de  la  croissance  rapide  du  nombre  de  règles  en  fonction  du  nombre  de  phénomènes  pris  en  compte  (le  lexique  inclut  en  fait  la  grammaire).  Plutôt  que  d'additionner  divers  phénomènes  au  sein  d'une  même  règle,  nous  proposons  au  contraire  de  soustraire  aux  règles  grammaticales  la  partie  lexicale  pour  permettre  à  la  règle  lexicale  de  se  combiner  avec  les  règles  grammaticales. Dans le cas du progressif, réalisé par une construction avec un auxiliaire BE + Ving nous  nous  ramenons  au  cas  d'une  forme  verbale  simple.  Pour  ramener  la  construction  A  au  cas  d'une  construction B, nous proposons simplement de calculer comme précédemment des règles pour A et B, puis  de  soustraire  la  règle  de  B  à  celle  de  A.  La  soustraction  est  contrôlée  par  les  polarités  selon  le  calcul  suivant :      -   =  suppression (autrement dit, tout objet manipulé dans A et B est supprimé)    ( -)  =  (un objet uniquement manipulé dans B doit être absolument introduit dans la règle A-B   pour être consommé ensuite par l'application de B)      Figure 6. Calcul de la règle pour le progressif   Dans  la  Figure  6,  la  première  ligne  montre  les  règles  obtenues  pour  le  Link  Grammar  et  la  deuxième   montre celles obtenues pour le Stanford Parser. Rappelons que la base d'exemple contient uniquement la  partie inférieure des règles (le graphe sémantique) pour les deux exemples, A = Mary is sleeping and B =  Mary sleeps). La partie supérieure est calculée par l'analyseur, puis la règle B est soustraite de A. Dans le  cas  du  Ling  Grammar,  le  lien  <S  de  B  ne  correspond  pas  au  lien  <S  de  A  (ils  n'ont  pas  le  même  gouverneur) et donne donc un lien <S dans A-B. Dans le cas du Standford Parser, les liens <nsubj de A  et B se correspondent et s'annulent donc l'un l'autre. Notons que le lien <S dans la règle du progressif  obtenue  avec  le  Ling  Grammar  n'est  pas  un  problème  même  pour  l'analyse  d'une  phrase  comme  Mary  seems  to  be  sleeping :  l'application  de  la  règle  pour  SEEM  (Figure  1)  permettra  de  se  ramener  à  une  structure similaire à celle de la phrase Mary is sleeping, où la règle du progressif pourra s'appliquer.    Terminons en présentant la règle pour le passif. Comme précédemment, le principe consiste à se ramener à   une forme plus simple, c'est-à-dire, dans ce cas, l'actif. La polarisation des noeuds est donnée dans la base  d'exemples (les parties inférieures qui s'annulent, puisque le passif n'a pas de contribution sémantique, ne  sont pas montrées), la partie supérieure est calculée par l'analyseur (ici le Link Grammar), puis la règle  définitive est obtenue par soustraction.                 Figure 7. Calcul de la règle pour le passif   L'idée  de  développer  une  interface  syntaxe-sémantique  entre  arbre  de  dépendance  et  graphe  de  relation   prédicat-argument est ancienne et remonte au début de la Théorie Sens-Texte dans les années 60 (Mel'uk  1988, Kahane 2002). L'originalité du présent travail est de proposer une stratégie simple pour construire  une telle interface à partir de ressources existantes, analyseurs syntaxiques et lexiques sémantiques. D'une  part,  notre  grammaire  s'adapte  automatiquement  aux  sorties  de  n'importe  quel  analyseur  syntaxique  en  dépendance.  D'autre  part,  notre  grammaire  est  très  modulaire  avec  une  seule  règle  pour  chaque  régime  d'une  unité  lexicale  et  des  règles  grammaticales  séparées  pour  les  différentes  constructions  et  redistributions  dont  cette  unité  lexicale  peut  faire  l'objet.  Nous  pensons  que  cette  approche  est  pragmatique dans la mesure où elle met en oeuvre des ressources de large couverture dans leur état actuel.  l'interface syntaxe-sémantique obtenue, l'indépendance vis-à-vis de tout analyseur syntaxique particulier,  et une facilité de prise en compte de nouvelles ressources. Notons que notre grammaire est complètement  réversible et peut servir aussi bien pour l'analyse que pour la génération de texte. La mise en oeuvre d'une  telle grammaire pose évidemment des difficultés qui ne sont pas abordées ici. Notons simplement que le  formalisme a déjà fait l'objet d'une implémentation (Lison, 2006) ; nous en développons actuellement une  nouvelle implémentation, en utilisant l'outil de réécriture de graphes GrGen et différentes heuristiques afin  d'éviter toute explosion combinatoire.   
