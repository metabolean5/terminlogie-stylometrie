Dans cet article nous relatons notre participation à  la campagne d'évaluation ESTER 2   (Evaluation des Systèmes de Transcription Enrichie d'Emissions Radiophoniques). Après avoir décrit les  objectifs de cette campagne ainsi que ses spécificités et difficultés, nous présentons notre système  d'extraction d'entités nommées en nous focalisant sur les adaptations réalisées dans le cadre de cette  campagne. Nous décrivons ensuite les résultats obtenus lors de la compétition, ainsi que des résultats  originaux obtenus par la suite. Nous concluons sur les leçons tirées de cette expérience.   In this paper, we report our participation to the ESTER 2 (Evaluation des Systèmes de  Transcription Enrichie d'Emissions Radiophoniques) evaluation campaign.  After describing the goals,  specificities and challenges of the campaign, we present our named entity detection system and focus on  the adaptations made in the framework of the campaign. We present the results obtained during the  competition and then new results obtained afterward. We then conclude by the lessons we learned from  this experiment.     entités nommées, évaluation, extraction d'information. named entities,  evaluation, information extraction.   La campagne d'évaluation ESTER 2 s'est déroulée de janvier 2008 à avril 2009, dans la continuité de la   première campagne ESTER . L'objectif principal était « de promouvoir une dynamique de l'évaluation en  France, autour du traitement de la parole de langue française, de mettre en place une structure pérenne  d'évaluation et de diffuser le plus largement possible les informations et les ressources concernées par ces  évaluations»                                                     . Ces campagnes visaient à  évaluer les performances des systèmes de transcription de la  parole,  les performances des systèmes de segmentation en tours de paroles, et la capacité à extraire  automatiquement des informations, en particulier les entités nommées (EN). Cette troisième tâche, à  laquelle se sont attelés 7 participants dans le cadre d'ESTER 2, est l'objet de cet article. Elle était divisée  transcriptions automatiques (NE-asr).     Dans le cadre d'ESTER2, il s'agissait d'extraire et de catégoriser des mentions directes d'EN, selon un   guide d'annotation comprenant 7 catégories principales et 38 sous-catégories :     La principale instruction d'annotation est de considérer les entités  en contexte, avec la prise en compte des  phénomènes d'ambiguïtés et de métonymie : par exemple, selon les contextes, «  Charles de Gaulle » doit  être annoté en tant que personne (le président), véhicule (le porte-avion) ou encore lieu (l'aéroport).  L'annotation des noms de personnes inclut celle des fonctions et l'annotation des expressions temporelles  couvre pour sa part un large éventail de possibilités, des classiques «  Lundi matin » aux plus complexes  «  Il y a un peu moins de trois jours environ ». Par ailleurs, dans la mesure où l'extraction d'entités est  réalisée sur des transcriptions de la parole, certains phénomènes propres à l'oral (hésitations ou répétitions)  doivent être inclus dans les annotations (<pers.hum>  Jacques  heu  Chirac</pers.hum>). Ces directives  d'annotation spécifiques, combinées au nombre important de catégories à prendre en compte,  complexifient la tâche d'annotation. En effet, les quantités de type âge et durée sont particulièrement  difficiles à distinguer des expressions temporelles, tout comme les lieux administratifs des entités  géopolitiques, puisqu'il s'agit de noms de villes ou de pays fréquemment employés en tant que l'un ou  l'autre. On peut donc constater que cette tâche est plus ambitieuse que l'extraction d'EN « classique » (i.e.  à la MUC).   Se mettre d'accord sur la manière d'annoter des EN n'est pas chose facile. Ce problème bien connu n'a   pas manqué d'apparaître durant ESTER2, avec de nombreuses discussions et remises en cause du guide  d'annotation, modifié au fur et à mesure de la campagne jusqu'à une version définitive en janvier 2009.  Les points délicats ont concerné, parmi d'autres, les expressions temporelles et les fonctions. Pour ce qui  est des premières, il fut principalement question de l'extension des expressions temporelles (inclusion ou  non des prépositions, déterminants et relatives), des difficiles distinctions entre dates et durées, et entre une  expression temporelle et une autre qui ne l'est pas. Concernant les fonctions,  deux points furent soulevés:  le manque de critères pour définir la portée de cette catégorie d'une part (il est facile de lister des fonctions  « standards » mais bien d'autres posent problèmes) et, d'autre part, la pertinence d'annoter conjointement,  comme il était demandé dans certains cas, personnes et fonctions (n'est-t-il pas préférable, d'un point de  vue sémantique, d'annoter des relations entre noms de personnes et noms de fonction ?).   Nous avons participé à  la campagne d'évaluation ESTER2 en adaptant l'analyseur syntaxique robuste   « Xerox Incremental Parser » (XIP,  (Ait-Mokthar et al., 2002)). XIP prend en entrée du texte tout venant,  sous format texte ou XML,  et produit en sortie de façon robuste une analyse syntaxique profonde. A partir  d'un ensemble de règles, l'analyseur désambiguise les catégories, construit les syntagmes noyaux et extrait  des relations de dépendances syntaxiques. En plus de l'analyse des relations syntaxiques de surface, XIP  effectue également une analyse syntaxique dite « profonde »  ou  « normalisée »  (prise en compte des  sujets et objets de verbes non finis, normalisation de la forme passive en forme active, etc.). Cet analyseur  intègre également un module de reconnaissance des entités nommées (Rebotier 2006), prenant en compte  les types classiques d'entités nommées, à savoir les expressions numériques, les monnaies, les dates, ainsi  que les noms de lieux, de personnes et d'organisations.  Il s'agit d'un module à base de règles, consistant en  un ensemble de règles locales ordonnées utilisant des informations lexicales et des informations  contextuelles concernant les parties du discours, les formes lemmatisées et un ensemble de traits lexicosémantiques.    Nous avons dû adapter le système développé pour le français aux consignes d'annotation ESTER 2, selon   les axes suivants :        Adaptation aux spécificités de la transcription : Les transcriptions de la parole, manuelles ou  automatiques, ont des particularités que l'on ne retrouve pas dans les textes « standards » ; il  s'agit de  disfluences, de répétitions, ou encore de bruits :   «   Il y a encore euh quelques mois... », « Une forme de de journalisme ... », « [rires-en-fond-] Voila !  [-rire-en-fond] »     Afin d'ignorer les disfluences et les bruits, nous avons converti les fichiers d'entrée originaux sous  format XML, en marquant ces éléments comme des balises ouvrantes/fermantes (</heu>, </[rires]>)   totalement transparentes pour les traitements linguistiques. Dans le cas des répétitions, nous avons  développé des règles qui groupent ces éléments sous un noeud de même catégorie qui hérite des traits  du premier élément.        Adaptation pour les catégories «standards » :  Nous avons tout d'abord utilisé les corpus  d'entraînement et de développement pour collecter semi-automatiquement le vocabulaire inconnu  (noms de lieux, d'organisations, etc.) et l'intégrer à nos lexiques. Nous avons ensuite adapté le système  pour prendre en compte de nouvelles catégories, telles que les fonctions, les âges, les productions  humaines, et la plupart des quantités, qui n'étaient pas préalablement couvertes par notre système.  Nous avons également adapté les règles existantes selon le guide d'annotation, en particulier pour  couvrir la portée des entités ; par exemple, les déterminants et prépositions sont inclus dans les  quantités et les noms de fonction en apposition d'un nom de personne sont inclus dans ce dernier :      «   Il est âgé  <amount.age> de 18 ans </amount.age>  « <pers.hum > Nicolas Sarkozy, président de la république </pers.hum> ... »  Il s'est principalement agit ici de développer et de modifier des règles locales de regroupement des  noms propres, en amont de l'analyse en syntagmes noyaux (chunks).      Traitement des expressions temporelles : Le vocabulaire relatif aux expressions temporelles étant  une liste fermée, le coeur du travail fut l'écriture de règles locales et de chunking. L'attention fut portée  sur les prépositions et adverbes principalement, ces derniers affectant radicalement le sens de telle ou  telle expression ( Il est parti <amount.phys.dur> pendant 10 mois </amount.phys.dur>  vs. Il est parti  incidences de la transcription de parole, comme par exemple avec l'expression «  19 cent 97 ».       Ambiguïtés et métonymies :  Une des spécificités les plus intéressantes d'ESTER 2 est la prise en  compte des ambiguités et des phénomènes de métonymies. Afin d'être à même de traiter ces cas, nous  avons utilisé les résultats de l'analyse syntaxique profonde fournis par XIP. En nous référant au guide  d'annotation, nous avons réalisé une étude de corpus pour détecter les régularités syntaxiques et  lexicales déclenchant un glissement métonymique ou permettant de résoudre une ambiguïté, selon la  méthodologie appliquée dans (Brun et al 2007). Cette étude a conduit à des hypothèses telles que « Si  un nom de lieu de type administratif est sujet d'un verbe de communication, il est employé comme  nom d'organisation géopolitique». L'analyseur fut alors enrichi par des lexiques sémantiques dédiés et  par des règles de dépendance modifiant l'interprétation des entités, appliquées en aval de l'analyse  syntaxique, par exemple :   Cette règle peut s'appliquer sur une phrase comme «  Dakar parle de 28 millions d'euros », alors  annotée « <org.gsp>  Dakar  </org.gsp> parle de 28 millions d'euros ». Notre étude s'est concentrée   sur les relations de type sujet, objet, modifieur (nominal et propositionnel) et attribut, et nous a  conduites à développer environ 150 règles de dépendances supplémentaires.     Comme dit précédemment, nous avons utilisé les corpus d'entraînement (100 heures d'émissions de   radio transcrites et annotées manuellement) et de développement (6 heures de journaux radiophoniques  transcrits et annotés manuellement)   pour la mise au point du système.  Le corpus de test était constitué de  7 heures de journaux radiophoniques datant de 2008. L'ensemble des corpus provenait de différentes  sources : France Culture, France Inter,  Radio France International, Radio Classique, Africa 1, Radio  Congo et Radio Télévision Marocaine. Du point de vue de l'évaluation quantitative, même si les mesures  classiques de précision et rappel étaient calculées, la mesure « officielle » était le « Slot Error Rate » (SER,  voir (M et al. 1999)), qui combine et pondère les différents type d'erreurs (insertion, effacement,  erreur de type) : SER = (Insertions+Effacements+Substitutions) / nb entités ref. C'est une mesure analogue  au « Word Error rate » (WER) utilisé pour mesurer les performances des systèmes de transcriptions de la  parole. D'autre part, si au début de la campagne il était prévu d'évaluer sur l'ensemble des sous-types,  c'est seulement sur les 7 catégories principales que les résultats ont été calculés. Enfin, les résultats  définitifs ont été obtenus après une phase d'adjudication qui permettait aux participants de contester les  annotations du corpus de test (sans bien évidemment changer les résultats de leur système).   Le tableau I présente les résultats obtenus par notre système sur transcriptions de référence (NE-Ref) en   termes de précision, rappel, f-mesure et  slot error rate. Les résultats de notre système sur les transcriptions  automatiques (NE-Asr) sont publiés dans (Brun et Ehrmann, 2009), les résultats complets de la campagne  dans (Galliano et al, 2009).  Avec un SER de 9.80 (et une f-mesure de 0,93), ces résultats s'avèrent très                                                      résultats « standards» dans ce genre de compétition, ce qui montre l'impact (et la difficulté) du traitement  de la métonymie, beaucoup d'erreurs venant de la confusion  loc.admi et org.gsp. Un constat équivalent  pour la catégorie  amount,  habituellement assez simple, peut être fait, dû aux ambiguïtés entre durées et  âges d'un coté et expressions temporelles de l'autre. Une dernière remarque concerne les noms de  productions humaines, dont le score est faible, en raison de leur faible représentation dans le corpus et de  la diversité des éléments que cette catégorie est censée couvrir : des véhicules aux titre d'oeuvres d'art en  passant par les documents légaux.    Nous avons trouvé intéressant de poursuivre de notre coté la campagne ESTER 2 en calculant les scores   (initialement planifiés) pour l'ensemble des sous-catégories.   erreur est comptée si les catégories hypothèse et référence ne sont pas exactement les mêmes, même si la  catégorie générale est commune. Le tableau II montre que les résultats restent très satisfaisants  globalement, mais on constate cependant une chute importante du rappel. Cette chute est particulièrement  marquée pour les noms d'organisations, ce qui indique que leurs sous-types sont encore mal distingués par  notre système.    Cet article décrit notre participation  à  la tâche de reconnaissance des entités nommées de la campagne   d'évaluation ESTER 2, qui s'est terminée en juin 2009. Nous avons adapté un système d'extraction  d'entités nommées préexistant au sein d'un analyseur robuste, XIP. La finesse d'annotation requise lors de  cette campagne nous a ainsi poussées à  utiliser les résultats de l'analyse syntaxique profonde, en  particulier pour le traitement des problèmes d'ambigüités sémantiques et de métonymies. Les résultats  obtenus sur transcriptions manuelles, pour l'annotation en catégories générales,  étaient très satisfaisants.  L'expérience que nous avons menée  a posteriori sur l'annotation en catégories fines a permis de mettre en  évidence certains éléments à améliorer dans notre système.    D'une façon générale, la participation à  cette campagne s'est avérée extrêmement bénéfique pour notre   système de repérage des entités nommées. Mais peut-être encore plus crucialement, cette évaluation a  permis aux participants de mener une réflexion approfondie sur les problèmes d'annotations des entités  nommées : quels sont les critères pour décider qu'une unité linguistique est une entité nommée, quel est  l'étiquette  à  donner dans un contexte donné, quels sont leurs frontières, etc. Cette réflexion a permis  d'aboutir à une première version d'un guide d'annotation qui vise à devenir un standard pour le français.    
