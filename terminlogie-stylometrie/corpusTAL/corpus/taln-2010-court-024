L'obtention de la représentation sémantique des propositions orales a été l'objet de nombreux travaux  au cours des vingt dernières années. L'introduction de composants stochastiques dans les systèmes de dialogue oral améliore leurs performances globales en les rendant plus robustes aux variabilités de la parole (Lefèvre, 2007; He & Young, 2006). A partir des transcriptions fournies par le module de reconnaissance automatique de la parole, le module de compréhension construit une représentation sémantique de haut niveau des propos du locuteur qu'il transmet au gestionnaire de dialogue. Dans un travail précedent (Meurs et al., 2009), nous avons proposé un modèle entièrement stochastique basé sur des réseaux bayésiens dynamiques (DBN) extrayant les concepts de base de la requête d'un utilisateur puis générant les sous-arbres de frames sémantiques à partir de tous les niveaux d'annotation disponibles (mots et concepts). La génération des fragments sémantiques par les DBN étant séquentielle, elle ne tient pas compte des dépendances longue-distances. Cet article décrit l'algorithme de recomposition que nous appliquons aux fragments pour obtenir la représentation sémantique globale de la requête de l'utilisateur. Un apprentissage strictement statistique des fragments en contexte (voir (Zettlemoyer & Collins, 2009) pour une approche utilisant des grammaires) permet à l'algorithme proposé de s'appuyer sur un processus de décision par classification binaire. L'article est organisé comme suit. La section 2 présente la génération des frames sémantiques sur le corpus M . La section 3 introduit ensuite notre algorithme de composition des fragments sémantiques. Enfin, la section 4 détaille expériences et résultats. Issu de la simulation d'un serveur téléphonique d'informations touristiques et de réservation d'hôtels, le corpus M (Bonneau-Maynard et al., 2005) est composé de 1.257 dialogues en français collectés en utilisant le protocole du Magicien d'Oz (un opérateur humain simule les réponses du serveur). Transcription et annotation sémantique manuelles ont été réalisées par deux experts. A partir du dictionnaire sémantique M (83 concepts) des segments de mots sont associés à une paire concept-valeur. Le choix dans nos travaux d'annoter le corpus M en frames sémantiques (Lowe et al., 1997) est motivé par l'aptitude des frames à représenter les dialogues de négociation. Une frame décrit une situation concrète ou abstraite impliquant ses rôles, les frame-éléments (FE). Nous avons défini, selon le paradigme du projet FrameNet (Fillmore et al., 2003), des frames couvrant le domaine du corpus M et adaptées à la nature particulière du support textuel. L'ontologie M est composée de 21 frames et de 86 FE définis par des modèles composés d'unités lexicales et de concepts de base. Les données d'entraînement sont automatiquement annotées par un système à base de règles (voir (Meurs et al., 2008)). Une annotation de référence en frames et FE est ainsi obtenue permettant l'apprentissage des paramètres des modèles stochastiques utilisés pour générer les fragments sémantiques de frames-FE associés à la proposition du locuteur.  Ces modèles à base de DBN (Bilmes & Zweig, 2002) sont exposés et évalués dans (Meurs et al., 2009).  L'apprentissage de leurs paramètres nécessite la détermination des fragments sémantiques associés aux concepts (et aux mots) de la requête du locuteur. La représentation en frames étant hiérarchique, des situations de recouvrement peuvent se produire lors de la détermination des frames et FE associés à un concept. Pour résoudre ce problème, un algorithme de projection d'arbre est appliqué sur l'annotation en frames et FE de la phrase complète. Il permet de définir les fragments sémantiques (sousbranches) associés à un concept. Partant d'une feuille de l'arbre, un fragment de frames-FE est obtenue en agrégeant les valeurs de noeuds pères aussi longtemps qu'ils sont associés au même concept (ou à aucun). Par exemple, la séquence de mots réserver un hôtel à Paris, illustrée figure 1, entraîne la création des branches projetées de frames et FE et . La séquentialité du décodage des fragments entraîne la perte d'une partie des liens entre frames et FE.  L'algorithme de projection réalise deux types d'opérations. Les séparations rompent des liens entre frames  et FE selon les concepts qui leurs sont associés. Les duplications des objets sémantiques (frames ou FE) sont nécessaires lorsque ces objets sont présents dans plusieurs sous-branches distinctes. L'algorithme de recomposition est développé pour rassembler les fragments produits par les DBN et rétablir l'arbre sémantique associé à la globalité du message. Il décide des opérations réciproques de celles effectuées lors de la projection, soit des opérations de liaison entre frames et FE et des opérations d'identification entre frames ou FE.  Les liaisons potentielles inter-fragments s'appuient sur l'ontologie développée pour le domaine du corpus  M : deux objets sémantiques ne peuvent être reliés que s'ils le sont dans l'ontologie. Les liaisons consistent donc en l'ajout d'arêtes entre des noeuds de fragments sémantiques distincts pour les rassembler sous un arbre sémantique unique. Les identifications potentielles concernent les objets sémantiques semblables présents au sein de plusieurs fragments associés à un même message. L'algorithme de recomposition considère ces objets et décide de la pertinence de leurs présences multiples. Les identifications suppriment ainsi les objets sémantiques redondants produits par les DBN. Lorsque deux noeuds de sousbranches sont identifiés, un seul est conservé dans l'arbre sémantique global et les noeuds fils du noeud supprimé sont reliés au noeud conservé.  La seconde méthode de connexion évaluée est basée sur l'apprentissage de classifieurs SVM. Le choix  du type de classifieurs linéaires employés est dicté par plusieurs considérations : la quantité de données disponibles, la rapidité de réponse ou encore les performances obtenues sur des données comparables. En raison de leurs propriétés, les classifieurs SVM s'adaptent parfaitement au contexte applicatif de ce travail.  Apprentissage  Les séparations et les duplications réalisées lors de la projection des arbres sont recensées. A chaque opération est associé l'ensemble des exemples du corpus d'entraînement contenant les objets sémantiques qu'elle fait intervenir. Ces messages sont répartis en deux classes selon qu'ils ont ou non déclenché l'opération. On dispose de T , ensemble des exemples d'apprentissage annotés en arbres sémantiques par le système à base de règles évoqué en section 2. Soit A l'ensemble construit à partir de T tel que tout élément de A est composé des mots, des concepts et de l'arbre sémantique associés à un exemple de T . Soit A l'ensemble construit à partir de A tel que tout élément de A est composé des mots, des concepts, des fragments sémantiques obtenus après projection de l'arbre sémantique et des opérations de projection réalisées lors de la projection de l'arbre sémantique d'un exemple de A. Soient O l'ensemble des opérations observées dans A . Par souci de simplification, une opération de projection et sa réciproque de recomposition (ou regroupement) seront également notées O , le contexte d'application levant toute ambigüité.  Chaque opération de projection O   O met en relation deux objets sémantiques f et f et on notera O = f Rf . Pour chaque paire {f , f } associée à une opération de O, on construit l'ensemble A des exemples de A contenant f et f . Les exemples de A pour lesquels l'opération O s'est appliquée lors de la projection sont dits "positifs" pour O . Les exemples A contenant f et f pour lesquels O n'a pas été appliquée sont "négatifs" pour O . On dispose pour chaque opération O de la partition {A , A } de A où A et A sont respectivement les sous-ensembles d'exemples positifs et négatifs de A .  Pour appliquer la méthode de classification SVM, il est nécessaire de plonger les données dans R  . Un exemple E est représenté dans R par un point E dont les coordonnées sont les index numériques des mots et trigrammes de mots de l'exemple, de la séquence de concepts associée à l'exemple et des frames et FE présents dans les fragments sémantiques associés à cet exemple. L'introduction des n-grammes de mots dans le point caractérisant un exemple permet de prendre en compte une information séquentielle. Les paramètres d'un classifieur linéaire binaire à base de SVM sont appris sur les points représentant les exemples de chaque ensemble A . A l'issue de cette procédure, on dispose d'un classifieur S par opération O et on a |O| = |S| = I, avec S l'ensemble des classifieurs entrainés.  Application aux exemples de l'ensemble de test  Pour chaque exemple E de l'ensemble de test, annoté en fragments sémantiques, on construit l'ensemble des opérations pouvant le concerner en fonction des paires d'objets sémantiques contenues dans ses fragments. Soit O cet ensemble, on a : O = {O = f Rf tel que f et f appartiennent aux fragments sémantiques associés à E} et O  O. Pour toute opération O  O , le point E représentant l'exemple E est soumis au classifieur S . La réponse de S quant à la classe de E détermine la pertinence de la réalisation de O sur les objets sémantiques de l'exemple. A l'issue de ce processus, la phase de composition sémantique est achevée par la réalisation sur les objets sémantiques de E de toutes les opérations jugées pertinentes par les S . Ces deux méthodes sont évaluées sur les fragments sémantiques produits par le DBN.  Les expériences sont menées sur l'ensemble de test M  (3005 tours de parole) dans trois conditions différentes, fonctions de la nature des données utilisées. Les données de type MAN rassemblent les tours de parole du locuteur, manuellement transcrits et annotés en concepts. Pour celles de type SLU, les concepts de base sont décodés à partir des transcriptions manuelles des tours de parole locuteur en utilisant le modèle à base de DBN décrit dans (Lefèvre, 2007) (taux d'erreurs concepts : 10,6%). Pour les données de type ASR+SLU, les concepts sont décodés par le modèle de compréhension en utilisant la meilleure hypothèse de séquence de mots générée par un système conforme à (Barrault et al., 2008) (taux d'erreurs : mots 27%, concepts 24,3%).  Quatre niveaux d'évaluation sont considérés. Au niveau Frames, les hypothèses de frames sont correctes  dès lors que les frames correspondantes sont présentes dans la référence, idem pour le niveau FE. Au niveau FE{Fr}, seules les hypothèses de FE appartenant à des hypothèses de frames correctes sont examinées et au niveau Liens{Fr}, seules les hypothèses de liens reliant des hypothèses de frames et FE correctes sont examinées.  Toutes les expérimentations ont été réalisées en utilisant la librairie libSVM (EL-Manzalawy & Honavar,  2005) pour WEKA (Bouckaert et al., 2008). Le tableau 1 regroupe les résultats issus de l'application des méthodes CF et SVMDP sur les fragments sémantiques issus des 3005 tours de parole de test (5,5 frames ou FE par tour en moyenne) ; des 515 tours contenant plus de 6 segments conceptuels (17,5 frames ou FE par tour en moyenne) et des 223 tours contenant plus de 10 segments conceptuels (24,2 frames ou FE par tour en moyenne). Les résultats sont donnés en termes de F-mesure pour les trois types de données (MAN, SLU, ASR+SLU).  Trs de parole  test complet + de 6 concepts + de 10 concepts Données F -m F -m F -m MAN CF SVMDP CF SVMDP CF SVMDP Frames 0,90 0,91 0,85 0,87 0,84 0,87 FEs 0,87 0,88 0,73 0,74 0,72 0,74 FE{F} 0,95 0,94 0,85 0,83 0,84 0,81 Liens{F} 0,88 0,87 0,67 0,64 0,66 0,61 SLU CF SVMDP CF SVMDP CF SVMDP Frames 0,89 0,90 0,83 0,85 0,82 0,85 FEs 0,82 0,83 0,65 0,67 0,66 0,68 FE{F} 0,91 0,91 0,79 0,78 0,79 0,77 Liens{F} 0,84 0,84 0,61 0,60 0,60 0,58 ASR+SLU CF SVMDP CF SVMDP CF SVMDP Frames 0,80 0,81 0,77 0,79 0,78 0,80 FEs 0,77 0,77 0,61 0,62 0,62 0,63 FE{F} 0,91 0,90 0,78 0,76 0,77 0,75 Liens{F} 0,84 0,83 0,61 0,59 0,59 0,57  La méthode SVMDP utilise 74 classifieurs appris sur le corpus d'entraînement : 18 classifieurs sont dédiés  à l'identification de frames (10) ou de FE (8) et 56 classifieurs sont dédiés à la liaison entre frames et FE.  Les résultats confirment l'aptitude de ces algorithmes à composer les fragments d'arbre sémantiques de  grande taille pour former une représentation complète consistante du message de l'utilisateur. La structure de la base de connaissance et le contexte relativement fermé des messages de test peuvent expliquer les performances comparables des deux méthodes. En effet, les opérations d'identification et de liaison des objets sémantiques contenus dans les fragments étant presque toujours justifiées, la méthode de connexion forte commet finalement peu d'erreurs mais on notera que l'emploi de la méthode SVMDP permet une sélection plus fine des frames et FE.  Cet article présente et évalue un processus de décision à base de SVM pour la composition de fragments  sémantiques (sous-arbres de frames). Les fragments sont séquentiellement décodés par un modèle entièrement stochastique à base de réseaux bayésiens dynamiques. La composition de ces fragments est réalisée par un processus de décision SVM dépendant du contexte. Les expériences menées sur le corpus M attestent la validité de notre approche. Les performances du système proposé confirment son aptitude à produire automatiquement une représentation sémantique consistante de la requête d'un utilisateur.  
