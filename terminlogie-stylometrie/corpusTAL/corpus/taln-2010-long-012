  Ce modèle est construit au cours d'un second parcours de nos corpus parallèles, dont l'objectif est de  recenser les normalisations associées aux séquences du lexique KN. Au cours de ce second parcours et contrairement au précédent, les séquences du lexique sont recherchées dans le corpus quel que soit le contexte (séparateurs ou non), afin d'assurer le recensement de toutes les normalisations possibles.  Chaque normalisation kn d'une séquence connue kn est pondérée comme suit :   p(kn|kn) =  Occ(kn, kn) Occ(kn) (8)  où Occ(x) note le nombre d'occurrences de x dans le corpus. Le FST R  est ensuite construit comme suit : R = S KN ( S KN ) S (9)  où :  - KN est un lexique pondéré, dans lequel chaque séquence KN est associée à la liste pondérée de ses normalisations. - S est un lexique pondéré, dans lequel chaque séparateur est associé à la liste de ses normalisations possibles. Souvent, la suppression du séparateur est l'une des normalisations possibles. Lorsque ce n'est pas le cas, cette possibilité de suppression (DEL) est ajoutée, et pondérée comme suit :  p(DEL|kn) =  0.1 Occ(kn) + 0.1 (10)  Les deux modèles précédents étaient des expressions régulières construites à partir de lexiques pondérés.  Celui-ci, par contre, correspond à une liste de règles de réécriture pondérées    / w, apprises à partir de l'alignement, où le remplacement    se voit attribuer le poids w. Pourquoi cette différence de modélisation ? Les expressions régulières des modèles précédents avaient pour objectif de contraindre le langage accepté, et plus particulièrement la place des séparateurs, de manière à forcer le système à favoriser certains solutions. Dans le cas des séquences inconnues, nous savons que dans l'absolu, tout doit être possible . Il n'était donc pas nécessaire de définir un langage différent de  .  Les cibles de nos règles () sont des séquences de 1 à 5 caractères prises du côté SMS de l'alignement,  tandis que les réécritures () sont leurs normalisations correspondantes. Une contrainte importante exprimée sur les listes de règles est que les règles sont classées de la plus spécifique à la plus générale, de  sorte qu'une règle donnée n'est appliquée que si aucune règle plus spécifique et plus pertinente n'a été  rencontrée plus haut dans la liste. Pour cette raison, nos règles sont classées dans l'ordre décroissant de la longueur de leurs cibles, afin que les règles aux cibles les plus longues soient choisies le plus souvent possible. Ceci réduit le nombre de normalisations proposées pour une séquence donnée, puisque les séquences les plus longues ont tendance à présenter moins de normalisations différentes.  Du large ensemble de séquences possibles de 2 à 5 caractères collectées dans le corpus, nous n'avons gardé  dans notre liste de règles que les séquences qui autorisent au moins une normalisation faite exclusivement de mots appartenant à la langue : lors du recensement des séquences candidates dans le corpus, nous avons systématiquement vérifié chaque forme normalisée dans un lexique de formes françaises standard. Le lexique utilisé contient 430 000 formes fléchies et est dérivé de la base de données lexicales Morlex .  Notre modèle de langue statistique est un 3-gramme de formes lexicales, lissé par interpolation linéaire  (Chen & Goodman, 1998), estimé sur la partie normalisée de notre corpus d'entraînement et compilé en un FST pondéré LM .  A ce stade, ce FST ne peut pas être combiné avec nos autres modèles, parce que l'alphabet sur lequel il  est défini est fait de formes lexicales et non de caractères. Ce problème est résolu en composant LM avec un autre FST L, qui représente un lexique associant chaque mot, considéré comme une séquence de caractères, avec le même mot, considéré cette fois comme une forme lexicale. Les formes lexicales sont ensuite supprimées définitivement du modèle de langue en ne conservant que la première projection (l'entrée) de la composition : LM = FirstProjection( L  LM ) (11)  L'efficacité et les performances de notre système ont été évaluées sur un MacBook Pro, Intel Core Duo  2,4 GHz, 4 Go SDRAM 667 MHz DDR2, tournant sous Mac OS X version 10.5.8. L'évaluation a été réalisée sur le corpus de 30 000 SMS présenté en section 4, par validation croisée en 10 blocs (Kohavi, 1995). Le principe de cette méthode d'évaluation est de diviser le corpus initial en 10 blocs de taille égale (ici, 3 000 SMS). Le système est ensuite entraîné et testé 10 fois, chaque bloc étant à son tour exclu du corpus d'entraînement, mais le seul à servir de corpus de test. Le tableau 1 présente le nombre moyen d'entrées/sorties des 10 modèles appris au cours de la validation croisée. Si les séquences inconnues (de 1 à 5 caractères) sont beaucoup moins nombreuses que les séquences connues, leur nombre de réécritures est par contre significativement plus élevé, ce qui est dû au fait que ces séquences sont sélectionnées indépendamment des séparateurs éventuels. Malgré le grand nombre de séquences connues apprises, le système a cependant traité en moyenne 85% des séquences SMS à l'aide du modèle UNK.  Avec une vitesse moyenne de 1836,57 caractères/sec (écart type de 159,65), le système traite un SMS  de 140 caractères en 76,23 ms (écart type de 22,34 ms). Le système semble donc efficace, étant donné le  KN  Réécritures KN UNK Réécritures UNK Lexèmes n-grammes Total 41 281 49 152 12 225 69 841 19 801 515 128 Rapport 1,19 5,71 26,01  1. Notre approche  Validation croisée, français Copie Hybride ¯ x  ¯ x  Sub. 25,90 1,65 6,69 0,45 Del. 8,24 0,74 1,89 0,31 Ins. 0,46 0,08 0,72 0,10 WER 34,59 2,37 9,31 0,78 SER 85,74 0,87 65,07 1,85 BLEU 0,47 0,03 0,83 0,01 ¯ x=moyenne, =écart type 2. Autres approches En français En anglais Guimier Kobus 2008 Aw Choudury Cook 2007 1 2 2006 2007 2009 11,94 2,36 2,21 16,51 10,82 41,00 44,60 76,05 0,736 0,8 0,81  temps considérable passé dans le modèle UNK. Sur ce point, il n'est malheureusement pas possible de  proposer une comparaison avec les autres systèmes, qui ne fournissent pas cette information.  Le tableau 2, partie 1, présente les performances de notre approche (Hybride) et les compare à un simple  copier-coller du SMS (Copie). Nous avons évalué le système en termes de score BLEU (Papineni et al., 2001), de taux d'erreur à la phrase (Sentence Error Rate, SER), et de taux d'erreur au mot (Word Error Rate , WER), le WER se subdivisant lui-même en substitutions (Sub.), suppressions (Del.) et insertions (Ins.). Les résultats du copier-coller donnent une idée du bruit réellement présent dans le corpus SMS, et mettent en évidence le fait que notre système a encore des difficultés à réduire le SER, alors que les résultats en termes de WER et de score BLEU sont plutôt encourageants.  Le tableau 2, partie 2, reproduit les résultats des autres approches de la littérature. La plupart des résultats  sont cependant difficiles à comparer aux nôtres, parce qu'ils ont été obtenus soit dans une langue différente (l'anglais), soit sur un corpus différent : c'est le cas de Kobus et al. (2008), qui d'une part ont combiné le corpus que nous avons utilisé à un autre corpus SMS, et d'autre part ont réalisé un seul test, basé sur un corpus d'entraînement plus important (36 704 SMS) pour un corpus de test comparable à l'un de nos blocs (2 998 SMS). Les seuls résultats véritablement comparables sont ceux de Guimier de Neef & Fessard (2007), qui ont évalué leur approche sur le même corpus que nous, mais sans validation croisée, parce que leur système expert ne nécessite pas d'apprentissage. Quoi qu'il en soit, le tableau 2 montre que notre méthode supporte très bien la comparaison avec les meilleures méthodes antérieures. L'analyse des normalisations produites par notre système a mis en évidence trois caractéristiques importantes :  1. Les séparateurs manquants (Pensa ms  Pense à mes) ou superflus (G t  J'étais) sont globalement   bien gérés, ce qui est reflété par nos taux de suppression et d'insertion réduits.   2. Le prétraitement est utile, puisque les unités non ambiguës ne sont pas modifiées.  3. Les erreurs sont souvent contextuelles : elles concernent le genre (quel(le)), le nombre (bisou(s)), la personne ([tu t']inquiète(s)) ou le temps (arrivé/arriver). Cependant, comme le soulignent Kobus et al. (2008), la fréquence de ces erreurs n'est pas surprenante en français, langue dans laquelle les modèles n-grammes sont souvent incapables de modéliser cette information, hors de leur portée. Dans cet article, nous avons présenté une normalisation SMS basée sur des machines à états finis et développée dans le contexte d'un système de synthèse de la parole à partir de SMS. Afin d'éviter la modification erronée des unités non ambiguës, nous avons conçu une méthode hybride, entre correction et traduction. Notre algorithme de normalisation est original à deux niveaux. Premièrement, il repose entièrement sur des modèles appris. Deuxièmement, le modèle de réécriture appliqué à un segment d'unité bruitée change selon que le segment est connu ou non.  Evalué par validation croisée, le système semble efficace, et les performances en termes de score BLEU  et de WER sont plutôt encourageantes. Cependant, le SER reste trop élevé, ce qui met en évidence le fait que le système a besoin d'être amélioré.  Avant tout, la normalisation devrait mieux modéliser les similarités phonétiques, au vu du grand nombre de  jeux phonétiques dans les SMS. Le modèle phonétique, par exemple, devrait savoir que o, au, eau, . . ., aux se prononcent [o], tandis que è, ais, ait, . . ., aient sont souvent prononcés [E]. Cependant, contrairement à Kobus et al. (2008), nous pensons que ce modèle doit éviter l'étape de conversion graphèmes-phonèmes, qui empêche aux étapes suivantes d'identifier les graphèmes présents dans la séquence initiale. A la place, nous proposons d'apprendre les similarités phonétiques à partir d'un dictionnaire de mots accompagnés de leurs transcriptions phonétiques, et de construire des règles graphèmes-graphèmes. Ces règles pourraient ensuite être pondérées, en apprenant leurs fréquences à partir de nos corpus alignés. Ce modèle devrait également autoriser les variations de timbre, comme [e]-[E], afin d'accepter des similarités entre graphèmes fréquemment confondus en français, comme ai ([e]) et ais/ait/aient ([E]).  Il serait également intéressant de tester l'impact d'un autre modèle de langue lexical, entraîné sur des  phrases non-SMS. En effet, le modèle lexical présente un inconvénient majeur dans le contexte de messages SMS : il doit être appris sur des formes standard, ce qui, dans le contexte des SMS, implique la retranscription du corpus, un processus coûteux qui réduit le nombre de données d'entraînement du modèle. . . Le corpus qui remplacerait le corpus SMS retranscrit devrait cependant partager deux points communs avec le langage SMS : il devrait mimer la syntaxe de l'oral et être le plus spontané possible. Sur la base de ces contraintes, notre intention est de récolter des phrases de forums Internet, en sélectionnant ces forums avec soin, parce que leurs textes partagent un autre point commun avec les SMS : ils sont bruités. De ce fait, l'idée est de choisir un forum dont la philosophie est explicitement d'éviter l'utilisation du langage SMS et d'accorder de l'importance à l'orthographe et à la grammaire.  La dernière amélioration que nous proposons ici est plus orientée correction : l'idée est d'autoriser la  correction orthographique à l'intérieur des modules TAL du système. Placées à ce stade du processus, guidées par l'analyse morphosyntaxique et en combinaison avec elle, des méthodes de correction plus sophistiquées pourraient ainsi se focaliser sur le problème non trivial des erreurs contextuelles.  Cette recherche a été co-financée par les projets FIRST Post-Doc « Vocalise » (convention 716619) et  WIST2 « Expressive » (convention 616422) de la Région wallonne.  
