La détection et le typage des entités nommées sont des tâches pour lesquelles ont été développés à la fois des systèmes symboliques et probabilistes. Nous présentons les résultats d'une expérience visant à faire interagir le système à base de règles , développé sur des corpus provenant de l'AFP, intégrant la base d'entités Aleda et qui a une bonne précision, et le système L NE, entraîné sur des transcriptions de l'oral provenant du corpus ESTER et qui a un bon rappel. Nous montrons qu'on peut adapter à un nouveau type de corpus, de manière non supervisée, un système probabiliste tel que L NE grâce à des corpus volumineux annotés automatiquement par . Cette adaptation ne nécessite aucune annotation manuelle supplémentaire et illustre la complémentarité des méthodes numériques et symboliques pour la résolution de tâches linguistiques. Named entity recognition and typing is achieved both by symbolic and probabilistic systems. We report on an experiment for making the rule-based system , a high-precision system developed on AFP news corpora and relies on the Aleda named entity database, interact with L NE, a high-recall probabilistic system trained on oral transcriptions from the ESTER corpus. We show that a probabilistic system such as L NE can be adapted to a new type of corpus in a non-supervized way thanks to large-scale corpora automatically annotated by . This adaptation does not require any additional manual anotation and illustrates the complementarity between numeric and symbolic techniques for tackling linguistic tasks. Détection d'entités nommées, adaptation à un nouveau domaine, coopération entre approches probabilistes et symboliques. Named entity recognition, domain adaptation, cooperation between probabilistic and symbolic approaches. La reconnaissance d'entités nommées est une des tâches les plus étudiées du domaine du traitement automatique des langues. Reconnaître dans du texte ou de la transcription de parole les mentions d'entités nommées reste un préalable nécessaire avant toute tâche plus complexe telle que l'analyse syntaxique, l'analyse sémantique ou l'extraction d'informations. Ainsi, la tâche de reconnaissance d'entités nommées fait l'objet de nombreuses campagnes d'évaluation depuis plus d'une vingtaine d'années, dont les premières ont été les campagnes MUC (Message Understanding Conference). Ces campagnes ont donné lieu à la construction de corpus de référence, notamment pour l'anglais, le chinois, l'espagnol ou le japonais. Pour le français, on peut citer notamment l'évaluation menée dans le cadre de la campagne ESTER sur des transcriptions de nouvelles, qui a donné naissance à un corpus français annoté en entités nommées selon des directives assez différentes de celles des campagnes MUC, notamment pour les emplois polysémiques et métaphoriques (Galliano et al., 2009). Pour une discussion plus précise de ces questions et des différents types d'ambiguïtés rencontrées en reconnaissance des entités nommées, on pourra se reporter à (Béchet, 2011).  Toutes les campagnes d'évaluation ont montré que la tâche de reconnaissance d'entités nommées peut être traitée  efficacement aussi bien avec des systèmes symboliques qu'avec des systèmes probabilistes. Elles ont également montré que les systèmes symboliques couplés à de très grands lexiques donnent de meilleurs résultats que les  systèmes probabilistes sur du texte canonique, en particulier en termes de précision. En revanche lorsque le texte à  traiter contient du bruit (absence de capitalisation, de formatage, de ponctuation, sortie d'un système de reconnaissance de la parole. . .), les systèmes probabilistes sont plus robustes et obtiennent de meilleurs scores en rappel, du moins tant que les données à annoter sont d'un genre similaire à celui du corpus de leur corpus apprentissage, voire que les entités nommées y sont globalement les mêmes.  C'est à partir de ce constat que nous avons décidé de tirer le meilleur parti des avantages des deux types de  systèmes, en effectuant des expériences d'adaptation non supervisée du système probabiliste L NE au domaine des dépêches d'agence, qui n'est pas celui du corpus ESTER sur lequel il est entraîné. Nous avons utilisé pour cela les annotations produites par le système symbolique couplé à la base d'entités Aleda, ayant été développé plus particulièrement pour le traitement de dépêches d'agence. Après avoir décrit successivement (section 2) et L NE (section 3), nous décrivons le détail de ce processus d'adaptation (section 4) puis en montrons les résultats sur le corpus d'évaluation formé de dépêches AFP développé par (Stern & Sagot, 2010a) (section 5).  Le système  , dont une version précédente a été décrite par Stern & Sagot (2010a), est un système de détection, typage et résolution d'entités nommées développé initialement pour le français et en cours d'adaptation à l'anglais. Bien que générique, il a été conçu en priorité pour le traitement de dépêches de l'Agence France-Presse. Il est donc adapté aux données traitées dans ce travail. est constitué de deux modules, l'un pour la détection ambiguë et l'autre pour la désambiguïsation et la résolution des mentions.  Les deux modules qui constituent  reposent sur Aleda, une base de données d'entités et de mentions possibles de ces entités construite automatiquement à partir de ressources librement disponibles. Aleda est distribuée librement en tant que composante du projet lexical libre Alexina . Une version préliminaire d'Aleda (intégrée à l'époque à la distribution de S Pipe) est décrite par Stern & Sagot (2010b). Aleda contient 855 403 entités et 2,32 million de variantes dénotationnelles qui dénotent des lieux (LOC), des organisations et entreprises (ORG), des personnes (PERS), des produits, des noms d'oeuvres, des noms de produits et de marques, des animaux remarquables (Dolly) et des personnages de fiction (Arsène Lupin). Ces entités ont été extraites principalement à partir de deux sources d'informations librement disponibles, à savoir pour les noms de lieu et la Wikipedia française pour les autres types d'entités . Chaque entité a un identifiant unique qui contient un identifiant de la ressource d'où elle a été extraite, un identifiant interne à chaque ressource (un identifiant ou un identifiant d'article de la Wikipedia française) et des informations supplémentaires dont un poids heuristique (le nombre d'habitants du lieu pour , le nombre de lignes de l'article correspondant dans Wikipedia).  Pour les noms de lieux, nous avons tout d'abord extrait de la volumineuse base  les entités dont le type a été jugé pertinent pour le traitement de dépêches d'agence (villes, pays, etc., mais pas les noms de montagnes par exemple). Même ainsi filtrée, la base contient beaucoup trop d'entités pour que toutes puissent être utilisées. Nous avons donc sélectionné heuristiquement un certain nombre d'entités jugées pertinentes au vu de la nature des données à traiter, à savoir des dépêches AFP émanant du bureau français de l'Agence (474 509 entités de lieu). Pour les autres types d'entités, nous nous sommes appuyés sur la Wikipedia française, dans la lignée de travaux antérieurs (Balasuriya et al., 2009; Charton & Torres-Moreno, 2009). Pour optimiser la couverture de la ressource extraite, nous avons procédé à une extraction en deux étapes, qui repose sur les catégories wikipedia . Ce mécanisme, bien qu'initié manuellement par un petit volume de données annotées, permet de récupérer un nombre très important d'entités nommées de la Wikipedia (400 169 entités). Pour chacune de ces entités, nous extrayons un nom normalisé qui est le titre de son article, un poids correspondant simplement au nombre de lignes de l'article, ainsi que différentes variantes dénotationnelles à partir des liens de redirection. Pour les noms de personnes, des variantes supplémentaires sont calculées avec prénoms abrégés et sans prénoms.  Le résultat de ces deux processus d'extraction est enrichi et corrigé par de courtes listes d'entités à éliminer et à  ajouter, développées à la main. Les 855 403 entités qui forment ainsi Aleda se répartissent de la façon suivante : 573 074 lieux et monuments, 215 179 personnes, 29 181 titres d'oeuvres, 25 247 organisations, 8 862 entreprises, 3 846 produits et marques, 997 personnages de fiction et 17 animaux remarquables.  Une grammaire non-contextuelle de 137 règles a été développée pour détecter et typer les mentions d'entités  nommées à partir des mentions présentes dans Aleda ; des motifs contextuels (p.ex. ville/localité/village de ou Dr/M./Mme. . . ) sont également utilisés. La reconnaissance est faite de façon ambiguë par l'architecture dag2dag de S Pipe (Sagot & Boullier, 2008). Des heuristiques de désambiguïsation sont appliquées pour réduire en partie cette ambiguïté. La sortie de ce module est donc un graphe (DAG) dans lequel chaque entité nommée candidate (empan et type) est représentée par une transition différente.  Un second module effectue alors deux tâches de façon conjointe : il résout les ambiguïtés d'empan et de type  (PERS, LOC, ORG. . .) et assigne à chaque mention une entrée dans la base (résolution). Comme pour la détection, ce module de typage et de résolution repose sur des heuristiques utilisant des informations qualitatives et quantitatives. Il fait par exemple usage du poids attribué aux entités par Aleda ainsi que de la notion de saillance pour favoriser les analyses impliquant des entités déjà rencontrées dans le même document (ici, la même dépêche) ou qui sont cohérentes avec le contexte (pays, ville) identifié dans le document.  Le processus de détection d'entités nommées dans un texte est composé de deux sous-tâches : une tâche de  segmentation consistant à trouver les bornes de début et de fin des entités ; une tâche de classification consistant à attribuer la bonne catégorie à l'entité détectée (PERS, LOC, ORG. . .). Ces deux tâches peuvent être effectuées de manière jointe si le processus de classification est appliqué au niveau des mots. Dans ce cas chaque mot appartenant à l'expression d'une entité nommée reçoit une étiquette correspondant à la fois au type de l'entité mais aussi à sa position à l'intérieur de celle-ci. Tous les mots ne participant pas à l'expression d'une entité reçoivent une étiquette vide. Trois étiquettes de position sont utilisés : B indique le début d'un chunk ; I indique que le mot est situé à l'intérieur d'un chunk mais ne le débute pas ; et O correspond à tous les mots hors chunks. En considérant le processus de détection des entités nommées comme un processus d'étiquetage, grâce aux étiquettes de positionnement et de type d'entités, toutes les méthodes développées dans le cadre de l'étiquetage en parties du discours (POS) peuvent être utilisées, notamment les méthodes numériques. Parmi celles-ci, deux principales approches ont été suivies : les modèles génératifs tels que les Modèles de Markov Cachés (Hidden Markov Models - HMM) (Bikel et al., 1999) et les modèles discriminants tels que MaxEnt (Brothwick et al., 1998) ou les Champs de Markov Aléatoires (Conditional Random Field - CRF) (McCallum & Li, 2003).  Le système L  NE utilisé dans cette étude est décrit dans Bechet & Charton (2010). Il est basé sur une approche mixte utilisant tout d'abord un processus génératif à base de HMM pour prédire une étiquette syntaxique (POS) et sémantique à chaque mot d'un texte ; ensuite un processus discriminant à base de CRF est utilisé pour trouver les bornes et le type complet de chaque entité en utilisant le modèle BIO présenté précédemment. Il y a deux raisons pour justifier l'emploi d'un HMM en amont d'un étiqueteur à base de CRF : - Tout d'abord les modèles d'étiquetage par HMM permettent de rajouter très facilement plusieurs sources d'information dans les estimations des probabilités des mots sachant les classes. Ils sont également assez tolérants au bruit dans les données d'apprentissage, à partir du moment où les fréquences relatives des différents événements modélisés sont respectées. - Ensuite cette première phase d'étiquetage va permettre d'enlever un certain nombre d'ambiguïtés en attribuant des étiquettes syntaxiques et sémantiques aux mots d'un texte. Cela permettra de simplifier l'étiquetage par le CRF qui pourra se concentrer sur les aspects de segmentation et d'attribution d'une étiquette finale. Les modèles HMM et CRF sont appris sur un corpus annoté selon le format suivant (sur la phrase d'exemple "Investiture aujourd'hui à Bamako Mali du président Amadou Toumani Touré") :  Comme décrit dans Bechet & Charton (2010), le système L  NE a été développé dans le cadre de la campagne d'évaluation ESTER portant sur la transcription et l'annotation en entités nommées d'émissions de radio, principalement des journaux d'information. Les corpus d'apprentissage utilisés sont ceux de la campagne ESTER, constitué d'une centaine d'heures de transcriptions annotées en entités nommées. Le corpus cible de cette étude est un corpus de dépêches AFP. Bien que le contenu thématique de ces deux cadres applicatifs soit proche (des données d'actualité), le type de langue utilisé est différent : transcriptions de l'oral pour le corpus ESTER provenant de sources multiples (France Inter, RFI, Radio Africa, Radio Tele Maroc, etc.). L'étude des résultats obtenus après application du système L NE au corpus AFP nous a permis de distinguer deux types de problèmes : - manque de couverture lexicale : les données ESTER contiennent des émissions diffusées en 2007 et début 2008 alors que les dépêches AFP analysées couvrent des périodes plus récentes ; - mauvaise modélisation des phénomènes spécifiques à l'écrit : le corpus ESTER étant composé de transcription de l'oral, il ne contient aucun « raccourci » spécifique à l'écrit tels que l'emploi d'abréviations (la graphie M. par exemple dans la séquence M. Dupond) ou l'ajout d'informations complémentaires par l'emploi de caractères de formatage (p.ex. les incises avec des parenthèses telles que : Le syndicat Force Ouvrière (FO) a annoncé. . .). L'adaptation d'un système statistique à un nouveau domaine nécessite généralement l'annotation manuelle d'un corpus d'adaptation couvrant les nouveaux phénomènes à modéliser. Ce corpus d'adaptation est souvent de taille modeste en raison du coût élevé des annotations en entités nommées. Comme indiqué dans l'introduction, nous avons exploré une voie alternative qui consiste à annoter automatiquement un corpus de très grande taille. Nous avons utilisé pour cela le système , spécialisé dans notre domaine cible, celui des dépêches d'agence. L'objectif est d'obtenir sans coût supplémentaire (aucune supervision n'est nécessaire) un système statistique adapté à un domaine particulier à partir d'un système symbolique existant. Ces deux types de systèmes ayant des particularités complémentaires (précision élevée pour le système symbolique, rappel important et robustesse au bruit pour le système statistique), il est intéressant de disposer des deux selon les applications visées.  La procédure d'adaptation non supervisée utilisée ici a consisté tout d'abord à collecter un corpus d'adaptation  brut, non annoté, appelé C . Nous avons utilisé ici un corpus de dépêches de l'AFP des années 2009 et 2010 constitué de 83M de mots (3M de phrases). Nous avons annoté en entités nommées le corpus C avec le système . Nous avons ainsi détecté 3,2M d'occurrences d'entités nommées, représentant 168K entités distinctes.  Le corpus C  a aussi été étiqueté avec l'étiqueteur morphosyntaxique HMM du système L NE. Nous avons enfin « fusionné » les étiquettes de parties de discours et les étiquettes d'entités nommées comme suit : tout nom propre ou mot inconnu contenu dans une entité nommée de type t détecté par reçoit l'étiquette t comme partie de discours. Nous avons obtenu ainsi le corpus C étiqueté à la fois en partie de discours et en entités nommées.  Par exemple, pour la phrase le commissaire du Portugal Jorge Palmeirim, le système  a produit : . De son coté, l'étiqueteur HMM de L NE a produit l'annotation suivante : . À partir de ces deux annotations, nous produisons automatiquement le corpus :  Les distributions du modèle HMM ont été directement réestimées sur C  , produisant ainsi la version adaptée L NE 1. Pour l'étiqueteur CRF, le corpus d'apprentissage constitué des corpus ESTER annotés manuellement a été enrichi avec des phrases sélectionnées du corpus C , avec un critère qui repose sur la fréquence des entités détectées : en sélectionnant les phrases contenant les entités les plus fréquentes on espère diminuer le risque d'erreur d'étiquetage dans les données d'apprentissage. Pour éviter que quelques entités ne représentent à elles seules la majorité des exemples retenus, nous avons également limité le nombre maximum de phrases contenant chaque entité. Ainsi nous gardons n phrases parmi toutes celles contenant les entités détectées plus de x fois dans le corpus. En faisant varier n et x on peut mesurer l'impact de l'ajout de données annotées automatiquement dans les corpus d'apprentissage de L NE. L'utilisation complète du corpus (n = 400, x = 10) permet de produire le second système adapté, nommé L NE 2.  L'évaluation de  , de la version initiale de L NE et des versions adaptées décrites à la section précédente a été réalisée sur une nouvelle version du corpus de référence développé et utilisé par Stern & Sagot (2010b), corpus qui est disponible librement dans le cadre de la distribution de S Pipe. Ce corpus, formé de 100 dépêches AFP contenant chacune une moyenne de 300 mots, a été annoté manuellement en entités nommées sous forme de marqueurs XML balisant le texte. Ces balises, outre l'empan de chaque mention, contiennent des informations sur son type ainsi que sur son référent représenté par un numéro d'entrée dans la base Aleda. Dans son état actuel, ce corpus de référence ne contient des annotations que pour des entités de type PERS, LOC et ORG (y compris les noms d'entreprises) . Il contient un total de 1456 mentions d'entités.  Les résultats sont donnés en fonction des mesures de rappel et précision strictes : une hypothèse est considérée  comme correcte uniquement si sa segmentation et son typage son corrects . Comme le montre le tableau 1, le système donne de bons résultats en terme de précision pour les trois types d'entités. Le rappel sur les LOC est également élevé (86.3%), illustrant ainsi l'apport de la base d'entités Aleda. Comme prévu, le système L NE obtient une précision plus faible, mais un rappel intéressant pour les entités ORG et PERS. Les résultats de L NE s'améliorent significativement en adaptant ce système, d'une part au niveau lexical en réestimant les distributions de l'étiqueteur HMM, mais également au niveau segmentation en ajoutant au corpus d'apprentissage des CRF des phrases annotés par . Le gain en terme de rappel est de l'ordre de 4% en absolu pour chaque catégorie. Rappelons que cette adaptation est non supervisée et qu'elle a pour but d'obtenir un système statistique complémentaire au système symbolique . La figure 1 illustre cette complémentarité en montrant les courbes d'apprentissage du système L NE sur les catégories ORG et PERS lorsque l'on fait croître le volume de données AFP annotées par ajouté au corpus d'apprentissage des CRF . On constate que le rappel augmente en fonction de l'ajout de données, mais au prix d'une certaine diminution en précision. L'augmentation du rappel est encourageante dans la perspective d'intégrer plus encore les deux types de systèmes, par rapport à un cadre applicatif précis, pour exploiter au mieux la bonne précision des modèles symboliques et le bon rappel des modèles numériques.  La traditionnelle opposition entre méthodes symboliques et méthodes numériques a nourri quantité de débats à  l'occasion de campagnes d'évaluation sur des tâches telles que l'étiquetage en parties de discours ou l'annotation en entités nommées. Si aucune méthode ne s'est avérée gagnante dans toutes les conditions de tests, ces deux familles de méthodes ont montré des comportements différents, et fortement complémentaires, que l'on peut résumer de façon simplificatrice comme suit : bonne précision pour les approches symboliques, bon rappel pour les approches numériques. Nous avons montré dans cette étude comment cette complémentarité pouvait être exploitée pour adapter un système numérique d'annotation en entités nommées à une nouvelle tâche à l'aide d'un système symbolique. Cette adaptation nous permet de disposer de deux systèmes complémentaires, l'un privilégiant la précision, l'autre le rappel, sans nécessiter aucune annotation supplémentaire, illustrant la complémentarité des méthodes numériques et symboliques pour la résolution de tâches linguistiques.  L'interaction  et L NE, qui est donc ici une instance simplifiée du paradigme du co-training, pourrait toutefois être poussée plus avant. Des expériences en marge de ce travail visant à intégrer un lexique issu d'Aleda aux versions adaptées de L NE se sont avérées inopérantes, probablement parce que les corpus annotés par utilisés pour l'entraînement intégraient déjà une partie importante de ces informations lexicales. Mais un couplage plus fin avec Aleda, notamment en prenant en compte les poids associés aux entités, devrait permettre d'améliorer les résultats de L NE. À l'inverse, la sortie de L NE pourrait servir de source d'information au module de chargé de la désambiguïsation. Ces pistes, et d'autres, devraient confirmer les résultats présentés ici et valider la pertinence d'approches mixtes ou hybrides pour des tâches telles que la reconnaissance d'entités nommées.  Ce travail a été effectué dans le cadre du projet ANR EDyLex (ANR-09-CORD-008).   
