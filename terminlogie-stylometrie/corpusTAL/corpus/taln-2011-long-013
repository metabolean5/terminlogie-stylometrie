Nous étudions dans cet article le problème de la comparabilité des documents composant un corpus comparable afin d'améliorer la qualité des lexiques bilingues extraits et les performances des systèmes de recherche d'information interlingue. Nous proposons une nouvelle approche qui permet de garantir un certain degré de comparabilité et d'homogénéité du corpus tout en préservant une grande part du vocabulaire du corpus d'origine. Nos expériences montrent que les lexiques bilingues que nous obtenons sont d'une meilleure qualité que ceux obtenus avec les approches précédentes, et qu'ils peuvent être utilisés pour améliorer significativement les systèmes de recherche d'information interlingue. We study in this paper the problem of enhancing the comparability of bilingual corpora in order to improve the quality of bilingual lexicons extracted from comparable corpora and the performance of crosslanguage information retrieval (CLIR) systems. We introduce a new method for enhancing corpus comparability which guarantees a certain degree of comparability and homogeneity, and still preserves most of the vocabulary of the original corpus. Our experiments illustrate the well-foundedness of this method and show that the bilingual lexicons obtained are of better quality than the lexicons obtained with previous approaches, and that they can be used to significantly improve CLIR systems Corpus comparables, comparabilité, lexiques bilingues, recherche d'information interlingue. Comparable corpora, comparability, bilingual lexicon, cross-language information retrieval.  Les lexiques bilingues sont une ressource incontournable dans différentes applications multilingues du traitement  automatique des langues comme la traduction automatique (Och & Ney, 2003) ou la recherche d'information interlingue (Ballesteros & Croft, 1997). Dans la mesure où la constitution manuelle de lexiques bilingues est une tâche coûteuse et qu'il est difficilement envisageable de développer un lexique pour chaque domaine d'étude, les recherches se sont intéressées à l'extraction automatique de ces lexiques à partir de corpus. Dans la mesure où la plupart des corpus bilingues existants sont par essence comparables, c'est-à-dire qu'ils regroupent des documents dans des langues différentes traitant du même domaine sur la même période sans être en relation de traduction, différents travaux s'intéressent à l'extraction de lexiques bilingues à partir de corpus comparables (Fung & McKeown, 1997; Fung & Yee, 1998; Rapp, 1999; Déjean et al., 2002; Gaussier et al., 2004; Robitaille et al., 2006; Morin et al., 2007; Garera et al., 2009; Yu & Tsujii, 2009; Shezaf & Rappoport, 2010, entre autres). Le socle commun à ces travaux est de reposer sur une hypothèse de distribution qui postule que les mots qui sont en correspondance de traduction sont susceptibles d'apparaître dans des contextes identiques pour des langues différentes. En s'appuyant sur cette hypothèse fondatrice, les chercheurs ont aussi cherché à identifier de meilleures représentations pour le contexte des mots de même qu'à utiliser différentes méthodes pour mettre en correspondance les mots entre différentes langues toujours en s'appuyant sur cette représentation du contexte. Ces méthodes semblent avoir atteint leur limite en termes de performance et les améliorations les plus récentes concernent plus le cadre d'évaluation des ces approches, plus contraint et limité (Yu & Tsujii, 2009), ou encore le traitement de langues spécifiques (Shezaf & Rappoport, 2010). Plus récemment, et en s'éloignant des approches traditionnelles, Li & Gaussier (2010) ont proposé une approche basée sur l'amélioration de la comparabilité des corpus comme préalable à l'extraction lexicale bilingue. Cette approche postule qu'il ne sert à rien d'essayer d'extraire des lexiques bilingues à partir d'un corpus avec un faible degré de comparabilité puisque la probabilité de trouver des traductions d'un mot donné sera faible dans une telle situation. Notre étude se situe dans la même veine que cette précédente approche et vise dans un premier temps à améliorer la comparabilité d'un corpus donné, tout en préservant une large part de son vocabulaire. Néanmoins, nous nous différencions de ce précédent travail en montrant qu'il est possible de garantir un certain degré d'homogénéité du corpus amélioré, et que celle-ci induit une amélioration significative de la qualité du corpus résultant et des lexiques bilingues extraits. En outre, nous montrons que les lexiques extraits avec notre approche améliorent de manière manifeste les résultats d'un système de recherche d'information interlingue, même lorsque ces lexiques sont issus d'un corpus différent de la collection interrogée.  Nous commençons par donner dans cette partie la mesure de comparabilité que nous utilisons, avant de décrire un  algorithme permettant d'améliorer la comparabilité d'un corpus donné. Nous fournissons également une preuve du bien-fondé de notre algorithme, ainsi qu'une approximation conduisant à une implantation efficace. Pour des raisons pratiques, notre discussion se fera sur la base du couple de langues anglais-français.  Afin de mesurer le degré de comparabilité d'un corpus bilingue, nous utilisons la mesure développée dans (Li &  Gaussier, 2010) : étant donné un corpus comparable P constitué d'une partie anglaise P et d'une partie française P , le degré de comparabilité de P est défini comme l'espérance de trouver la traduction d'un mot du vocabulaire source (respectivement cible) dans le vocabulaire cible (respectivement source). Soit  une fonction indiquant si une traduction de l'ensemble des traductions possibles T du mot w se trouve dans le vocabulaire P du corpus P, c'est-à-dire :  (w, P) =  1 si T  P =  0 sinon et soit D un dictionnaire bilingue dont le vocabulaire anglais (respectivement français) est noté D (respectivement D ). La mesure du degré de comparabilité M est définie par : où # (P) représente le nombre de mots différents présents dans P. Comme on peut le voir d'après cette définition, M mesure la proportion de mots source et cible dont une traduction est présente dans le vocabulaire cible et source de P. Pour des raisons qui deviendront claires plus tard, nous utiliserons aussi des mesures partielles où seuls les vocabulaires français ou anglais sont considérés. Ainsi, la proportion de mots anglais traduits sera notée M , définie par : . La mesure M est définie de la même façon. L'hypothèse distributionnelle sous-tendant l'extraction de lexiques bilingues est d'autant plus valide que les documents dans les différentes langues couvrent des thématiques proches, car les auteurs ont alors tendance à puiser dans le même vocabulaire (voir (Morin et al., 2007) pour une analyse reliée). En d'autres termes, si un corpus couvre un nombre limité de thématiques, il est plus à même de contenir une information répétée et cohérente qui pourra être exploitée au mieux pour l'extraction de lexiques bilingues. Le terme homogénéité rend compte de ce phénomène et nous dirons, de façon informelle, qu'un corpus est homogène s'il couvre un nombre limité de thématiques. Nous conjecturons ici que si l'on peut garantir un certain degré d'homogénéité, en plus d'un certain degré de comparabilité, alors les lexiques bilingues extraits seront de meilleure qualité. Comme nous le verrons, cette conjecture sera validée par les expériences menées. De façon à garantir un certain degré d'homogénéité, nous nous appuyons sur des techniques de classification non supervisée (clustering). Nous utilisons ici des techniques de classification agglomérative ascendante, mais toute autre technique, pour peu qu'elle dispose d'une procédure de filtrage adaptée, peut être utilisée.  2.2.1  Algorithme de classification bilingue  L'ensemble du processus permettant de construire, à partir d'un corpus donné, un corpus plus homogène et de  plus fort degré de comparabilité peut être résumé par les étapes suivantes : 1. À partir de la mesure de similarité, définie en 2.2.2 et fondée sur la mesure de comparabilité présentée ci-dessus, et de l'ensemble des documents anglais et français du corpus originel P, construire les dendrogrammes en suivant les étapes classiques de la classification agglomérative ascendante ; 2. Filtrer les dendrogrammes en ne retenant que les classes les plus profondes (voir ci-dessous) ; 3. Fusionner les classes retenues pour former un nouveau corpus P , qui contient une sous-partie homogène et fortement comparable de P ; 4. Répéter les étapes ci-dessus pour enrichir la partie restante de P (partie qui sera notée P , P = P \ P ) avec des documents extraits d'autres corpus. Les trois premières étapes sont détaillées dans l'algorithme 1, où CAA signifie Classification Agglomérative Ascendante. Comme on peut le remarquer, seul P est utilisé pour construire P , à travers des étapes de classification et de filtrage. Ainsi, l'algorithme 1 vise à extraire de P une sous-partie fortement comparable et homogène. Une fois cela réalisé, c'est-à-dire une fois que P a été exploité, il est nécessaire de recourir à des ressources externes si l'on veut construire un corpus fortement comparable à partir de P (qui est la partie restante de P). Pour cela, deux nouveaux corpus comparables sont considérés dans l'étape 4 du processus global : le premier consiste en la partie anglaise de P et la partie française d'un autre corpus P ; le second consiste en la partie française de P et la partie anglaise de P . Les deux sous-parties fortement comparables et homogènes obtenues à partir de ces deux corpus sont alors ajoutées à P pour constituer le corpus final. L'utilisation de la classification agglomérative ascendante et du filtrage associé garantit que le corpus final est homogène. La propriété 1 que nous présentons plus Algorithm 1: Algorithme de classification bilingue Entrée : Ensemble U de tous les documents anglais et français de P Réel positif  (seuil de profondeur) Sortie : P , sous-partie fortement comparable et homogène de P Initialiser P =  ; CAA(U )  ensemble S de dendrogrammes for chaque dendrogramme T de S do m  profondeur maximale de T ; for tous les noeuds n de T do if profondeur(n)  m ·  then Ajouter tous les documents sous le noeud n à P ; end if end for end for Supprimer les doublons de P ; return P ;  loin établit que ce corpus est fortement comparable. Mais avant de voir en détail cette propriété, nous introduisons  la mesure de similarité utilisée.  2.2.2  Mesure de similarité  Imaginons deux classes de documents bilingues C  et C . Pour la tâche d'extraction de lexiques bilingues, ces deux classes sont similaires et devraient être regroupées si leur combinaison permet de compléter le contenu de chacune des classes prise isolément, ou, en d'autres termes, si la partie anglaise C de C et la partie française C de C sont comparables à leur contrepartie dans l'autre classe (respectivement la partie française C de C et la partie anglaise C de C ) . Ceci conduit à la mesure de similarité suivante pour C et C :  où  (0    1) est un poids qui permet de contrôler l'importance de chacune des deux parties (C  , C ) et (C , C ). De façon intuitive, on aimerait donner plus de poids dans cette combinaison à la partie la plus importante, car elle contient plus d'information. Si nous utilisons le nombre de paires de documents anglais-français pour quantifier cette information, le poids  peut être défini comme la proportion de paires de documents dans (C , C ) sur l'ensemble des paires de documents dans le corpus fusionné :  où #  (C) représente le nombre de documents dans C. Dans la mesure où les classes sont tout d'abord formées de documents anglais et français isolés, la mesure de similarité correspond à un score de comparabilité normalisé entre les corpus anglais et français qui forment la nouvelle classe. Cependant, cette mesure ne tient pas compte des longueurs relatives des corpus anglais et français, qui ont pourtant un impact sur la qualité des corpus bilingues extraits. Si une contrainte de type 1-1 (c'est-à-dire imposant à chaque classe de contenir le même nombre de documents anglais et français) est trop forte, se reposer sur des classes par trop déséquilibrées n'est pas non plus souhaitable. Nous introduisons donc une nouvelle fonction  qui a pour but de pénaliser les classes pour lesquelles les nombres de documents anglais et français sont trop différents :  avec   R  . Cette fonction de pénalité fournit une nouvelle mesure de similarité sim qui est celle utilisée dans l'algorithme 1 : Dans la suite de cette étude,  est fixé à 1 dans .  2.2.3  Analyse théorique  Le processus de classification utilisé dans l'algorithme 1 garantit que les documents qui portent sur la même  thématique seront regroupés avant les documents portant sur des thématiques différentes. Le corpus obtenu (P ) sera ainsi homogène, c'est-à-dire qu'il ne couvrira qu'un nombre restreint de thématiques. De plus, le fait que le corpus comparable (que nous noterons P ) obtenu au travers des étapes 1 à 4 découle du corpus originel P indique que la plus grande partie du vocabulaire de P sera préservée dans P . Nous verrons dans la partie expérimentale que c'est bien le cas. Ce qui semble moins évident, c'est le fait que le processus que nous avons défini garantisse un fort degré de comparabilité. La propriété suivante établit que c'est bien le cas.  Propriété 1 Soit C  et C deux classes de documents qui doivent être regroupées dans le processus de classification. Nous faisons l'hypothèse que le dictionnaire bilingue D a été construit indépendamment des documents traités, ce qui implique que le degré de comparabilité M (respectivement de même pour M ) est à peu près le même pour différentes parties du corpus . Nous faisons de plus l'hypothèse que :  (I)  = Alors :  Démonstration (esquisse) : Soit V = C   C . En utilisant le fait que M (C , C )  M (C , C ) pour tout C tel que C  C (et de même pour la direction français vers anglais), nous avons, pour i = 1, 2 :  et, pour les mots de V :  Alors, d'après l'hypothèse d'indépendance entre corpus et dictionnaire faite en énonçant la propriété 1 :  Un développement similaire sur M  et l'utilisation de la condition (I) complètent la démonstration. La propriété précédente garantit que la classe obtenue en fusionnant deux classes existantes a un degré de comparabilité au moins égal à celui de la classe la moins comparable. Le degré de comparabilité ne peut donc décroître dans le processus de classification agglomérative. Comme l'on commence par fusionner les documents les plus comparables, on ne construit que des classes avec un bon degré de comparabilité. Enfin, la condition (I) a de grandes chances d'être réalisée car tous les corpus sont prétraités de façon à éliminer les documents trop courts ou trop longs, souvent source de bruit, et la pénalité utilisée dans la mesure de similarité fournit des classes comprenant des nombres comparables de documents dans les deux langues. Le processus global que nous avons défini permet donc d'obtenir des corpus homogènes et fortement comparables.  Dans la mesure où les corpus comparables disponibles à l'heure actuelle comprennent en général un nombre  important de documents, la classification agglomérative peut s'avérer trop coûteuse. Nous proposons ici une borne inférieure de la mesure de comparabilité qui peut être calculée efficacement ainsi qu'une mise à jour efficace de la matrice de similarité pendant le processus de classification. Le fait de se reposer sur une borne inférieure de la mesure de similarité garantit que les classes obtenues auront un bon degré de comparabilité, car seules les classes les plus similaires sont regroupées à chaque itération de l'algorithme de classification. La propriété suivante établit une telle borne inférieure, sur la base du degré de comparabilité moyen des paires de documents.  Propriété 2 Soit P un corpus comparable comprenant une partie anglaise P  et une partie française P , et soit D un dictionnaire bilingue, D dénotant le vocabulaire anglais et D le vocabulaire français. Supposons que le dictionnaire est distribué de façon uniforme sur le corpus, c'est-à-dire que :  et de même pour la partie française. Supposons de plus que tous les documents, ainsi que les parties anglaise et  française du corpus, ont à peu près la même longueur :  Alors :   Nous ne détaillons pas ici la démonstration de cette propriété, purement technique. La première hypothèse faite  semble raisonnable (et rejoint celle faite dans la propriété précédente) en l'absence de toute connaissance a priori sur les thématiques couvertes par le corpus et leur lien avec le dictionnaire. La seconde hypothèse est en partie garantie dans notre cas par le processus de construction que nous avons défini et la fonction de pénalité associée. Remplacer M par la borne ci-dessus dans l'équation 1 conduit à une mesure de similarité qui peut être vue comme la valeur accumulée de toutes les connexions entre deux classes. Il est alors possible de mettre à jour la matrice de similarité de façon itérative. Supposons en effet que le processus de classification doive, à un instant donné, fusionner les classes C et C en une seule classe C . Un nouveau score de similarité entre C et toutes les autres classes doit alors être calculé. La similarité entre C et une autre classe C peut s'écrire, à partir de l'équation 3 et de la formule de similarité : où (j = 1, 2) et : Dans le processus de classification, dans la mesure où sim (C , C ) et sim (C , C ) sont déjà connus avant le calcul de sim (C , C ), la matrice de similarité peut directement être mise à jour à chaque itération. En notant N le nombre de classes avant fusion, la complexité de cette mise à jour est de l'ordre de O(N ), alors qu'elle atteint O(N × ¯ C ) si l'on applique directement les équations 1 et 3 ( ¯ C représentant le nombre moyen de documents par classe).  Les différentes expériences que nous avons réalisées ont pour objectif d'évaluer : (i) si l'algorithme que nous  avons proposé induit des corpus d'une meilleure qualité en ce qui concerne la comparabilité, (ii) si les lexiques bilingues extraits de ces corpus sont eux aussi d'une qualité plus importante, et (iii) si ces lexiques peuvent être utilisés pour améliorer les performances des systèmes de recherche d'information interlingue.  Dans nos expériences, différents corpus sont utilisés : le corpus anglais TREC  de l'Associated Press (noté AP) et les corpus fournis dans les tâches multilingues des campagnes CLEF dont pour l'anglais le Los Angeles Times (LAT94) et le Glasgow Herald (GH95) et pour le français Le Monde (MON94), le SDA 94 (SDA94) et 95 (SDA95). Outre ces corpus existants, deux corpus monolingues ont été extraits à partir de Wikipédia : le corpus anglais WikiEn construit en retenant l'ensemble des articles appartenant à la catégorie Society pour une profondeur inférieure à 4 (soit 33 000 mots anglais distincts) et le corpus français Wiki-Fr toujours pour la catégorie Société pour une profondeur inférieure à 7 (soit 28 000 mots français distincts). Le dictionnaire bilingue bd nécessaire pour la tâche d'extraction de lexiques est quant à lui construit à partir de dictionnaires en ligne. Dans toutes nos expériences, nous utilisons la méthode décrite dans le présent article complétée par celle présentée dans (Li & Gaussier, 2010). Cette dernière méthode est à notre connaissance la seule approche alternative pour améliorer la comparabilité des corpus, d'où son importance dans l'évaluation.  L'algorithme de classification décrit en section 2.2.1 est utilisé pour améliorer le degré de comparabilité d'un  corpus comparable. Les corpus GH95 et SDA95 sont utilisés pour construire le corpus comparable P (56 000 mots pour l'anglais et 42 000 le français). En outre, nous exploitons deux corpus comparables supplémentaires pour nous assurer que l'efficacité de notre algorithme n'est pas liée à une ressource externe spécifique : i) P composé à partir des corpus LAT94, MON94 et SDA94 (109 000 mots pour l'anglais et 87 000 pour le français) et ii) P composé à partir des corpus Wiki-En et Wiki-Fr (368 000 mots pour l'anglais et 378 000 pour le français).  Après le processus de classification, nous obtenons les corpus P  (pour le corpus externe P ) et P (pour le corpus externe P ). Comme nous l'avons indiqué précédemment, nous utilisons aussi la méthode décrite dans (Li & Gaussier, 2010) sur les mêmes données pour comparer nos résultats et obtenons ainsi le corpus P (pour P ) et P (pour P ) à partir de P . Au niveau de la couverture lexicale, P couvre 97,9% du vocabulaire de P , tandis que P couvre 99,0% de celui de P . Nous pouvons ainsi constater qu'une très grande partie du vocabulaire du corpus d'origine a été conservé, ce qui est l'une des exigences de notre approche. En ce qui concerne les scores de comparabilité, P atteint 0,924 et P 0,939. Les deux corpus comparables ont donc bien un degré de comparabilité supérieur au corpus d'origine qui était de l'ordre de 0,881 comme cela est suggérée par la propriété 1. En outre, les corpus P et P sont plus comparables que le corpus P (comparabilité de 0,912) et P (comparabilité de 0,915) ce qui montre bien que l'homogénéité est un élément crucial pour évaluer la comparabilité.  T  1 - Évaluation des lexiques bilingues extraits pour différents corpus comparables P P P P P P > P P > P Précision 0,226 0,277 0,325 0,295 0,461 0,069 30,5 % 0,235 104,0 % Rappel 0,103 0,122 0,145 0,133 0,212 0,030 29,1 % 0,109 105,8 %  T  2 - Comparaison de la précision pour différents intervalles de fréquences des mots de la liste d'évaluation P P P P > P P > P P > P W 0,135 0,206 0,304 0,071 52,6 % 0,169 125,2 % 0,098 47,6 % W 0,256 0,390 0,564 0,134 52,3 % 0,308 120,3 % 0,174 44,6 % W 0,434 0,632 0,667 0,198 45,6 % 0,233 53,7 % 0,035 5,5 ,All 0,226 0,325 0,461 0,099 43,8 % 0,235 104,0 % 0,136 41,8 %  Comme les travaux antérieurs en extraction de lexiques bilingues à partir de corpus comparables exploitent des  ressources différentes et opèrent des choix distincts des nôtres, il est relativement difficile de se comparer à ceuxci (Laroche & Langlais, 2010). En outre, puisque notre approche vise à améliorer la comparabilité de corpus, elle peut être ensuite couplée à une méthode existante d'extraction de lexiques bilingues. Il est donc tout aussi intéressant de directement évaluer si un tel couplage peut conduire à des performances accrues en termes de qualité des lexiques extraits.  L'extraction de lexiques bilingues à partir de corpus comparables repose sur la méthode proposée par Fung & Yee  (1998) plus connue maintenant sous le nom d'approche standard notamment dans les travaux de (Déjean et al., 2002; Gaussier et al., 2004; Yu & Tsujii, 2009). Dans cette approche, chaque mot est représenté sous la forme d'un vecteur de contexte composé des mots qui co-occurrent avec lui dans une fenêtre donné. Les vecteurs de contexte de la langue source sont ensuite traduits vers la langue cible en s'appuyant sur un dictionnaire bilingue. Enfin, la traduction d'un mot est obtenue en comparant son vecteur de contexte traduit à l'ensemble des vecteurs de la langue cible à travers une mesure de distance ou similarité vectorielle telle que le cosinus.  3.2.1  Paramètres expérimentaux  Afin d'évaluer la qualité des lexiques bilingues extraits, nous divisons notre dictionnaire bilingue bd  en deux parties : 10 % des mots anglais accompagnés de leurs traductions sont choisis aléatoirement et uniquement utilisés comme liste d'évaluation, les 90 % restant sont utilisés pour assurer la traduction des vecteurs de contexte dans l'approche standard. Les mots anglais absents de P ou pour lesquels aucune traduction n'a été trouvée dans P sont retirés de la liste d'évaluation. Pour chaque mot anglais de la liste d'évaluation, tous les mots français de P sont ordonnés suivant leur similarité avec les mots anglais. Les mesures de précision et rappel sont ensuite calculées sur les N premiers candidats. Les valeurs de la précision dans ce cas correspondent à la proportion de listes contenant la traduction correcte (en cas de traductions multiples, une liste est réputée contenir la traduction correcte dès lors que l'une des traductions possibles est présente). Le rappel est quant à lui la proportion de traductions correctes trouvée dans les listes sur toutes les traductions fournies dans le corpus. Cette manière de procéder a été utilisée dans différents travaux antérieurs et peut être maintenant considérée comme un méthode d'évaluation attestée. En outre, plusieurs études ont montré qu'il est plus facile de trouver les traductions correctes pour les mots fréquents que pour les mots rares (Pekar et al., 2006). Afin de prendre en compte ce phénomène, nous distinguons différents intervalles d'effectifs pour évaluer la validité de notre approche. Ainsi, les mots avec un effectif inférieur à 100 sont définis comme étant des mots de faibles fréquence (W ), ceux avec un effectif supérieur à 400 sont définis comme étant des mots très fréquents (W ), et enfin les mots dont l'effectif est compris entre ces deux seuils sont considérés comme des mots de fréquence intermédiaire (W ).  3.2.2  Analyse des résultats  Dans une première série d'expériences, les lexiques bilingues sont extraits à partir des corpus obtenus ii) par notre  approche (P et P ), ii) par la méthode décrite dans (Li & Gaussier, 2010) (P and P ) et iii) enfin avec le corpus d'origine P , avec N fixé à 20. La table 1 présente les résultats obtenus. Les deux dernières colonnes "P > P " et "P > P " indique les différences absolue et relative, exprimées en pourcentage, par rapport à P . Comme nous pouvons le constater, les meilleurs résultats sont obtenus à partir des corpus construits avec la méthode que nous avons proposée. Les lexiques extraits à partir du corpus où le degré de comparabilité a été renforcé sont d'une bien meilleure qualité que ceux obtenus à partir du corpus d'origine ou encore du corpus construit avec l'approche de (Li & Gaussier, 2010). La différence de qualité est encore plus notable avec P qui est obtenu à partir d'un corpus externe volumineux P . Ces résultats semblent confirmer l'intuition qu'il est possible de trouver plus aisément dans des corpus volumineux des documents en relation avec un corpus donné. Afin d'évaluer la relation entre la qualité de ces méthodes et la fréquence des mots à traduire, nous nous concentrons sur les meilleurs résultats sur P pour l'approche précédente et sur ceux de P pour notre approche. La table 2 résume les résultats obtenus. On remarquera, sans véritablement de surprise, que les résultats obtenus pour les mots ayant une haute fréquence sont meilleurs que ceux obtenus pour les mots de faible fréquence. En outre, notre approche est la meilleure quel que soit l'intervalle de fréquence pris en compte. La précision globale peut être augmentée en relatif de 41,8 % (de 0,325 à 0,461). En comparant P avec le corpus d'origine P , nous pouvons noter pour la précision globale, une augmentation relative de 104,0 % (de 0,226 à 0,461), ce qui est très satisfaisant dans ce contexte d'évaluation. Enfin, l'amélioration pour les mots de faible et moyenne fréquence est plus importante pour P , ce qui démontre que notre approche se comporte bien mieux sur ce qui est généralement considéré comme un problème difficile (Pekar et al., 2006).  T  3 - Score MAP pour la tâche de recherche d'information interlingue suivant différents dictionnaires bilingues mon bd bd +cc bd +cc bd +cc bd bd +cc bd +cc bd +cc MAP 0, 422 0, 313 0, 327 0, 328 0, 338 0, 375 0, 382 0, 377 0, 391  Dans la dernière série d'expériences, nous cherchons à évaluer l'apport des différents lexiques extraits à partir de  corpus comparables pour une tâche de recherche d'information interlingue. Pour ce faire, nous exploitons les sujets des campagnes CLEF de 2001 et 2002, rassemblant environ 100 sujets distincts, comme requêtes sur une collection de 113 000 documents issus du Los Angeles Times. Les sujets anglais correspondants sont utilisés pour interroger la même collection (référence mon). Seul le titre et la partie description des sujets CLEF sont utilisés pour construire des requêtes. En outre, les mots outils et les phrases non pertinentes telles que find documents which report about sont supprimés des requêtes. La recherche est réalisée avec le modèle Indri du système de recherche d'information Lemur (http ://www.lemurproject.org). Une variante de l'approche introduite dans (Pirkola, 1998) et (Talvensaari et al. , 2007) est aussi utilisée pour transformer les sujets français en requêtes en anglais. L'idée est de borner toutes les possibilités de traduction d'un mot français dans le sujet du texte avec un opérateur WSYN. Ensuite, toutes les traductions candidates dans l'opérateur WSYN sont traitées comme des synonymes avec des poids différents.  Dans nos expériences, nous combinons deux dictionnaires bilingues de langue générale bd  (68 0000 traductions) et bd (116 000 traductions) avec les lexiques bilingues obtenus automatiquement dans la précédente section. Nous utilisons ici les lexiques cc (extrait de P ), cc (extrait de P ) et cc (extrait de P ). Différentes combinaisons de ces ressources sont réalisées, y compris bd , bd +cc , bd +cc , bd +cc . Lorsque qu'un dictionnaire de langue générale et un lexique extrait sont combinés, plus de poids est attribué aux traductions candidat du dictionnaire de langue générale. Le poids des différents mots traduits à partir de cc est quant à lui le cosinus entre les vecteurs de contexte de chaque mot (c'est-à-dire le score donné par l'approche standard précédemment évoquée). Le poids pour les traductions trouvées dans le dictionnaire bilingue est fixé empiriquement à 25. Comme il est d'usage en recherche d'information, nous utilisons la mesure MAP (Mean Average Precision) afin d'évaluer les performances des différents systèmes. L'importance des différences entre les différents systèmes est estimée par un t-test apparié de Student (p-value fixée à 0,1). Les résultats obtenus sont indiqués dans la table 3. Pour le dictionnaire de langue générale bd , on note toujours une amélioration significative des résultats (identifiée par la marque ) du score MAP lorsque l'un des lexiques bilingues extraits du corpus comparables est utilisé. Lorsque bd , qui est beaucoup plus riche que bd , est utilisé, seulement le lexique bilingue cc extrait avec notre méthode à partir P conduit à une amélioration significative des résultats. Cela montre que cc est supérieure à cc et cc dans la tâche de recherche d'information interlingue, en particulier lorsque le dictionnaire de langue générale utilisé est d'une taille importante. Ces résultats semblent confirmer que notre approche basée sur de la classification est plus adaptée que l'approche gloutonne des travaux précédents de (Li & Gaussier, 2010). Enfin, la combinaison actuelle des lexiques extraits avec le système de recherche d'information interlingue est relativement simple et pourrait être certainement améliorée en exploitant d'autres modèles de combinaison. Dans cet article, nous avons proposé une nouvelle approche pour augmenter le degré de comparabilité des documents constituant un corpus comparable afin d'améliorer la qualité des lexiques bilingues extraits de corpus comparables et les performances des systèmes de recherche d'information interlingue. Nous avons démontré théoriquement puis empiriquement que notre approche permet de garantir un certain degré de comparabilité et l'homogénéité du corpus tout en préservant une large part du vocabulaire du corpus d'origine. Enfin, nos expériences montrent que les lexiques bilingues que nous obtenons sont d'une meilleure qualité que ceux obtenus avec les approches précédentes, et que ces lexiques peuvent être utilisés pour améliorer significativement les résultats des systèmes de recherche d'information interlingue.  Les deux étapes cruciales de notre approche sont d'une part l'extraction d'un noyau fortement comparable du  corpus original, et, d'autre part, l'alignement des parties du corpus orignal, non présentes dans ce noyau, avec un corpus externe. Le seuil introduit au niveau du degré de comparabilité permet de contrôler la taille et la qualité du noyau extrait dans la première étape. Si le corpus original n'est que très faiblement comparable, il est alors possible que ce noyau soit vide (ce qui est un résultat souhaitable dans ce cas). Dans tous les cas, excepté celui où le noyau correspond au corpus original, le corpus final dépend de la proximité du corpus original (en fait de la partie restante après extraction du noyau) et du corpus externe utilisé. Bien évidemment, si le corpus externe est trop différent du corpus original, l'on ne pourra pas compléter correctement le noyau. Considérer des corpus externes les plus larges possibles permet ici d'augmenter les chances de trouver des documents comparables L'idéal serait bien sûr d'avoir accès à la collection la plus large possible, et le web constitue ici un excellent candidat. Il est cependant nécessaire de pouvoir, à partir d'un document donné dans une langue source, extraire du web un ensemble de documents comparables en langue cible (on peut ensuite directement utiliser notre méthode sur l'union de ces ensembles). Or nous n'avons pas réussi jusqu'à présent à réaliser correctement cette extraction. La constitution entièrement automatique de collections comparables à partir du web nous semble être un problème difficile, qui requiert d'autres attributs que ceux utilisés pour les corpus parallèles. C'est un point que nous comptons développer dans le futur.  Ce travail qui s'inscrit dans le cadre du projet METRICC (www.metricc.com) a bénéficié d'une aide de  l'Agence Nationale de la Recherche portant la référence ANR-08-CORD-009. Enfin, nous tenons à remercier les relecteurs pour leurs commentaires précieux.  
