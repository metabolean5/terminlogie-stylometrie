Il  s'agit  d'un  démonstrateur  qui  permet  d'illustrer  les  traitements  automatiques  réalisés   sur  des  contenus  vidéo.  Le  principe  est  de  visualiser  sous  forme  de  timeline  des  informations  de  structuration  extraites  automatiquement.  Pour  chaque  segment,  un  onglet  permet  de  visualiser  des  informations  issues  du  canal  audio  (typiquement  la  transcription automatique synchronisée avec le player) et un onglet permet de visualiser  des  informations  liées  au  canal  vidéo  (typiquement  des  images  clé  ou  key  frames).  L'interface offre des fonctionnalités de navigation d'un segment à l'autre. Au-delà de ces  fonctionalités  de  base,  l'intérêt  de  l'outil  est  de  pouvoir  cumuler  plusieurs  timeline  et  observer  ainsi  l'apport  de  traitement  multi-niveaux.  Plusieurs  résultats  de  travaux  de  recherche seront montrés via cette interface.   Reconnaissance du rôle du locuteur    La capture d'écran ci-contre représente une analyse en rôle   des  tours  de  parole  dans  des  Journaux  Télévisés.  Elle  illustre  une  approche  multi-vue  qui  consiste  à  fusionner  une  analyse  purement  acoustique  modélisant  l'intonation  des locuteurs en fonction de leur rôle et une analyse purement linguistique basée sur une  analyse  de  la  transcription  automatique  du  contenu  parlé  (Damnati  et  Charlet,  2011).  L'interface  permet  de  visualiser  les  résultats  de  chacune  des  analyses  ainsi  que  de  leur  fusion, afin de mieux analyser leur complémentarité.    Détection de personnes dans des documents vidéo    Les travaux réalisés dans le cadre du défi REPERE (Béchet et al., 2012) seront également   montrés  via  cette  interface.  Ce  projet  a  pour  but  d'identifier  les  personnes  dans  des  contenus télévisés en exploitant conjointement le canal audio (contenu parlé et analyse  en locuteurs) et le canal vidéo (texte incrusté et analyse de visages). L'interface permet  de visualiser les informations extraites dans les différentes modalités ainsi que le résultat  de la fusion.   Cette interface à pour vocation de proposer aux utilisateurs une expérience de navigation   enrichie  dans  des  contenus  purement  audio,  en  s'appuyant  sur  des  métadonnées  produites  automatiquement.  Elle  propose  en  quelque  sorte  de  &#34;visualiser&#34;  des  contenus  audio.  Elle  est  déclinée  à  Orange  Labs  dans  différents  domaines,  allant  de  la  consommation  de  podcast  de  radio  à  l'écoute  de  conversations  issues  des  centres  d'appels.   La  capture  d'écran  ci-contre  illustre  une   interface  de  visualisation  conversations  client/téleconseiller,  et  s'inscrit  dans  le  domaine plus large du Speech Analytics. Elle  permet  d'avoir  une  vue  synthétique  du  déroulé  de  la  conversation,  structurée  en  locuteurs,  une  visualisation  d'expressions  clés  extraites  des  transcriptions  automatiques,  un  filtrage  des  conversations  par motif d'appel, etc...       
