Nous présentons un algorithme d'alignement sous-phrastique permettant d'aligner très facilement  un couple de phrases à partir d'une matrice d'alignement pré-remplie. Cet algorithme s'inspire de travaux antérieurs sur l'alignement par segmentation binaire récursive ainsi que de travaux sur le clustering de documents. Nous évaluons les alignements produits sur des tâches de traduction automatique et montrons qu'il est possible d'atteindre des résultats du niveau de l'état de l'art, affichant des gains très conséquents allant jusqu'à plus de 4 points BLEU par rapport à nos travaux antérieurs, à l'aide une méthode très simple, indépendante de la taille du corpus à traiter, et produisant directement des alignements symétriques. En utilisant cette méthode en tant qu'extension à l'outil d'extraction de traductions Anymalign, nos expériences nous permettent de cerner certaines limitations de ce dernier et de définir des pistes pour son amélioration. Hierarchical sub-sentential alignment with Anymalign  We present a sub-sentential alignment algorithm that aligns sentence pairs from an existing  alignment matrix in a very easy way. This algorithm is inspired by previous work on alignment by recursive binary segmentation and on document clustering. We evaluate the alignments produced on machine translation tasks and show that we can obtain state-of-the-art results, with gains up to more than 4 BLEU points compared to our previous work, with a method that is very simple, independent of the size of the corpus to be aligned, and can directly produce symmetric alignments. When using this method as an extension of the translation extraction tool Anymalign, our experiments allow us to determine some of its limitations and to define possible leads for further improvements. corpus parallèle ; alignement sous-phrastique ; traduction automatique statistique. parallel corpus ; sub-sentential alignment ; statistical machine translation.  L'alignement sous-phrastique consiste à identifier des traductions d'unités textuelles à partir  d'un corpus parallèle aligné en phrases, c'est-à-dire dont les phrases ont été préalablement mises en correspondance avec leur traduction. Cette tâche constitue la première étape du processus d'entraînement de la plupart des systèmes de traduction automatique fondée sur les données (traduction statistique ou par l'exemple). L'approche la plus répandue est actuellement la traduction automatique statistique par segments (n-grammes de mots), où le modèle central prend la forme d'une table de traductions, obtenue à partir de correspondances sous-phrastiques. Cette table consiste en une liste pré-calculée de couples de segments (source, cible), à chacun desquels est associé un certain nombre de scores reflétant la probabilité que source se traduise par cible.  Le problème de l'identification d'associations sous-phrastiques à partir de textes parallèles, entre  mots isolés ou n-grammes de mots par exemple, est bien connu, et de nombreuses propositions ont été faites pour le résoudre. On peut grossièrement classer ces méthodes en deux catégories. La première, l'approche probabiliste, introduite par Brown et al. (1988), considère le problème d'identifier des liens entre mots ou groupes de mots dans des phrases parallèles. Cette approche consiste à définir un modèle probabiliste du texte parallèle, dont les paramètres sont estimés par un processus de maximisation global qui considère toutes les associations possibles du corpus en même temps. Le but est de déterminer le meilleur ensemble de liens d'alignement entre les mots source et cible de chaque couple de phrases parallèles. Les plus connus dans cette catégorie sont les modèles IBM (Brown et al., 1993), permettant d'aligner des mots isolés, et qui ont donné lieu à une impressionnante liste de variantes et d'améliorations (voir par exemple les travaux de Vogel et al. (1996); Wu (1997); Deng et Byrne (2005); Liang et al. (2006); Fraser et Marcu (2007); Ganchev et al. (2008), pour ne citer qu'eux). La généralisation des modèles d'alignement de mots à l'alignement de segments s'avère être un problème bien plus difficile, et au vu des déficiences des propositions de Marcu et Wong (2002) et Vogel (2005), de tels alignements sont généralement produits en combinant des alignements de mots 1-n asymétriques (« orientés ») dans les deux directions à l'aide d'heuristiques (Koehn et al., 2003; DeNero et Klein, 2007). Une fois l'ensemble de ces liens d'alignement constitué, il est possible d'attribuer des scores à chacun des couples de segments extraits.  La seconde approche, associative (qualifiée d'heuristique par Och et Ney (2003)), a été introduite  par Gale et Church (1991). Celle-ci ne nécessite pas de modèle d'alignement : pour détecter des traductions, elle repose sur des mesures d'indépendance statistique telles que, par exemple, le coefficient de Dice, l'information mutuelle (Gale et Church, 1991; Fung et Church, 1994), ou le rapport de vraisemblance (Dunning, 1993) - voir aussi les travaux plus récents de Melamed (2000) et Moore (2005). On limite généralement les tests à une liste d'associations candidates pré-calculée à partir de motifs et de filtres, en se concentrant par exemple uniquement sur les n-grammes de mots les plus fréquents. Dans cette approche, on utilise un processus de maximisation locale, où chaque segment est traité indépendamment des autres. Cette approche permet généralement d'extraire directement des couples de traductions. Dans ce courant, on trouve par exemple les travaux de Gale et Church (1991), qui ont été depuis étendus aux corpus non strictement parallèles (Fung et Church, 1994; Fung et Yee, 1998), de Dagan et Church (1994); Gaussier et Langé (1995); Smadja et al. (1996) pour apprendre des associations de segments ou de termes, ou encore des travaux ayant recours à diverses mesures d'association, telles que le G (Gale et Church, 1991) ou le  (Dunning, 1993; Moore, 2004, 2005). Dans un second temps, on peut induire des liens d'alignement à la façon des méthodes probabilistes, comme l'a proposé Melamed (2000) avec le competitive linking.  L'approche probabiliste est la plus répandue, principalement du fait de sa bonne intégration avec  la traduction automatique statistique, dont elle constitue un fondement depuis l'introduction des modèles IBM (Brown et al., 1993). Les deux approches présentent des forces et faiblesses complémentaires, comme l'ont montré par exemple les travaux de Johnson et al. (2007), où les associations extraites à partir d'alignements de mots sont ensuite filtrées selon des mesures d'association. Nous avons récemment proposé une méthode d'extraction de traductions de segments sousphrastiques (Lardilleux et al., 2011a), nommée Anymalign, qui s'attaque à un certain nombre de problèmes souvent négligés dans le domaine. En particulier, cette méthode permet le traitement d'un nombre quelconque de langues simultanément, ne fait aucune distinction entre source et cible, est massivement parallélisable, passe facilement à l'échelle, et est très simple à implémenter. Cette méthode, qui s'inscrit dans le courant des méthodes associatives, est meilleure que l'état de l'art sur des tâches de constitution de lexiques bilingues. Les résultats obtenus lorsqu'on l'utilise pour construire des modèles de traductions statistiques s'avèrent toutefois inférieurs aux méthodes standard (Lardilleux et al., 2011b).  Une des hypothèses que nous avons précédemment émises pour expliquer ces résultats contrastés  est qu'Anymalign ne comporte pas de phase d'alignement à proprement parler. Cette méthode ne produit donc pas de liens à la manière des méthodes probabilistes, mais directement des tables de traductions avec leurs scores associés. Ces tables ont des profils très différents de celles extraites à partir d'alignements produits par les méthodes probabilistes, principalement en termes de distribution des n-grammes (Luo et al., 2011). En particulier, malgré de récentes améliorations (Lardilleux et al., 2011b), la quantité de traductions de longs n-grammes est relativement faible comparée aux tables de traductions obtenues à partir des méthodes probabilistes. Dans cet article, nous proposons une extension à notre méthode lui permettant de produire des liens d'alignement, à la manière des approches probabilistes, tout en conservant le caractère local de la recherche des traductions propre aux approches associatives. Notre but principal n'est pas ici de proposer une nouvelle méthode d'alignement destinée à améliorer les outils de l'état de l'art, mais d'essayer de mieux comprendre les limitations actuelles d'Anymalign, en l'utilisant ici de manière non plus directe, mais détournée, pour construire le modèle de traduction. La méthode pour construire des alignements, très simple, est donc indépendante d'Anymalign et pourrait être remplacée par tout autre procédé équivalent.  Cet article est organisé comme suit : la section 2 présente en détail chacune des étapes qui  compose notre méthode d'alignement, la section 3 présente une évaluation de la méthode sur des tâches de traduction automatique et une analyse des résultats obtenus, et la section 4 conclut ces travaux.  En un mot, notre méthode consiste à segmenter chaque couple de phrases d'un corpus parallèle de  façon binaire, déterminer parmi les deux segments cible obtenus lequel est la bonne traduction de chacun des deux segments source (traduction monotone ou inversée), et recommencer récursivement sur chacun des deux couples de segments obtenus.  Ces travaux s'inspirent fortement de ceux de Wu (1997) et Deng et al. (2006). Les premiers  présentent des grammaires de transduction inversibles où les parties source et cible d'un couple de phrases alignées sont analysées simultanément selon un arbre de dérivation binaire dont la particularité est de permettre l'inversion des constituants d'une langue à l'autre à n'importe quel niveau de l'arbre (approche bottom-up). On retrouve un concept similaire dans les seconds, où on extrait des bi-segments plus ou moins grossiers à partir de textes parallèles non préalablement alignés en phrases en appliquant une segmentation binaire de façon itérative selon le principe « diviser pour régner » (approche top-down).  Nos travaux se rapprochent davantage de ces derniers en ce sens que nous ne nous intéressons  qu'à une procédure simple ne reposant que sur des décomptes au niveau lexical, plutôt que sur une grammaire telle qu'utilisée par Wu. Néanmoins, alors que Deng et al. produisent des alignements de segments plus ou moins grossiers à partir d'un bi-texte non préalablement aligné en phrases, dans le but de simplifier des tâches subséquentes d'alignement sous-phrastique par exemple, notre but est plus classiquement d'aligner directement le grain le plus fin possible, ici le mot typographique, à partir de textes préalablement alignés en phrases. Le critère que nous utilisons pour décider de la segmentation d'un couple de phrases est adapté en conséquence.  Notre point de départ se compose :  - d'un bi-texte préalablement aligné en phrases ; - d'une fonction w associant à chaque couple de mots (source, cible) du bi-texte un score reflétant la force du lien de traduction entre source et cible. Plusieurs définitions de w sont possibles ; il est néanmoins naturel de la définir de façon endogène à partir des occurrences des mots sur l'ensemble du bi-texte. En ce qui nous concerne, les scores que nous utiliserons seront dans un premier temps obtenus à partir des sorties d'Anymalign. Nous verrons par la suite que ceux-ci mènent à de meilleurs résultats que d'autres scores obtenus à partir de modèles plus répandus, principalement du fait de la grande redondance des sorties d'Anymalign, qui permet de renforcer les scores de traductions se produisant dans des contextes variés.  Par la suite donc, le score w(s, c) entre un mot source s et un mot cible c sera défini comme le  produit des deux probabilités de traduction orientées p(s|c) × p(c|s), celles-ci étant calculées à partir des décomptes associés aux traductions produites par Anymalign :  w(s, c) = p(s|c) × p(c|s)   =  (s, c)  (S , C ) k s  S k × (s, c)  (S , C ) k c  C k  =  (s, c)  (S , C ) k s  S k × c  C k  avec :  - x = 1 si x est vrai, 0 sinon ;  - N le nombre d'entrées (couples de segments source-cible) dans la table de traductions produite  par Anymalign ; - S (resp. C ) le segment source (resp. cible) d'une entrée de la table de traductions ; - k le décompte associé au couple (S , C ) dans la table de traductions. Ce nombre n'est pas en soi un indicateur de la qualité de l'entrée ; il s'agit simplement du nombre de fois où le couple a été produit par Anymalign (voir détails dans (Lardilleux et al., 2011a)). La figure 1 donne un exemple.  En pratique, ce que nous faisons ici revient à partir d'une table de traductions pour aller vers  des liens d'alignements - pour retourner ultimement vers une nouvelle table de traductions. Cela va à rebours des usages du domaine, qui construisent la table de traductions à partir de l'ensemble des liens d'alignements calculés sur un corpus parallèle. Cette particularité ouvre de nouvelles pistes pour l'amélioration de la qualité des liens d'alignements et d'une table de traductions, l'amélioration des uns pouvant avoir des répercussions sur l'autre, et vice-versa, de façon itérative, à la manière des approches probabilistes reposant par exemple sur l'algorithme Espérance Maximisation. Cela sort néanmoins du cadre de cet article, et nous nous consacrons pour l'instant au passage de la table de traductions vers les liens d'alignements.  Le critère de segmentation décrit ci-après est issu des travaux de Zha et al. (2001) sur le  clustering de documents. Leur problème consiste à partitionner de façon optimale un graphe biparti représentant les occurrences d'un ensemble de termes au sein d'un ensemble de documents. Nous le transposons à la recherche du meilleur alignement entre l'ensemble des mots d'une  phrase source et l'ensemble des mots d'une phrase cible.   Pour cela, nous considérons un couple de phrases (S, C) du corpus parallèle, où la phrase source  S est constituée de I mots source et la phrase cible C est constituée de J mots cible : S = [s . . . s ] et C = [c . . . c ]. Nous considérons par ailleurs des indices de coupure x et y définissant une segmentation binaire des phrases source et cible (le symbole « . » désigne la concaténation de chaînes de mots) :  S = A . ¯A avec A = [s  . . . s ] et ¯ A = [s . . . s ] C = B . ¯B avec B = [c . . . s ] et ¯B = [c . . . c ]  Le choix de x et y sera guidé par la somme W des scores d'association entre chacun des mots  source et cible d'un couple de segments (X , Y )  {A, ¯A} × {B, ¯B} :  W (X , Y ) =  w(s, c)  On retrouve l'ensemble des notations utilisées dans la figure 2, qui donne une représentation  schématique de la segmentation d'un couple de phrases.  On définit alors :  cut(X , Y ) = W(X , ¯Y) + W( ¯X, Y )  Notons que cut(X , Y ) = cut( ¯X, ¯Y). Dans notre cas, une valeur faible indique que les scores  d'association entre les mots de X et ¯ Y d'une part, et entre ceux de ¯ X et Y d'autre part, sont faibles également, autrement dit que ces deux couples de segments ont peu de chances d'être de bonnes traductions, (X , Y ) et ( ¯X, ¯Y) constituant alors éventuellement de bonnes traductions. Idéalement donc, nous désirons déterminer le couple (x, y) qui mène à la plus petite valeur de cut(X , Y ) possible. Zha et al. (2001) pointent néanmoins le fait que cette quantité tend à produire des segments (clusters de documents dans leur cas) déséquilibrés du fait de l'absence de normalisation, et en proposent par conséquent une version normalisée :  Ncut(X , Y ) =  cut(X , Y ) cut(X , Y ) + 2 × W(X , Y ) + cut( ¯X, ¯Y) cut( ¯X, ¯Y) + 2 × W( ¯X, ¯Y)  Cette variante permet de rajouter une contrainte de densité sur (X , Y ) et ( ¯X, ¯Y), ce qui est  partiellement satisfait par l'introduction des dénominateurs dans l'expression ci-dessus. Sa valeur est comprise entre 0 et 2.  Notre problème consiste finalement à déterminer le couple (x, y) qui minimise Ncut. Bien que  des méthodes de recherche performantes existent et sont couramment utilisées en théorie des graphes, nos « graphes » (couples de phrases) sont petits en pratique : environ 30 mots par phrase en moyenne dans le corpus Europarl que nous utilisons pour la suite de nos expériences. Nous nous contentons donc par la suite de déterminer la meilleure segmentation en testant toutes les coupures possibles.  À partir du critère défini précédemment, nous pouvons segmenter et aligner un couple de  phrases de façon récursive. À chaque étape, nous testons tous les couples (x, y) possibles afin de déterminer le plus faible Ncut. Le pire des cas se produit lorsque la matrice est coupée de la façon la plus déséquilibrée possible ; la complexité de l'algorithme est donc cubique (de l'ordre de I × J × min(I, J)). Pour un couple (x, y) donné, nous calculons deux valeurs : l'une correspondant à un alignement monotone (Ncut(A, B)) et l'autre à une inversion des deux segments (Ncut(A, ¯B)). Le processus est alors appliqué sur chacun des couples de segments correspondant au Ncut minimal. Il s'arrête lorsqu'un segment ne comporte qu'un seul mot : les alignements produits sont tous de multiplicité 1-n ou n-1, et il en résulte que tous les mots sont nécessairement alignés. Des variantes où le processus récursif s'arrête plus tôt sont envisageables, en fixant un seuil sur Ncut par exemple, auquel cas les alignements produits seraient de multiplicité m-n. Nous gardons cette possibilité pour des recherches futures.  La figure 3 présente l'algorithme complet, et la figure 4 illustre le processus sur deux exemples  réels. Dans la suite de l'article, nous ferons référence à cet algorithme sous le nom « Cutnalign ».  L'algorithme en lui-même est indépendant de la taille du corpus parallèle à aligner, car chaque  couple de phrases est traité indépendemment des autres. On peut donc très facilement paralléliser l'alignement d'un corpus : le temps d'alignement total est divisé par le nombre de processeurs à disposition. Un autre avantage est que les alignements produits sont symétriques tout au long du processus, contrairement à des modèles plus répandus comme les modèles IBM qui produisent de  meilleurs résultats lorsqu'exécutés dans les deux sens de traduction puis leurs sorties combinées  à l'aide d'heuristiques. Nous évaluons notre méthode d'alignement en tant que premier module d'un système de traduction automatique statistique par segments. Nous utilisons pour cela le système de traduction Moses (Koehn et al., 2007), et des données constituées d'un échantillon du corpus parallèle Europarl (Koehn, 2005), couvrant trois couples de langues : finnois-anglais (langue agglutinante- langue isolante), français-anglais, et portugais-espagnol (langues très proches). Pour chacun, nous utilisons un jeu d'entraînement de 350 000 couples de phrases (30 mots par phrase en moyenne en anglais), et des jeux de développement et de test de 2 000 couples de phrases chacun. L'optimisation des systèmes est réalisée à l'aide de la procédure MERT (Och, 2003). Sauf mention contraire, un modèle de réordonnancement lexicalisé est utilisé. Nous comparons quatre approches :  MGIZA++ (Gao et Vogel, 2008), implémentant les modèles IBM (Brown et al., 1993) et le  modèle caché de Markov de Vogel et al. (1996). Intégré à Moses, il s'agit toujours de la référence du domaine. Nous l'utilisons avec ses paramètres par défaut, en enchaînant 5 itérations de chacun des modèles IBM1, HMM, IBM3 et IBM4. Une table de traductions est ensuite produite à partir des alignements à l'aide des outils de Moses.  Anymalign (Lardilleux et al., 2011a), produisant directement des tables de traductions. Cet  outil pouvant être arrêté à tout moment, nous fixons son temps d'exécution de façon à ce qu'il soit exécuté pendant la même durée que MGIZA++. Nous répétons la même expérience en faisant varier son paramètre « -i », permettant de contrôler la longueur des segments qu'il produit en sortie, de 1 à 4 (voir détails dans (Lardilleux et al., 2011b)). Nous y faisons référence par la suite sous les noms « Anymalign-1 » à « Anymalign-4 ». Le modèle de réordonnancement utilisé dans cette configuration n'est qu'un simple modèle basé sur la distance entre mots, car Anymalign seul ne peut fournir l'information nécessaire à un modèle de réordonnancement lexicalisé.  Anymalign + Cutnalign : nous appliquons l'algorithme décrit dans la section précédente à  chacune des quatre tables de traductions produites par Anymalign-1 à Anymalign-4. Les alignements obtenus sont utilisés pour construire de nouvelles tables de traductions à l'aide du jeu d'outils de Moses.  Simples probabilités + Cutnalign : cette configuration permet d'évaluer non pas l'algorithme  proposé précédemment, mais le choix de la fonction w, qui sert de base à l'algorithme. Nous utilisons pour cela un score d'association très simple : la probabilité qu'un mot source et un mot cible soient traductions l'un de l'autre (produit des deux probabilités de traduction), cette probabilité étant calculée à partir de leurs occurrences dans le corpus d'entraînement. La définition de w est donc ici la même qu'à la section 2.1, à deux différences près : - les décomptes ne sont pas effectués sur une table de traductions produite par Anymalign, mais directement sur le bi-texte d'entraînement ; - k = 1, n. Les traductions sont évaluées selon les mesures BLEU (Papineni et al., 2002) et TER (Snover et al., 2006, contrairement à BLEU, des scores faibles sont meilleurs).  Les résultats sont présentés dans le tableau 1. Sur chacune des trois tâches, Anymalign (version  « de base ») est plus ou moins en retrait par rapport à MGIZA++. L'utilisation du paramètre « -i » permet de réduire cet écart de moitié environ, à l'exception notable du couple finnois- anglais (langue agglutinante-langue isolante), ce qui est conforme aux résultats présentés dans (Lardilleux et al., 2011b).  L'ajout de Cutnalign mène à un gain considérable dans toutes les configurations : de 1,6 à  4,6 points BLEU (fr-en, Anymalign-1 + Cutnalign), avec un gain moyen de 2,6 points BLEU et 2,7 points TER. Anymalign+Cutnalign est toujours en retrait de 1,1 à 1,6 point BLEU en finnois-anglais par rapport à MGIZA++, mais produit des résultats de même qualité, voire meilleurs mais de façon non significative, en français-anglais et portugais-espagnol.  L'approche « simples probabilités + Cutnalign » produit des résultats de qualité intermédiaire,   Comme précisé en introduction, l'une des raisons pour laquelle nous avons proposé cette méthode  d'alignement est que, malgré de récentes améliorations, Anymalign peine toujours à extraire suffisamment de traductions de longs n-grammes. Dans cette section, nous étudions quelques caractéristiques des alignements produits par la méthode que nous avons proposée. Elles sont présentées dans le tableau 1.  En ce qui concerne les tables de traductions d'abord, on constate que celles qui sont obtenues  à partir de Cutnalign contiennent un nombre beaucoup plus important d'entrées que les tables correspondantes produites par Anymalign seul (trois fois plus en moyenne), à l'exception notable d'Anymalign-1 en finnois-anglais. Elles sont néanmoins toujours beaucoup plus petites que les tables obtenues à partir de MGIZA++ et contiennent deux fois moins d'entrées en moyenne. La longueur moyenne de ces entrées est en outre quasiment égale à celles des tables de traductions de MGIZA++, alors que celles produites par Anymalign sont beaucoup plus courtes : la production d'une table de traductions à partir de liens d'alignement permet bien de combler le manque de longs n-grammes comme nous le désirions.  Dans un second temps, nous étudions plus en détail les liens d'alignement à proprement parler,  tels qu'ils sont avant la production des tables de traductions. La colonne « Liens » du tableau 1 montre que le nombre de liens d'alignement produits par notre méthode est bien supérieur à celui de ceux produits par MGIZA++ : entre 1,5 et 3 fois plus selon la tâche. La dernière colonne en donne la principale raison : les blocs d'alignement extraits par notre méthode, c'est-à-dire les rectangles obtenus au niveau de récursion maximal, sont toujours plus longs que les blocs minimums obtenus à partir des alignements de MGIZA++ (+ 26 % en moyenne). Comme nous alignons systématiquement tous les mots source avec tous les mots cible d'un tel rectangle, et tous les mots d'un couple de phrases étant par conséquent nécessairement alignés, le nombre total de liens produits est naturellement élevé. Cela explique également le fait que le nombre d'entrées dans les tables de traductions est toujours beaucoup plus faible que dans celles obtenues à partir de MGIZA++, ce dernier produisant des alignements de multiplicité 0-1 qui sont à l'origine de l'extraction de très nombreux segments lors de la constitution de la table par Moses (heuristique grow-diag-final-and par défaut) (Ayan et Dorr, 2006). Malgré cela, les alignements produits par notre méthode permettent d'atteindre des scores identiques à l'état de l'art dans deux tâches de traduction automatique sur trois dans nos expériences.  Nous avons présenté une méthode d'alignement sous-phrastique fondée sur un découpage  récursif binaire de la matrice d'alignement entre une phrase source et sa traduction. Inspirée des travaux de Wu (1997) et Deng et al. (2006) sur l'alignement et de Zha et al. (2001) sur le clustering de documents, nous avons montré qu'en dépit de sa simplicité, cette méthode produit des résultats du niveau de l'état de l'art dans deux tâches sur trois dans nos expériences. Couplée à Anymalign, elle permet des gains conséquents (jusqu'à 4,6 points BLEU en français-anglais) par rapport à l'utilisation d'Anymalign seul. Nos expériences ont confirmé que le principal handicap d'Anymalign concerne bien les traductions de longs n-grammes. Une étape complémentaire d'alignement au sens strict du terme se révèle donc souhaitable pour améliorer ses résultats en traduction automatique, car elle permet de combler la plupart de ses manques en termes de traductions de longs segments. La méthode d'alignement proposée ici est relativement simple, symétrique du point de vue du sens de la traduction, et le caractère local du calcul des alignements lui permet de passer facilement à l'échelle. Dans l'optique d'améliorer les alignements, de multiples enrichissements de la méthode sont possibles, comme par exemple l'intégration des valeurs seuils lors de la recherche du meilleur découpage de la matrice afin d'arrêter le processus d'alignement à des blocs plus larges et plus sûrs, ou encore l'examen d'un découpage ternaire plutôt que binaire afin de rendre compte de constructions linguistiques plus complexes générant des constituants non connexes.  Ces travaux ont été financés par le projet Cap Digital SAMAR.   
