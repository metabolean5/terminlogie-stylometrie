Ces dernières décennies, le développement considérable des technologies de l'information et de la  communication a modifié la manière dont nous accédons et manipulons les connaissances. Nous constatons une diversité toujours plus importante des types de contenus échangés (texte, image, audio, vidéo, SMS, tweet, données statistiques, spatiales, etc.), ce qui nécessite de résoudre de nombreuses problématiques, parmi lesquelles la recherche d'information, qui a intéressé la communauté du TALN dès les années 90 avec les campagnes d'évaluation MUC (Grishman et Sundheim, 1996). Les travaux sur le sujet ont porté une attention particulière aux noms propres de personnes, de lieux et d'organisations, appelés entités nommées (EN). Au gré des besoins, celles-ci ont été étendues aux dates, aux expressions numériques, aux marques ou aux fonctions, avant de recouvrir un large spectre d'expressions linguistiques.  De nombreux systèmes ont été élaborés pour réaliser la reconnaissance d'entités nommées (REN),  selon des approches orientées connaissances ou orientées données. Les premières ont généralement une grande précision mais nécessitent un coup humain de développement important, ce qui se traduit généralement par une couverture (et donc un rappel) perfectible. Les approches orientées données, par ajustement automatique de paramètres d'un modèle numérique, permettent d'obtenir de bonnes performances, avec un coup d'entrée limité, du moment où l'on dispose d'une base d'apprentissage de taille suffisante. Ils sont également réputés présenter une dégradation graduelle de leurs performances sur des données bruitées. Cependant, l'aspect "boîte noire" des algorithmes d'apprentissage rend difficile l'amélioration ciblée de leurs performances.  Ces constats ont été vérifiés par de nombreuses campagnes d'évaluation. À titre d'exemple, lors  de la campagne d'évaluation francophone Ester2 (Galliano et al., 2009), portant sur le traitement de transcriptions de parole radio ou télédiffusée, les deux meilleurs systèmes travaillant sur des transcriptions manuelles étaient des systèmes à base de connaissance, tandis que les tests effectués sur des sorties de reconnaissance de la parole ont été dominés par un système orienté données.  Les travaux que nous présentons dans cet article ont été menés dans le cadre de la campagne  Etape (qui a fait suite à Ester2) qui visait notamment à évaluer des systèmes de REN sur des flux de parole conversationnelle. Nous y proposons une approche novatrice pour la REN : l'utilisation de méthodes de fouille de données séquentielle hiérarchique. À nos yeux, ces travaux présentent plusieurs originalités du point de vue du TALN : (i) nous élaborons un moyen-terme entre les approches orientées données et orientées connaissances reposant sur la recherche, à partir de données d'apprentissage, de motifs pour la REN : cette technique centrée données permet l'extraction de connaissances interprétables ; (ii) la stratégie de détection des entités nommées est originale, par la recherche séparée du début et de la fin des entités, en nous appuyant sur le contexte immédiat pour placer les balises d'annotation : cela présente l'intérêt de conserver une certaine robustesse en cas de disfluence ou d'erreur de reconnaissance au sein de l'entité nommée.  Cet article porte sur l'élaboration, l'implémentation et l'évaluation d'une telle approche. En partie  2, nous faisons un état de l'art des approches pour la REN. La partie 3 présente le formalisme de fouille pour l'extraction de règles d'annotation et leur utilisation pour reconnaître des entités nommées. En partie 4, nous décrivons le jeu de données utilisé et les résultats obtenus lors de l'évaluation dans le cadre de la campagne Etape.  Les approches orientées connaissances sont basées sur la description de règles décrivant les  entités nommées et leur contexte à l'aide d'indices linguistiques fournis par le texte lui-même et des ressources externes (dictionnaires). Généralement, les textes sont étiquetés syntaxiquement (éventuellement sémantiquement) grâce aux dictionnaires, puis un ensemble de règles, qui prennent en compte les indices morphologiques (présence de majuscule, ponctuation), morphosyntaxiques et sémantique, permettent de repérer les ENs. Les règles utilisent ces éléments, soit comme preuves internes de la présence d'une entité nommée, soit par description de son contexte d'apparition (McDonald, 1996; Friburger et Maurel, 2004). Une preuve interne sera, par exemple, la présence d'un prénom avant un mot commençant par une majuscule ; ce prénom indiquera un nom de personne (ex : 'François Hollande'). Nous voyons que c'est la "connaissance" qui guide cette approche, celle de l'expert qui créé les règles, selon les informations à sa disposition (dont les ressources externes).  Dès les années 1990, un certain nombre de systèmes (Stephens, 1993; Hobbs J. R. et Tyson, 1996)  mettent en oeuvre cette approche orientée connaissances. Les automates sont particulièrement adaptés à l'élaboration et l'utilisation des règles. De plus, l'utilisation de transducteurs permet de produire très intuitivement une annotation à l'aide de balises ('<pers>', '</pers>', '<org>', '</org>', etc.), ils sont donc largement utilisés pour ce type de tâche (Friburger et Maurel, 2004; Brun et Ehrmann, 2010; Béchet et al., 2011). Enfin, les transducteurs peuvent être organisés sous forme de cascades, chaque transducteur permettant de lever des ambiguïtés et de mettre à disposition des reconnaissances pour les transducteurs suivants (ce qui permet de reconnaître des imbrications). L'ordre dans lequel sont appliqués les transducteurs a alors une grande importance.  Étant donné les traitements qu'elles mettent en oeuvre, les approches orientées connaissances  insèrent au sein des séquences de mots ce que nous appelons des marqueurs, comme le montre la figure 1 pour l'expression 'fondation Cartier'.  F  1 - Annotation par balises  Les approches orientées connaissances peuvent être utilisées et adaptées à des textes sans  apprentissage préalable. Leur limitation est liée au fait que les ressources utilisées sont rarement exhaustives (par exemple, les noms propres forment une classe "ouverte") : il semble illusoire de bâtir ce type d'approche sur l'hypothèse d'un lexique complet des entités nommées existantes. Les approches orientées données paramètrent un modèle automatiquement grâce à un apprentissage sur un corpus d'entraînement. Ce corpus d'entrainement, créé par des experts, fournit de nombreux exemples de données : le système apprend sur ces exemples puis prédit l'étiquette d'une nouvelle donnée, selon son modèle. Le corpus d'entrainement est constitué d'un ensemble de textes annotés en entités nommées par des experts. L'apprentissage automatique sera chargé d'ajuster les paramètres disponibles, cette procédure étant guidée à chaque itération par les erreurs que commet le système sur les jeux de données disponibles. Une fois l'apprentissage réalisé, le système est en mesure d'annoter de nouveaux textes en entités nommées selon les paramètres de son modèle. Traditionnellement, l'apprentissage automatique se rapproche plutôt d'une classification (attribution d'une classe à un mot) que d'une annotation (délimitation d'une expression linguistique).  Pour la REN, le format BIO  s'est imposé. La figure 2 présente la classification par mots réalisée pour l'énoncé '<org> fondation <pers> Cartier </pers> </org>'. Signalons qu'en partie 3, nous présentons une approche orientée donnée, mais qui est apparentée à un mécanisme de transduction (à l'aide d'indices locaux) plutôt que de classification.  F  2 - Annotation par classification Généralement, ces approches estiment la probabilité des classes selon les tokens et les informations qui y sont associées. Parmi les modèles numériques adaptés, figurent les modèles bayésiens, la régression logistique (ou maximum d'entropie), les machines à vecteur de support (SVM) , etc. La régression logistique a démontré son efficacité pour la reconnaissance d'entités nommées (Mikheev et al., 1999; Ekbala et al., 2010), permettant de prendre en compte de multiples traits discriminants (morphologiques, morpho-syntaxiques, lexicaux) interdépendants. D'autres modèles tirent parti de la séquentialité, comme les HMM (Bikel et al., 1999), par modélisation des transitions entre états (types d'entités nommées) et des générations d'observations (mots).  Pour prendre en compte simultanément la multiplicité des indices locaux et les aspects séquentiels  au sein d'un modèle unifié, les MEMM (McCallum et al., 2000) puis les CRF (Raymond et Fayolle, 2010; Zidouni et al., 2010) sont les modèles réputés les plus adéquats à ce jour. L'inconvénient est qu'ils restent difficiles à interpréter : les traits découverts sont généralement composites et exhibent des dépendances complexes dont il est difficile d'affirmer qu'elles sont nécessaires ou suffisantes pour déterminer les entités nommées.  A ce jour, les approches orientées données se basent majoritairement sur une représentation  "plate" des entités nommées. Comme nous le verrons en partie 4, nous cherchons à réaliser la REN structurée (avec imbrications). Notons que quelques travaux (Finkel et Manning, 2005; Dinarelli et Rosset, 2011) ont adapté avec un certain succès des méthodes orientées données à la reconnaissance de structure.  De manière générale, nous remarquons que les approches automatiques nécessitent un travail  préalable conséquent (préparation des jeux de données, implémentation du modèle, des procédures d'apprentissage et d'estimation, sélection des traits et dépendances pertinents, etc.) avant d'être en mesure de paramétrer les modèles, et qu'il reste difficile de les utiliser pour extraire des connaissances ou pour étudier des phénomènes particuliers.  Nous le voyons, les approches guidées par les données s'appuient sur des indices locaux variés.  La nature "locale" de la structuration en entités nommées est alors un atout. Les systèmes orientés connaissances ont l'avantage de modéliser la structure interne des entités nommées. Ainsi un système à base de connaissances aura plus de facilité à analyser l'encapsulation d'entités nommées comme dans l'exemple suivant (issu d'Etape) : 'le député UMP de Haute-Saône' où l'entité nommée globale est construite à l'aide de l'entité 'UMP', de type organisation, et de l'entité 'Haute-Saône', de type division géographique administrative.  Cependant, ces dernières approches utilisent une connaissance dont la construction est coûteuse et  délicate. Aussi avons-nous souhaité développer une approche permettant l'extraction automatique sur corpus de motifs se rapprochant des règles de reconnaissance mises en oeuvre par la REN symbolique. La fouille hiérarchique séquentielle de données est adéquate à cet effet. Par ailleurs, les systèmes orientés connaissances sont aujourd'hui contraints à modéliser intégralement la structure des entités, voire de ses contextes d'introduction. Ce choix est discutable et met à l'épreuve la robustesse des systèmes lorsqu'ils traitent de la parole spontanée. Une erreur de reconnaissance sur un seul mot de l'entité (dûe par exemple à une disfluence) empêche l'application de la règle de détection.  Afin de répondre à cette insuffisance, nous proposons de séparer la détection du début et de  la fin de l'entité, pour ensuite chercher à associer une marque de début et de fin d'entité. Notre hypothèse est que l'on dispose de suffisamment d'indices locaux pour caractériser précisément le début ou la fin d'une entité.  Considérons pas exemple l'énoncé annoté suivant 'En <date> <num> 1969 </num> </date>  <pers> <prenom> Georges </prenom> <famille> Pompidou </famille> </pers> dirige la <org> <loc> France </loc> </org>'. Notre hypothèse est que chacune des marques d'annotation ('<pers>', '<prenom>', '</prenom>', '</pers>', etc.) est détectable séparément. De plus, la détection d'une entité encapsulée telle que '<prenom>' peut guider la détection de l'entité englobante. Il s'agira, pour le système, d'extraire des règles d'annotation, d'estimer localement les marqueurs probables, puis de déterminer, par leurs combinaisons, l'annotation la plus vraisemblable. Nous implémentons un système de reconnaissance d'entités nommées, mXS, selon cette approche originale. Grâce à ce procédé, notre système reconnaît par exemple le montant 'deux cent ça compte mille' (erreur de transcription pour deux cent cinquante mille), alors qu'un système symbolique sera mis en difficulté.  F  3 - Représentation des structures à fouiller  L'approche que nous mettons en oeuvre repose sur des analyses fréquemment conduites pour  traiter le langage naturel (morpho-syntaxe, lexiques). Pour la fouille, ces traitements sont interprétés comme autant d'enrichissements des données, à utiliser pour rechercher des motifs généralisés dans les données. La figure 3 présente de manière schématique, sur l'exemple 'Pierre a visité le Centre Georges Pompidou', la manière dont se superposent ces enrichissements. La fouille de données devra alors tenir compte de deux axes : paradigmatique, pour la superposition d'enrichissements, et structurel, pour l'examen des contigüités entre items. Comme nous le verrons par la suite, ce processus est flexible : les enrichissements peuvent être plus ou moins profonds selon les éléments considérés. Nous pouvons moduler à volonté l'axe paradigmatique selon les éléments observés et la tâche d'annotation à réaliser.  3.1.1 Morpho-syntaxe   Nous réalisons conjointement la tokenisation, la lemmatisation et l'étiquetage morpho-syntaxique  avec (Schmid, 1994). De surcroît, nous en adaptons la sortie comme suit :  - Déterminants : les déterminants définis ('le', 'la', 'les', 'l") sont sous-catégorisés en 'DET/DEF'.  - Prépositions : la sous-catégorie 'PRP:det' ('au', 'du', 'des') forme une catégorie 'PRPDET'. - Nombres : les nombres sont sous-catégorisés selon leur nombre de chiffres . - Noms propres et abréviations : ces deux catégories se généralisent en 'NAMABR'. - Nom propres, abréviations, noms, verbes : ces éléments sont sous-catégorisés par le suffixe des trois derniers caractères ('NOM/SUFF:ier', 'NAMABR/NAM/SUFF:ges', 'VER/SUFF:vre'). - Verbes : les sous-catégories relatives au mode et temps du verbe sont supprimées.  Pour le processus de fouille de données, nous omettons les variations surfaciques (majuscules)  et flexionnelles (déclinaisons et conjugaisons) : nous ne conservons pas les items lexicaux euxmêmes et faisons reposer la recherche de motifs sur les lemmes proposés par . Par exemple, le 'En 1970 les socialistes [...]' donnera la séquence : 'PRP/en NUM/DIGITS:4/PREF:19/1970 DET/DEF/le NOM/SUFF:ste/socialiste'. 'NUM/DIGITS:MANY' 'NUM/DIGITS:4/PREF:20' 'NUM/DIGITS:1' 3.1.2 Lexiques  Les lexiques nous permettent d'ajouter un niveau sémantique aux hiérarchies. Nous exploitons  des ressources diverses, dont certaines sont importées à partir des dictionnaires et motifs du système CasEN . Nous y ajoutons quelques listes, constituées manuellement, en particulier pour les fonctions, lieux, organisations, quantités et dates. Ces ressources contiennent 221 547 expressions distinctes qui produisent 443 112 catégorisations sémantiques . Une large part est dédiée à la reconnaissance des personnes et des lieux. Signalons qu'une partie de ces ressources est générée à partir d'automates (transducteurs CasEN) qui reconnaissent des expressions linguistiques utiles à la REN.  Ces ressources sont utilisées telles quelles pour produire les enrichissements. Ceux-ci peuvent  alors être sémantiquement ambigus, ce que nous notons comme une disjonction exclusive . Par exemple, au nom propre Washington seront affectées les catégories sémantiques 'CELEB TOPO ORG-LOC-GOV PREN VILLE'. Notons ici que nous considérons que les noms propres forment une classe ouverte et qu'ils n'ont pas vocation à être utilisés lexicalisés au sein des motifs extraits : lorsqu'ils ont donné lieu à des enrichissements sémantiques, les items lexicaux sont omis afin que la fouille de données ne repose que sur les catégories sémantiques.  Les données ainsi enrichies forment le langage   et ont vocation a être fouillées afin d'y rechercher des motifs séquentiels d'intérêt (Fischer et al., 2005; Cellier et Charnois, 2010) pour la REN.  Le langage des motifs   comprend celui des données enrichies et toutes leurs généralisations. Un élément de motif (item) couvre une donnée, notée  , lorsqu'il s'y trouve en tenant compte des disjonctions . Par exemple, l'item 'TOPO/Washington' couvre la donnée enrichie 'CELEB/Washington TOPO/Washington'. Dès lors, nous nous inspirons de travaux intégrant des hiérarchies aux séquences (Srikant et Agrawal, 1996), en y ajoutant la notion de segment particulièrement adaptée au traitement de structures au sein desquelles des items se répètent (comme des syntagmes sémantiquement catégorisés).  Couverture d'un motif de segments sur des données : soient un motif de segments P =  p p . . . p   et une séquence de la base de données enrichie I = i i . . . i   , alors P couvre les segments de I, noté P  I, s'il existe une fonction discrète croissante S() définie de [1, p] vers [1, n] telle que, pour tout j  [1, p], alors p  i Ce même mécanisme sera pris en compte lorsqu'il s'agit de généraliser selon l'axe paradigmatique : l'objectif est que, par exemple, 'CELEB' couvre indifféremment 'Pompidou' et 'Valery Giscard d'Estaing'. Plus généralement, nous définissons trois relations de généralisation entre motifs :  - Généralisation hiérarchique entre motifs de segments : soient deux motifs de segments  P = p p . . . p   et Q = q q . . . q   , alors P généralise hiérarchiquement les segments de Q, noté P  Q, s'il existe une une fonction discrète croissante S() définie de [1, p] vers [1, n] telle que, pour tout j  [1, p], alors p  q . - Généralisation par affixation entre motifs : soient deux motifs P = p p . . . p   et Q = q q . . . q   , alors P généralise par affixation Q, noté P  Q, si p  n et s'il existe au moins un k  [0, p  n] tel que, pour tout j  [1, n], alors q = p . - Généralisation sur marqueurs entre motifs : soient deux motifs P = p p . . . p   et Q = q q . . . q   , alors P généralise sur marqueurs Q, noté P  Q, si p  n et s'il existe une fonction discrète strictement croissante C() définie de [1, n] vers [1, p] telle que, pour tout j  [1, n], alors p = q et, pour tout k  [1, p] tel que k  {C(j), j  [1, n]}, alors q   .  Ces généralisations nous permettent de rechercher des motifs dans lesquels apparaissent les  marqueurs d'entités nommées. Par exemple, au sein de l'énoncé 'Le <fonc> président </fonc> <pers> Georges Pompidou </pers> débattait souvent.', nous relevons, par relations de couverture et de généralisation, une occurrence pour les motifs 'NOM/président <pers> CELEB </pers>' ou 'NOM/président CELEB </pers> VERB/débattre', par exemple.  Finalement, La notion de règle d'annotation partielle découle de celle de motif de segments :   Règle d'annotation partielle une règle d'annotation partielle est un motif de segments P    contenant au moins un élément de  et un élément de  .  Notons qu'à ce stade les règles d'annotation contiennent un nombre indéterminé de marqueurs.  Il conviendra de filtrer au besoin lors de l'extraction des motifs et de s'assurer que l'on utilise ces règles de manière adéquate afin de produire une annotation.  La combinatoire du langage   étant importante, il est nécessaire de filtrer les règles. Pour cela, nous déterminons la fréquence et la confiance des règles, afin d'éliminer celles qui n'ont que peu d'intérêt. À l'aide de la couverture et des généralisations définies ci-dessus, nous déterminons la fréquence F req(P, ) d'une règle P comme son nombre d'occurrences au sein du corpus . La confiance d'une règle d'annotation P estime la proportion de phrases où la règle est appliquée avec justesse :  Con f (P, ) =  F req(P, ) F req(Ret (P), ) (la fonction Ret (P) retire les marqueurs de P)  Même en fixant des seuils de support et confiance sélectifs, les règles d'annotation peuvent être  trop nombreuses à cause des combinaisons possibles au travers de la hiérarchie. Afin de contenir cette abondance de règles, nous proposons de grouper les règles, puis d'éliminer celles qui ne sont pas informatives, à l'instar de (Pasquier et al., 1999). L'idée forte est que deux motifs qui couvrent les mêmes exemples sont redondants car ils appartiennent à la même classe d'équivalence :  Équivalence de motifs au regard d'une base de données : soient P et Q deux motifs et  une  base de données, alors P est équivalent à Q au regard de , notée P  Q, si P  Q ou Q  P et F req(P, ) = F req(Q, )  Dans la suite, plutôt que d'extraire toutes les règles d'une même classe d'équivalence, nous nous  contentrerons des motifs les plus spécifiques car ils sont porteurs de plus de corrélations. Par ailleurs, nous étendons cette équivalence par une marge de tolérance lors de la comparaison des fréquences à %, ce que nous appellons alors filtrage .  Les règles d'annotation sont utilisées par  pour réaliser l'annotation en entités nommées. Pour une position j d'un texte, de nombreuses règles peuvent proposer des marqueurs. Nous estimons la probabilité d'insérer des marqueurs en M (transductions) par régression logistique, ce qui nous permet de tenir compte de la multiplicité des règles P   selon la formule : P(m  M | ) = · exp    Dans une annotation (et plus particulièrement si elle est structurée), plusieurs marqueurs peuvent  se trouver à une position donnée. Il nous faut être en mesure de faire le lien entre la probabilité d'insérer un marqueur individuel et celle d'insérer une séquence de marqueurs. Pour cela, nous tenons compte des statistiques issues du corpus sous forme de probabilités conditionnelles :  P(M  = m m . . . m ) = 1 p ·  P(m  M | )P(m . . . m |m )  Lorsque les probabilités de séquences de marqueurs P(M  ) sont estimées, nous les utilisons afin de déterminer quelle est, pour un énoncé donné, l'annotation la plus vraisemblable parmi les annotations valides. Une hypothèse d'indépendance entre marqueurs au sein d'un énoncé nous permet de résoudre la recherche de l'annotation par programmation dynamique.  T  1 - Caractéristiques du corpus Etape  Le travail a été réalisé dans le contexte de la campagne d'évaluation Etape  , en interaction avec le programme Quaero . Cette campagne a porté sur le traitement d'émissions radiodiffusées et télévisuelles, donc orales et en partie spontanées. L'objectif est d'annoter les entités nommées structurées, tant sur les transcriptions manuelles qu'en sortie de systèmes de reconnaissance de la parole. La table 1 indique les parties à disposition. Le corpus étant en cours d'adjudication, nous ne l'utilisons pas pour mener nos expériences. est volumineux et reste difficile à exploiter par la fouille. En conséquence, nous n'utilisons que (extraction des règles et paramétrage du modèle) et (évaluation). Les types principaux d'entités nommées sont les personnes ( )), fonctions ( ), organisations ( ), lieux ( ), productions humaines ( ), points dans le temps ( ), quantités ( ) et évènements ( ). À granularité fine (sur laquelle est réalisée l'évaluation), ils sont répartis en 34 sous-types. La figure 4 indique leur répartition au sein du corpus Etape. Notons que les entités nommées sont étendues à des expressions construites à partir de noms communs, ce qui amène à considérer une large gamme d'expressions linguistiques.  F  4 - Répartition des types principaux d'entités  En plus des entités nommées, leurs composants sont annotés, soit spécifiques à certains types  (jour, mois, etc. pour une date) ou transverses (valeur, unité, qualificateur, etc.). Ces éléments permettent de mieux décrire les entités lors de leur annotation (Rosset et al., 2011).  Le nombre d'entités nommées rapporté au nombre de tokens du corpus est de 12,3%, dont 4,8%  pour les entités et 7,5% pour les composants. Globalement, ce corpus, quoiqu'assez volumineux, est bien équilibré pour les types principaux d'entités et de composants. Notons que nous réalisons l'exploration des données sur un corpus qui contient des disfluences, répétitions, etc.  Pour implémenter la fouille de données, nous construisons un arbre des préfixes communs  par niveaux, le processus est optimisé en exploitant la propriété d'anti-monotonie (Agrawal et Srikant, 1995) et les hiérarchies (Wang et Han, 2004). De plus, nous poussons deux contraintes supplémentaires pour l'extraction des règles d'annotation :  - Nombre de marqueurs : une règle d'annotation partielle ne contient qu'un marqueur.  - Niveaux : le nombre d'itérations de l'algorithme par niveaux est limité à 7.  L'approche que nous adoptons nous permet d'explorer exhaustivement les motifs fréquents et  confiants. Les seuils minimaux sont fixés à 3 en fréquence et 5% en confiance. Le système extrait alors 143 205 règles d'annotation partielles . La figure 5 montre que la longueur des règles varie autour de trois éléments, et leur profondeur d'items se situe autour de quatre. Ces statistiques confirment que les règles d'annotation sont explorées sur les deux axes que nous avons définis. Nous voyons aussi que la répartition des règles d'annotation par types d'EN est diversement corrélée au corpus. Les types et sont moins représentés : il y a moins de descripteurs pour ces types, il pourrait alors être assez homogène dans les données. Inversement, le type , est sur-représenté et nous faisons l'hypothèse qu'il est assez hétérogène.  F  5 - Caractéristiques des règles d'annotation extraites  Nous utilisons l'outil scikit-learn  (Pedregosa et al., 2011) pour réaliser la régression logistique. La figure 6 présente les résultats obtenus en SER et les taux par types d'erreurs (Galibert et al., 2011). Ces graphiques confirment que le système réduit graduellement ses erreurs à mesure que les seuils de fréquence et de confiance sont abaissés.  F  6 - Performances (SER) et erreurs selon la Fréquence (F) et la Confiance (C)  Nous menons des expériences supplémentaires , dont les résultats sont reportés dans le tableau 2  pour les configurations suivantes :  -  : système par défaut - : désactivation des ressources lexicales - : apprentissage en fusionnant les corpus et - : filtrage  à 25%, - : filtrage  à 50%, - : filtrage  à 75%, Le système donne des résultats satisfaisants, étant donné la difficulté de la tâche. Sans surprise, la désactivation des dictionnaires dégrade considérablement les performances. Lorsque les données comportent les données d'évaluation ( ), le surapprentissage est modéré, ce qui est lié au fait que les règles d'annotation ne sont pas lexicalisées. Les expériences nous montrent clairement que le système obtient encore des performances très acceptables lorsque l'on réduit significativement le nombre de règles extraites à l'aide du filtrage .  T  2 - Performances (SER), erreurs d'Insertion (I), de Délétion (D), de Substitution (S), Précision (P), Rappel (R), F-mesure (Fm) des approches  Nous menons des évaluations séparées des types primaires (sans sous-types) d'entités nommées  et de composants. La figure 7 en donne les résultats. Les entités nommées sont moins bien reconnues que les composants et plusieurs types (en particulier les expressions de temps) posent encore problème. Ceci dit, le système équilibre relativement bien sa précision et son rappel et la reconnaissance d'entités nommées selon l'approche présentée donne des résultats.  F  7 - SER, précision (gauche) et rappel (droite) par types primaires et composants  La phase d'adjudication de la campagne d'évaluation ETAPE n'est pas achevée à l'heure de  la rédaction de cet article. Nous avons cependant été autorisés à reporter en table 3 les performances anonymes des systèmes avant adjudication. Les SER présentés sont donnés sur les transcriptions manuelles et sur les sorties de différents systèmes de reconnaissance, pour lesquels sont mentionnés les WER .  Parmi les autres systèmes participants, le système 3 utilise des CRF (binarisés, un par type), le  système 6/7/8 utilise un CRF pour les composants et un PCFG pour reconstituer les entités, CasEN utilise des transducteurs. De manière générale, affiche de bonnes performances (entre la 1 ère et la 3 ème position). Les taux d'erreurs élevés sont liés à la difficulté de la tâche (parole spontanée, imbrications, typologie fine). Sans surprise, les performances sont dégradées sur les données bruitées par la reconnaissance de parole. Nous voyons que et résiste bien aux erreurs de reconnaissance de la parole.  T  3 - SER de la campagne Etape par système (OC=Orienté Connaissances) sur les transcriptions avant adjudication (manuel : Man, transcription automatiques : Rover et WERXX, dont WER24 avec majuscules)  La reconnaissance d'entités nommées structurées sur de la parole spontanée nécessite de mettre  au point des systèmes robustes. Dans cet article, nous présentons une approche originale à base de fouille de données, qui extrait des règles d'annotation partielles et paramètre un modèle numérique les utilisant.  Les résultats obtenus dans le cadre de la campagne Etape indiquent que notre approche novatrice  fait jeu égal avec les systèmes état de l'art. Pour éviter tout biais méthodologique, nous restons toutefois en attente d'une référence débarrassée de toute erreur d'annotation : c'est l'objectif de la phase d'adjudication en cours. Notre objectif à court terme est de mieux caractériser les points forts et limitations du modèle (détection séparée du début et de la fin des annotations). Nous comptons également mettre à l'épreuve le système sur d'autres tâches qui pourraient bénéficier de l'extraction de motifs de segments.  Ces travaux ont été réalisés dans le cadre du projet ANR Etape. Merci en particulier à Olivier  Galibert (LNE), Matthieu Carré (ELDA) et Guillaume Gravier (IRISA).  
