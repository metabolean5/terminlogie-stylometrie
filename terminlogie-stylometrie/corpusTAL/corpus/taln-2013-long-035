C'est un truisme de nos jours de dire qu'Internet est une mine d'information, de ressources et  de savoirs qui peuvent sembler infinis. Ces informations sont utiles à toute personne souhaitant s'informer ou se distraire, mais également aux chercheurs de nombreux domaines (biologie, sciences sociales, informatique, . . .) pour qui Internet est devenu un objet de recherches. Cependant, malgré les progrès liés, par exemple par le passage au WEB 2.0, on ne peut que constater que les informations sur le Web ne sont pas fiables, ni même accessibles aisément.  Les systèmes de réponses aux questions sont un moyen efficace de rendre cette information à la  fois plus accessible et plus fiable. Plus accessible, car ces systèmes répondent de façon précise, rapide et concise aux questions qui leur sont posées en langue naturelle (à l'instar des moteurs de recherche usuels ). Plus fiable, car les réponses sont validées par le système (Peñas et al., 2007). Une étape primordiale (dans toutes les acceptions du terme) pour les systèmes de questionsréponses (QR) est l'opération qui consiste à extraire le contenu textuel des pages. Pour cela, il est nécessaire (Grau, 2004) de nettoyer, restructurer et filtrer leur contenu (par exemple de corriger les balises HTML et les erreurs d'encodage, d'éliminer les codes javascript résiduels et les spams).  La qualité (au sens de leur adéquation à la tâche QR) des textes obtenus dépend fortement de  l'extracteur employé (Baroni et al., 2008) et de la qualité intrinsèque de l'information contenue dans les documents à l'origine. Une tâche cruciale, mais souvent mésestimée, pour un système QR est de pouvoir filtrer les documents dont la qualité intrinsèque est faible, afin d'augmenter la précision de la sélection des meilleurs candidats lors de l'extraction de réponses.  Au cours de travaux précédents (Foucault et al., 2011), nous avons mis en place une stratégie  de sélection des documents pertinents pour un système QR en français, en complément de la sélection de documents traditionnelle effectuée par le moteur de recherche du système. Cette sélection repose sur une mesure de la qualité intrinsèque des documents en utilisant un modèle de langue, qui nous fournit a priori des mesures objectives sur le degré d'informativité d'un texte. Cette stratégie permet d'écarter de la liste des candidats sélectionnés par le moteur de recherche du système, les documents les plus bruités (c'est-à-dire de faible qualité) pour la tâche QR. Ici, un texte est considéré comme pertinent ou non dans sa globalité.  Il est de coutume en QR (Ligozat, 2006) de découper les documents en passage soit au moment  de leur indexation, soit au cours des recherches. L'idée est de réduire la variabilité naturelle des documents en taille et en contenu. En effet, avec des segments textuels plus petits, on peut espérer une variabilité plus faible, et un contenu informationnel (corrélé au contenu linguistique, en particulier lexical et sémantique) plus cohérent, ce qui en retour doit permettre l'extraction de réponses plus pertinentes que celles issues de la globalité du texte. Cette stratégie a fait ses preuves par le passé et des travaux récents autour du découpage de textes en passages (Tiedemann, 2007; Khalid et Verberne, 2008) l'ont consolidée.  A notre connaissance, personne n'a tenté de segmenter les documents préalablement à leur  indexation, tout en découpant les segments obtenus en passages au moment des recherches dans le but de réduire plus fortement la variabilité des documents. Dans cet article, nous détaillons plusieurs expériences visant à mesurer l'impact d'une telle pré-segmentation sur la tâche QR.  L'idée de segmenter des documents textuels (article de journaux, livres, . . .) en blocs de texte  est un axe de recherche activement exploré dans les années 90 en Recherche d'Information (RI) textuelle. Pour réduire la variabilité linguistique d'un texte, une première idée, explorée notamment par Salton (Salton et al., 1996) et Hearst (Hearst, 1997) consiste à opérer une segmentation en blocs thématiques, les frontières de blocs étant les endroits où on détecte un changement de thème. Des calculs de proximité lexicale entre blocs adjacents permettent de réorganiser le texte d'origine en segments plus homogènes (mais toujours de taille variable). Chez Salton, la proximité lexicale est obtenue à l'aide de mesures de distances vectorielles, chaque bloc étant représenté par un vecteur lexical. En fonction de valeurs seuils sur ces distances, des fusions entre paragraphes sont opérées. (Salton et al., 1996) effectue une fusion itérative, chaque itération fusionnant les paragraphes jugés similaires selon cette distance, le document étant exploré du début à la fin, de gauche à droite. L'itération s'arrête lorsque le texte ne contient plus que des blocs thématiquement homogènes. A partir de cette segmentation, Salton dérive un graphe des relations thématiques qu'entretiennent les blocs au sein du document. Dans le même esprit, (Hearst, 1997) fusionne des blocs de textes entre eux, mais de manière plus fine. Elle se fonde sur une analyse plus linguistique du texte que Salton. En effet, l'algorithme de segmentation de Hearst (TextTiling) utilise la structure du discours (ici la théorie des chaînes lexicales) et se fonde sur une segmentation en unités lexicales élémentaires (tokens). Ces tokens forment les unités de base pour la représentation de chaque bloc textuel. Cet algorithme utilise une mesure de distance fondée sur les statistiques de co-occurence. TextTiling ne fonctionne pas sur les paragraphes d'origine du texte contrairement à l'algorithme de Salton, mais sur des blocs de pseudo-phrases construits sur la base de ces paragraphes.  Plus récemment, des travaux dans le contexte de la RI dans des documents multimédia ont  proposé une nouvelle méthode d'indexation de pages web (Faessel, 2008). Celle-ci s'appuie sur l'information des représentations DOM et CSS des pages web pour segmenter ces dernières avant indexation ; (Bruno et al., 2009) démontrent que l'utilisation de ces informations conduit à une augmentation des performances d'un moteur de recherche dans sa tâche. D'autres travaux de segmentation pour la classification automatique de pages web en thème (Qi et Davison, 2009) utilisent la représentation DOM. Dans (Gupta et al., 2003) l'extraction du contenu textuel des pages se fait automatiquement à l'aide de la structure des arbres DOM. Dans (Asirvatham et al., 2001), l'échantillonnage des couleurs des images est utilisé pour catégoriser les pages qui les contiennent. Dans (Kovacevic1 et al., 2004), pour la même tâche, on utilise le rendu visuel. (Guo et al., 2007) utilise des indices visuels (le rendu des pages fourni par le moteur de Mozilla ), géométriques (les coordonnées des éléments de l'arbre DOM au sein du rendu des pages) et le style des pages (la répétition d'information) pour définir les blocs d'information pertinents trouvés dans les pages et les annoter sémantiquement. Dans (Feng et al., 2005), les auteurs étudient l'impact d'indices visuels et structurels sur la segmentation en blocs au travers d'une tâche de catégorisation fonctionnelle (blocs de type menu, titre, contenu, etc.). Dans (Vadrevu et al., 2005), les auteurs utilisent des critères de découpage fondés sur l'homogénéité locale du contenu informationnel des pages web (i.e. modèle de segmentation basé sur le concept de path entropy) et d'indices visuels dérivés de leur représentation DOM. Certains systèmes comme VIPS (Cai et al., 2003) se fondent essentiellement sur ce type d'indices pour segmenter les pages. A notre connaissance, aucune tentative d'application de ces techniques comme procédure de segmentation de pages web n'a été faite en QR. Nous avons choisi dans un premier temps d'utiliser un algorithme de première génération : le TextTiling de Hearst. Ce dernier a montré son intérêt pour la sélection de document pertinent en RI et se fonde sur une philosophie sousjacente commune à notre domaine en TAL (Traitement Automatique des langues). De plus, on en trouve des implémentations en libre accès (contrairement à certains des algorithmes évoqués plus haut). Par ailleurs, TextTiling présente l'avantage de fournir une segmentation en blocs thématiques qui pourrait être mise à contribution par la suite pour renforcer l'analyse sémantique des documents par un système QR. Dans la perspective de travaux futurs en segmentation de pages web autour de leur représentation visuelle en QR, le TextTiling nous permettra de bénéficier d'une segmentation TAL de référence à comparer à des approches de RI non textuelles. C'est dans cette optique que le travail présenté dans cet article se positionne.  Dans la section 3, nous présentons notre méthode de segmentation textuelle développée sur  la base du TextTiling de Hearst ; dans la section 4 nous évaluons cette méthode sur la tâche Questions-Réponses. Nous concluons et présentons les perspectives de ce travail dans la section 5.  Dans cette section, nous présentons la méthode de segmentation de pages web que nous avons  mise en place pour la sélection de documents pertinents en QR.  La figure 1 présente les étapes-clés de la chaîne de traitement qui correspond à cette métode : de  l'extraction du contenu textuel des pages à l'obtention de blocs de textes normalisés. Chaque étape clé de cette chaîne est décrite successivement dans les sections 3.2, 3.3 et 3.4.  F  1 - Notre procédure de segmentation de pages web en lien avec les (pré-)traitements QR. En théorie, nous aurions dû inverser les étapes de normalisation et de segmentation afin que la segmentation bénéficie des traitements de normalisation (voir section 3.4). Cependant, une telle inversion nécessite certaines modifications de notre chaîne de normalisation : en effet, cette dernière supprime l'indentation des textes utile à l'algorithme de TextTiling (voir section 3.3.1). Nous n'avons malheureusement pas eu le temps de mettre en place les modifications adéquates.  Notre procédure d'extraction se déroule en deux temps : pré-traitement (section 3.2.1) puis  représentation et extraction du contenu textuel des pages web (section 3.2.2). La phase de prétraitement des pages web est prise en charge par Kitten (Falco et al., 2012), un outil de traitement de documents web développé au LIMSI. La représentation et l'extraction du contenu textuel des pages web se fait sur les versions des pages pré-traitées par Kitten à l'aide du navigateur textuel de pages web Lynx .  3.2.1 Pré-traitement des pages web   Le pré-traitement des documents web est réalisé à l'aide de Kitten. Ce choix est motivé par les  performances état de l'art que ce dernier a obtenu en qualité d'extracteur textuel (Falco et al., 2012) dans le cadre d'évaluations QR sur le système Fidji (Moriceau et Tannier, 2010).  Kitten est un outil développé au LIMSI, dédié aux traitements et à la normalisation de données  Html. Les pages web fournies en entrée sont traitées et de nouvelles pages web au format Xhtml valide W3C (encodées en UTF8) sont produites en sortie. Ces pages sont bien formées (correction de leur squelette Html via jTidy ), sans erreurs d'encodage (correction de leur encodage via jChardet et conversion des caractères Html spéciaux dans une base Unicode via HTMLCleaner ).  Kitten produit des pages web exploitables en Extraction d'Information (EI) (Baroni et al., 2008)  sans appliquer d'heuristiques de nettoyage prédéfinies contrairement à des outils classiques de nettoyage de contenu comme Boilerpipe (Kohlschütter et al., 2010) ou Ncleaner (Evert, 2008) ; par exemple Ncleaner préserve pour l'essentiel le texte des balises et contenus dans le corps des pages, toute autre balise étant jugée non pertinente pour l'extraction. Par ailleurs, Kitten dispose de nombreuses fonctions et filtres configurables qui le rendent flexible. Ainsi, il est possible de conserver le contenu des attributs associé à un lien tout en supprimant le lien ou au contraire conserver ce lien tout en supprimant les attributs qui lui sont associés. Kitten se rapproche donc plutôt d'outils de développement populaires dans le domaine de l'EI web comme la librairie Python Beautiful Soup et le framework de crawling web Scrapy .  Kitten dispose de son propre module d'extraction de pages web et d'un système d'extraction  back-off basé sur Lynx. Celui-ci sert d'extracteur principal dans notre système de segmentation. 3.2.2 Représentation des pages web et extraction textuelle  La représentation des pages utilisée par notre moteur d'extraction se fait grâce à Lynx. Ce  dernier est un navigateur d'informations distribuées à portée générale pour Internet. Il permet de naviguer sur le Web depuis une console, en mode textuel uniquement. C'est un outil libre intégré automatiquement dans la plupart des distributions Linux grand public comme Ubuntu, qui intègre de nombreuses fonctionnalités web, dont l'extraction du contenu textuel de pages web.  Nous avons retenu Lynx pour deux raisons. La première raison est qu'il fournit une extraction  textuelle de pages web qui reflète leur rendu visuel. La seconde raison est que Lynx peut fournir une décomposition linéaire du contenu des pages web en blocs de texte, adaptée à la plupart des traitements d'analyses de documents en QR.  Si Lynx produit des extractions textuelles fidèles au rendu visuel des pages web, l'agencement des  blocs d'extraction diffère de celui observé dans un navigateur web classique du type Firefox . En effet, Lynx effectue une traversée gauche-droite descendante des pages web. En conséquence, les blocs d'information textuelle rencontrés le long du parcours sont mis bout à bout dans le fichier d'extraction résultant. Ainsi, on trouve souvent dans les extractions textuelles de Lynx la suite de blocs suivant (donnés ici selon leur contenu visuel) : bandeau, menus, colonne gauche, bloc de contenu principal, colonne droite, puis pied de page. On peut aussi trouver des agencements moins stéréotypiques selon le design des pages et trouver des séries de blocs de contenu principal qui s'enchaînent. L'étape d'extraction textuelle est réalisée par Lynx via le système d'extraction back-off de Kitten.  Nous avons utilisé deux stratégies de segmentation en blocs de texte. La première stratégie  consiste à segmenter les textes extraits par Lynx en blocs thématiques de taille variable par l'algorithme de TextTiling de Hearst (Hearst, 1997). La seconde stratégie vise à contrôler la précédente et segmente les textes extraits par Lynx de façon uniforme en blocs de taille identique.  3.3.1 Segmentation par TextTiling   L'algorithme de TextTiling de Hearst (Hearst, 1997) segmente un texte en unités appelées  multi-paragraphes en fonction des thématiques abordées dans le texte. Traditionnellement, il est utilisé pour détecter les thématiques dans des textes fortement structurés (e.g. articles de journaux, textes issus de livres . . .) et de grande taille (i.e. de plusieurs pages). Une des questions sous-jacente aux expériences que nous avons menées était de savoir si cet algorithme pourrait être utile pour la segmentation de pages web.  L'algorithme, présenté en détail dans (Hearst, 1997) s'articule autour des 3 étapes suivantes :   - tokenisation ;  - calcul de scores lexicaux ; - identification de frontières. La procédure de segmentation démarre par une étape de tokenization du texte qui lui est fourni en entrée. Les mots qui sont des stopwords ne sont pas tokenisés et sont écartés. Les autres subissent une étape de stemming basée sur une fonction d'analyse morphologique. Le texte est tokenizé en pseudo-phrases de longueur prédéfinie censée représenter la longueur moyenne d'un paragraphe (20 pseudo-phrases par défaut). Les paragraphes d'origine du texte servent de point d'ancrage pour la tokenization, qui elle-même dépend de l'indentation dans le texte.  L'algorithme évalue ensuite la proximité lexicale qui existe entre toutes les paires de blocs  adjacents possibles, et fournit un score fondé sur des co-occurences lexicales de tokens qui mesure l'écart entre deux blocs. Les blocs sont constitués des pseudo-phrases obtenues lors de la phase de tokenization. La détermination des scores lexicaux varie selon la stratégie utilisée. TextTiling dispose de 2 stratégies de comparaison de blocs différentes. La première (block comparison), compare deux blocs adjacents de texte et calcule leur écart sur la base du nombre de tokens qu'ils ont en commun. La seconde méthode (vocabulary introduction) évalue ce même écart sur la base des tokens issus des pseudo-phrases qui bordent la frontière entre deux blocs.  Enfin, l'algorithme procède au marquage des frontières de blocs pertinentes sur la base des  écarts mesurés à l'étape précédente. Ceci est fait à l'aide d'une fenêtre glissante sur les blocs. Les frontières de blocs présentant les plus forts écarts sont sélectionnées comme frontières thématiques.  L'implémentation que nous avons utilisée du TextTiling est fournie par le package Python NLTK  sans la fonction d'analyse morphologique. Le calcul des scores lexicaux se fait par block comparison.  3.3.2 Segmentation uniforme   Cette segmentation représente la condition contrôle dans nos expériences. Elle se contente de  segmenter chaque fichier texte qui lui est présenté en 8 blocs, c'est-à-dire la moyenne du nombre de blocs de segmentation obtenus par TextTiling sur notre corpus d'expérimentation au cours de tests préliminaires ; ceci revient à fixer la taille moyenne des blocs en nombre de lignes (voir la section 4.3).  La segmentation se fait selon un parcours linéaire du texte d'entrée, du début jusqu'à la fin, les  points de coupe sont déterminés à l'avance selon le nombre total de lignes dans le texte et le nombre maximum de blocs fixé en sortie (8). Les textes trop petits (ceux de moins de 8 lignes) ne sont pas segmentés et sont considérés comme des blocs uniques.  La normalisation est une étape durant laquelle un texte brut est traité afin qu'une unité lexicale  soit explicitement définie. Au cours de la normalisation, le texte est transformé dans une forme où les mots et les nombres sont clairement délimités, la ponctuation est séparée des mots, et des phrases ou pseudo-phrases sont clairement formées.  Notre normalisation passe par plusieurs étapes : séparation des mots et nombres de la ponctuation,  reconstruction de la casse sur les mots, ajout de la ponctuation le cas échéant et séparation en phrases ou pseudo-phrases du texte d'entrée. Elle s'appuie sur des lexiques, des dictionnaires de règles et des modèles de langue (Déchelotte et al., 2007).  Les expériences présentées ont pour but d'examiner l'hypothèse selon laquelle une fenêtre  d'analyse plus réduite pour traiter les documents web permettrait une sélection du système QR plus précise (c'est-à-dire obtenir des réponses plus pertinentes et en plus grand nombre). À cette fin, nous réalisons une segmentation avant l'indexation des documents en plus du découpage habituel en passages réalisé lors de l'extraction des réponses. La segmentation des documents est effectuée par TextTiling ou uniformément par notre algorithme de segmentation contrôle.  Les évaluations de l'impact de ces algorithmes de segmentation sur le système RITEL-QR (voir  section 4.2) sont présentées section 4.5 selon 3 conditions expérimentales :  - condition 1 : condition sans segmentation ou baseline (bsln) ;  - condition 2 : condition en segmentation par TextTiling (TT) ; - condition 3 : condition en segmentation contrôle (ctrl). Le système RITEL-QR que nous utilisons dans les expériences est complètement décrit dans (Bernard et al., 2009) et (Galibert, 2009). Il s'agit d'un système qui a été conçu à l'origine comme un système de dialogue (Toney et al., 2008). D'un point de vue général, on peut dire que le système s'appuie sur une analyse multi-niveaux, appliquée sur les questions et sur les documents. Les documents sont totalement analysés et indexés d'après les résultats d'analyse. La recherche est effectuée dans l'index complet des documents. L'analyse permet de repérer et typer des éléments pertinents d'information qui peuvent prendre la forme d'entités nommées, complexes et structurées, de chunks morpho-syntaxiques, d'actes de dialogue et de marqueurs thématiques.  La première étape de RITEL-QR consiste à créer un descripteur de recherche (DDR) qui contient  toutes les informations utiles pour la recherche de documents, l'extraction de passages pertinents et l'extraction de réponses. Ces informations sont les éléments de la question, leurs transformations possibles (dérivations morphologiques, synonymes etc. et les poids associés), et les types attendus de la réponse (avec les poids associés). Ces types sont le plus souvent des types d'entités nommées (personne, lieu . . .) et reflètent la taxonomie d'entités utilisée au moment de l'analyse.  La sélection des documents consiste à fournir, à partir de l'index, les n documents les plus  pertinents, c'est-à-dire ceux contenant le plus d'informations présentes dans le DDR. En fonction de ces informations et de leur densité, les documents obtiennent un score. Ensuite, des passages sont extraits de chaque document. Ces passages sont de tailles variables (une fenêtre d'analyse différente est appliquée selon la catégorie de la question) et sont scorés selon le même principe que les documents. L'extraction et l'évaluation des candidats réponses s'appuient sur la redondance de ces derniers dans les documents et les passages. On considère que les éléments de passages qui correspondent à un type possible de réponse du DDR et qui ne sont ni des élements ni des sous-éléments définis dans le DDR, sont des candidats réponses potentiels. A chacun d'eux est finalement attribué à un score de pertinence (Bernard et al., 2009).  Le corpus de pages web utilisé dans nos expérimentations (ci-après Q07fr) est composé de  499 734 pages web (5Gbytes) tout venant (i.e. journal, Wikipédia, blog, site de vente, forum, etc.) et en français. Il nous est fourni par le projet Quaero et sert de corpus standard dans le cadre des évaluations QR au sein du projet (Quintard et al., 2010). Les questions de test et d'entraînement utilisées (309 et 722 questions) proviennent du même projet. Elles ont été créées à partir de logs utilisateurs (Quintard et al., 2010) du moteur de recherche français Exalead et sont composées de questions factuelles (e.g. Qui est Gandhi ?, Combien pèse la tour Eiffel ?, Où se situe Pondichéry ? et Que signifie CSDPTT ?).  T  1 -  Le tableau 1 (a) présente les résultats des traitements (en terme de nombre de fichiers traités) de  chacune des étapes de notre chaîne de segmentation, ainsi que des étapes de pré-traitements QR (annotation et indexation), pour chacune des conditions d'expérimentations (Cond) à partir de Q07fr. Ces résultats suivent le schéma de la figure 1. Chaque sortie d'une étape dépend du résultat qui précède pour une condition donnée. On peut noter que le nombre de fichiers issus de la segmentation dans les conditions contrôle (ctrl) et TextTiling (TT) sont proches (environ 20K blocs de différence). Les blocs indexés dans ces 2 conditions correspondent aux 484 060 textes indexés en condition baseline (bsln). La durée des traitements (parallèles/mêmes serveurs) dans ces conditions est respectivement de 2 à 5 fois plus longue qu'en condition sans segmentation.  Le tableau 1 (b) présente le nombre moyen de blocs (nbB) et de lignes par bloc (nbL) obtenus en  conditions contrôle et TextTiling. Ces informations sont aussi données pour la condition baseline à titre indicatif (un bloc par fichier). On constate que l'algorithme de TextTiling et le contrôle se comportent de façon très similaire : en moyenne, le nombre de blocs produits (ctrl : 8 et TT : 7,3) ainsi que leur taille (ctrl : 19,4 et TT : 20,9) sont semblables. Le TextTiling produit une légère sur-segmentation : la déviation standard est 2 fois plus élevée que celle du contrôle (ctrl) en nombre de lignes, le maximum de blocs pour un même document étant également plus grand.  Dans ce travail, nous employons les métriques habituellement utilisées en QR :   - la précision définie équation (1), est le ratio entre le nombre de réponses correctes et le nombre  total de questions. Si le système est capable de fournir plusieurs réponses par question, on ne considère que la première. CR est le rang de la première réponse correcte pour la question i. CR prend pour valeur + si aucune réponse correcte n'a été trouvée. - Le top-n défini équation (2), mesure la précision selon les réponses correctes de rang 1 à n. - Le Mean Reciprocal Rank (Moyenne des Réciproques des Rangs ou MRR) défini équation (3), permet de mesurer la qualité du classement des réponses (10 par question) effectué par le système. La réponse correcte la mieux classée est pondérée par l'inverse de son rang initial. Une absence de réponse correcte entraîne une contribution nulle. Le score final correspond à la moyenne des contributions.  précision =  #CR = 1 #questions (1) top-n = #CR  n #questions (2) MRR =  CR #questions (3)  Les résultats sont présentés dans les parties (a) et (b) du tableau 2. On a utilisé le test de  McNemar (McNemar, 1947; Agresti, 1990) de R pour juger de la significativité des résultats présentés tableau 2 (a). Les résultats du test sont donnés tableau 3 .  On constate, tableau 2 (a), que les deux conditions de segmentation testées sont proches de la  condition baseline suggérant ainsi que la segmentation n'apporte pas de réels bénéfices à notre système QR. Les performances du système sont très proches en terme de précision (0,6 point de différence au plus entre bsln et TT). Mais le MRR présente un écart plus important entre les conditions (2 points entre les conditions bsln et ctrl, et 1 point entre les conditions bsln et TT). La segmentation ctrl semble donc permettre au système de trouver de meilleures réponses (i.e. des réponses plus précises) qu'en condition baseline ou TextTiling. Toutefois, d'après les tests statistiques des performances QR présentés tableau 3, ceci n'est qu'une tendance.  Le test de McNemar (McNemar, 1947), que nous avons utilisé dans nos expériences, établit la  significativité des résultats observés entre 2 conditions A et B et une mesure M donnée, selon des variations observées entre A et B, synthétisées dans une table de contingence 2x2. De là, le test (bilatéral) estime une valeur Q (i.e. khi² de McNemar) pour un degré de liberté d f donné et dérive une valeur p. Si p est inférieure (ou égale) au seuil critique , l'hypothèse nulle H est rejetée et la différence observée entre A et B est jugée significative. Dans notre cas, une table de contingence comptabilise le total de questions (#q) pour lesquelles RITEL-QR trouve une réponse de même exactitude en conditions A et B. Il y a 4 types de compte, nombre total de questions avec une réponse : correcte (r) selon A et selon B (#rr), fausse (w, xs ou xl) selon A et selon B (#WW), correcte selon A et fausse selon B (#rW) et inversement (#Wr). Ainsi, on peut constater que l'hypothèse H selon laquelle la différence observée entre les conditions bsln et ctrl n'est pas significative pour les performences QR en top-10, est à peine rejetée : la valeur de p obtenue dans ces conditions n'étant pas inférieure mais tout juste alignée sur le seuil critique de significativité  .  L'étude du nombre total de bonnes réponses fournies par le système selon leur position au sein  du top-10 tableau 2 (b) (bsln : 178, TT : 183 et ctrl : 190) confirme cette tendance. On voit aussi dans ce tableau que les réponses apportés par le système en condition ctrl (jusqu'à 12 réponses suppplémentaires, soit 3,9% de réponses en plus) se trouvent dans le top-3, là où la segmentation par TextTiling a tendance à apporter de nouvelles réponses à des rangs inférieurs.  Nous avons pu constaté que la segmentation des documents accélérait les (pré-)traitements QR.   Cond  P MRR top-10 #q bsln 31.4 39.6 57.6 309 ctrl 31.7 41.6 61.5 309 TT 32.0 40.5 59.2 309 Cond #r #xs #xl #w bsln 97 6 8 198 ctrl 98 11 5 195 TT 99 11 2 197 Cond rang bsln ctrl TT 1 97 98 99 2 26 32 26 3 17 26 19 4 9 7 7 5 8 7 11 6 6 4 6 7 7 5 3 8 6 5 4 9 2 2 8 10 0 4 0 Total 178 190 183  T  2 -        T  3 -   Au cours de travaux précédents (Foucault et al., 2011) nous avons mis en place une stratégie  de sélection de documents pertinents pour un système QR sur le français. Elle s'appuie sur un modèle de langue qui fournit a priori une mesure objective du degré d'informativité d'un texte. Cette mesure de la qualité intrinsèque des documents sert à filtrer les documents non pertinents pour la tâche QR. L'effet d'un tel filtrage appliqué à l'échelle globale des documents, c'est avéré assez limité. La variabilité naturelle des pages web en taille et en contenu (comme leur caractère multi-thématique) pénalise vraisemblablement le système dans sa tâche. Nous avons donc cherché à développer un système de segmentation qui permette d'appliquer ce filtrage à une échelle non plus globale mais locale, sur des sous-parties de document. Le travail présenté dans cet article avait pour objectif de mettre un tel système de segmentation en place.  La question à laquelle nous avons voulu répondre dans cette article est la suivante : segmenter  les documents avant l'indexation, en plus du découpage habituel des documents en passages lors de l'extraction des réponses, améliore-t-il les performances d'un système de questions-réponses ?  Pour répondre à cette question, nous avons testé deux types de pré-segmentation supportée par  une extraction de contenu textuel de pages web maison. L'une segmente les textes extraits via un algorithme de texttiling classique (TextTiling) en blocs thématiques de taille variable. L'autre les segmente uniformément en blocs de taille fixe, sans découpage thématique.  Les résultats obtenus ne nous permettent pas de trancher nettement en faveur de l'une ou l'autre  de ces approches de segmentation. Cependant, les tendances observées suggèrent qu'une présegmentation des pages web comme nous l'avons définie peut servir un système QR ; segmenter les documents avant l'indexation afin de renforcer l'effet du découpage de ces derniers en passages lors de l'extraction des réponses, améliore la précision du système en terme de top-10 sans pour autant diminuer cette dernière en terme de top-1. Cette tendance est plus marquée pour la segmentation uniforme de pages web que pour une segmentation plus « intelligente » à l'aide de l'algorithme de TextTiling (sans analyse morphologique, le calcul des scores lexicaux se faisant par block comparison). Ce constat est contradictoire avec d'autres travaux, mais confirme certaines conclusions apportées par Hearst dans ses travaux de segmentation thématique de textes en Recherche d'Information (Hearst, 1997). Il serait intéressant de déterminer les raisons amenant à ce constat. Si la nature des documents (page web versus texte), est l'une des raisons qui pourrait l'expliquer, qu'en est-il par exemple de la longueur des documents et de la version du TextTiling que nous avons utilisé dans nos expériences ?  En perspective des travaux présentés dans cet article, nous projetons d'abord d'étudier l'impact  d'une pré-segmentation uniforme des pages web sur notre stratégie de sélection de documents pertinents développée dans (Foucault et al., 2011). Concernant nos travaux de segmentation de pages web en QR à partir de la représentation visuelle des pages, nous comptons évaluer la pertinence de la procédure d'extraction mise en place au sein du système de segmentation de pages web présenté dans cet article.  Ce travail a été financé partiellement par l'OSEO, dans le contexte du programme Quaero.   
